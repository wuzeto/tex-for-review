\documentclass{ctexbook}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multirow}

\usepackage{float}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows.meta, positioning, chains, decorations.pathreplacing, calligraphy}
\usepackage[dvipsnames, svgnames, x11names]{xcolor}
\usepackage{tcolorbox}
\usepackage[normalem]{ulem}
\usepackage{enumitem} % 用于调整列表格式
\usepackage{titlesec} % 用于自定义章节样式
\usepackage{lipsum}   % 用于生成占位文本（可删除）
\usepackage{listings} % 用于插入伪代码
% 设置页面边距
\geometry{a4paper, margin=2cm}
% 自定义章节样式
\titleformat{\chapter}[display]
{\normalfont\bfseries\centering}
{\chaptertitlename\ \thechapter}{6pt}{\Large}
% 配置伪代码环境
\lstset{
	language=C,
	basicstyle=\ttfamily\small,
	keywordstyle=\color{blue},
	commentstyle=\color{gray},
	stringstyle=\color{red},
	showstringspaces=false,
	breaklines=true,
	frame=single,
	numbers=left,
	numberstyle=\tiny\color{gray},
	rulecolor=\color{black},
	captionpos=b,
	tabsize=4,
	title=\lstname,
	escapeinside={(*@}{@*)},
}
% 自定义小节样式
\titleformat{\section}[hang]
{\normalfont\bfseries}
{\thesection}{0.3em}{}

\titleformat{\subsection}[hang]
{\normalfont\bfseries}
{\thesubsection}{0.3em}{}


% 设置枚举环境的缩进和间距
\setlist[enumerate]{leftmargin=*, itemsep=3pt}

\title{408操作系统与计算机网络笔记}
\author{}
\date{}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\chapter{计算机系统概述}
	
	\section*{【考纲内容】}
	
	\begin{enumerate}
		\item 操作系统的基本概念
		\item 操作系统的发展历程
		\item 程序运行环境
		
		
		CPU 运行模式：内核模式与用户模式；中断和异常的处理；系统调用；
		
		程序的链接与装入；程序运行时内存映像与地址空间\footnote{此处为脚注示例。}
		
		\item 操作系统结构
		
		
		分层、模块化、宏内核、微内核、外核
		
		\item 操作系统引导
		\item 虚拟机
	\end{enumerate}
	
	\section*{【复习提示】}
	
	本章通常以选择题的形式考查，重点考查操作系统的功能、运行环境和提供的服务。要求读者能从宏观上把握操作系统各部分的功能，微观上掌握细微的知识点。因此，复习操作系统时，首先要形成大体框架，并通过反复复习和做题巩固知识体系，然后将操作系统的所有内容串成一个整体。本章的内容有助于读者整体上初步认识操作系统，为后面掌握各章节的知识点奠定基础，进而整体把握课程，不要因为本章的内容在历年考题中出现的比例不高而忽视它。
	
	\section{操作系统的基本概念}
	\subsection{操作系统的概念}
	在信息化时代，软件是计算机系统的灵魂，而作为软件核心的操作系统，已与现代计算机系统密不可分、融为一体。计算机系统自下而上可以大致分为 4 部分：硬件、操作系统、应用程序和用户（这里的划分与计算机组成原理中的分层不同）。操作系统管理各种计算机硬件，为应用程序提供基础，并且充当计算机硬件与用户之间的中介。
	
	硬件如中央处理器、内存、输入/输出设备等，提供基本的计算资源。应用程序如字处理程序、电子制表软件、编译器、网络浏览器等，规定按何种方式使用这些资源来解决用户的计算问题。操作系统控制和协调各用户的应用程序对硬件的分配与使用。
	
	在计算机系统的运行过程中，操作系统提供了正确使用这些资源的方法。
	
	综上所述，操作系统（Operating System，OS）是指控制和管理整个计算机系统的硬件与软\footnote{这两个考点将在第 3 章的 3.1 节中介绍。}
	件资源，合理组织、调度计算机工作及资源分配，为用户和其他软件提供便捷接口与环境 ，强调其是计算机系统中最基础的系统软件。
	\subsection{操作系统的功能和目标}
	为给多道程序提供良好的运行环境，操作系统应具有以下几方面的功能：处理器管理、存储器管理、设备管理和文件管理。为方便用户使用操作系统，还要向用户提供接口。同时，操作系统可用来扩充机器，以提供更方便的服务、更高的资源利用率。
	
	下面用一个直观的例子来理解这种情况。例如，用户是雇主，操作系统是工人（用来操作机器），计算机是机器（由处理机、存储器、设备、文件几个部件构成），工人有熟练的技能，能够控制和协调各个部件的工作，这就是操作系统对资源的管理；同时，工人必须接收雇主的命令，这就是“接口”；有了工人，机器就能发挥更大的作用，因此工人就成了“扩充机器”。
	
	\subsubsection{操作系统作为计算机系统资源的管理者}
	\begin{enumerate}
		\item 处理机管理
		在多道程序环境下，处理机的分配和运行都以进程（或线程）为基本单位，因此对处理机的管理可归结为对进程的管理。并发是指在计算机内同时运行多个进程，因此进程何时创建、何时撤销、如何管理、如何避免冲突、合理共享就是进程管理最主要的任务。进程管理的主要功能包括进程控制、进程同步、进程通信、死锁处理、处理机调度等。
		\item 存储器管理
		存储器管理是为了给多道程序的运行提供良好的环境，方便用户使用及提高内存的利用率，主要包括内存分配与回收、地址映射、内存保护与共享和内存扩充等功能。
	\end{enumerate}
	
	\subsubsection{操作系统作为计算机系统资源的管理者}
	\begin{enumerate}
		\setcounter{enumi}{2}
		\item 文件管理
		计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统。文件管理包括文件存储空间的管理、目录管理及文件读/写管理和保护等。
		\item 设备管理
		设备管理的主要任务是完成用户的 I/O 请求，方便用户使用各种设备，提高设备的利用率，主要包括缓冲管理、设备分配、设备处理和虚拟设备等功能。
		这些工作都由“工人”负责，“雇主”无须关注。
	\end{enumerate}
	
	\subsubsection{操作系统作为用户与计算机硬件系统之间的接口}
	为让用户方便、快捷、可靠地操纵计算机硬件并运行自己的程序，操作系统还提供用户接口。操作系统提供的接口主要分为两类：一类是命令接口，用户利用这些操作命令来组织和控制作业的执行；另一类是程序接口，编程人员可用来请求操作系统服务。
	\begin{enumerate}
		\item 命令接口
		使用命令接口进行作业控制的主要方式有两种，即联机控制方式和脱机控制方式。按作业控制方式的不同，可将命令接口分为联机命令接口和脱机命令接口。
		联机命令接口也称交互式命令接口，适用于分时或实时系统的接口。联机命令由一组键盘操作命令组成。用户通过控制台或终端输入操作命令，向系统提出各种服务要求。用户每输入一条命令，控制权就转给操作系统的命令解释程序，然后由命令解释程序解释并执行输入的命令，进而完成指定的功能。之后，控制权转回控制台或终端，此时用户又可输入下一条命令。联机命令接口可以这样理解：“雇主”说一句话，“工人”做一件事，并做出反馈，这就强调了交互性。
		脱机命令接口也称批处理命令接口，适用于批处理系统。脱机命令由一组作业控制命令组成。

		\setcounter{enumi}{1}
		\item 程序接口
		
		\colorbox{gray!20}{命题追踪 \enspace 操作系统为应用程序提供的接口（2010）}
		
		程序接口由一组系统调用（也称广义指令）组成。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务，如使用各种外部设备、申请分配和回收内存及其他各种要求。
		
		当前最流行的是图形用户界面（GUI），即图形接口。GUI 最终是通过调用程序接口实现的，用户通过鼠标和键盘在图形界面上单击或使用快捷键，就能方便地使用操作系统。严格来说，图形接口不是操作系统的一部分，但图形接口所调用的系统调用命令是操作系统的一部分。
	\end{enumerate}
	
	\subsubsection{操作系统实现了对计算机资源的扩充}
	没有任何软件支持的计算机称为裸机，它仅构成计算机系统的物质基础，而实际呈现在用户面前的计算机系统是经过若干层软件改造的计算机。裸机在最内层，外面是操作系统。操作系统所提供的资源管理功能和方便用户的各种服务功能将裸机改造成功能更强、使用更方便的机器；因此，我们通常将覆盖了软件的机器称为扩充机器或虚拟机。
	
	“工人”操作机器，机器就有更大的作用，于是“工人”便成了“扩充机器”。
	
	注意，本课程关注的是操作系统如何控制和协调处理机、存储器、设备和文件，而不关注接口和扩充机器，对于后两者，读者只需要有个印象，能够理解即可。
	
	\subsection{操作系统的特征}
	操作系统是一种系统软件，但与其他系统软件和应用软件有很大的不同，它有自己的特殊性即基本特征。操作系统的基本特征包括并发、共享、虚拟和异步。这些概念对理解和掌握操作系统的核心至关重要，将一直贯穿于各个章节中。
	
	\subsubsection{并发（Concurrency）}
	并发是指两个或多个事件在同一时间间隔内发生。在多道程序环境下，在内存中同时装有若干道程序，以便当运行某道程序时，利用其因 I/O 操作而暂停执行时的 CPU 空档时间，再调度另一道程序运行，从而实现多道程序交替运行，使 CPU 保持忙碌状态。
	
	\colorbox{gray!20}{命题追踪 \enspace 并行性的定义及分析（2009）}
	
	并行性是指系统具有同时进行运算或操作的特性，在同一时刻能完成两种或两种以上的工作。在支持多道程序的单处理机环境下，一段时间内，宏观上有多道程序在同时执行，而在每个时刻，实际仅能有一道程序执行，因此微观上这些程序仍是分时交替执行的。可见，操作系统的并发性是通过分时得以实现的。而 CPU 与 I/O 设备、I/O 设备和 I/O 设备则能实现真正的并行。
	
	若要实现进程的并行，则需要有相关硬件的支持，如多流水线或多处理机环境。
	
	注意同一时间间隔（并发）和同一时刻（并行）的区别，下面以生活中的例子来理解这种区别。例如，若你在 9:00—9:10 仅吃面包，在 9:10—9:20 仅写字，在 9:20—9:30 仅吃面包，在 9:30—10:00 仅写字，则在 9:00—10:00 吃面包和写字这两种行为就是并发执行的；又如，若你在 9:00—10:00 右手写字，左手同时拿着面包吃，则这两个动作就是并行执行的。
	
	在操作系统中，引入进程的目的是使程序能并发执行。
	
	\subsubsection{共享（Sharing）}
	资源共享即共享，是指系统中的资源可供内存中多个并发执行的进程共同使用。资源共享主要可分为互斥共享和同时访问两种方式。
	\begin{enumerate}
		\item 互斥共享方式
		系统中的某些资源，如打印机、磁带机，虽然可供多个进程使用，但为使得所打印或记录的结果不致造成混淆，应规定在一段时间内只允许一个进程访问该资源。
		
		为此，当进程 A 访问某个资源时，必须先提出请求，若此时该资源空闲，则系统便将之分配给 A 使用，此后有其他进程也要访问该资源时（只要 A 未用完）就必须等待。仅当 A 访问完并释放该资源后，才允许另一个进程对该资源进行访问。我们将这种资源共享方式称为互斥共享，而将在一段时间内只允许一个进程访问的资源称为临界资源。计算机系统中的大多数物理设备及某些软件中所用的栈、变量和表格，都属于临界资源，它们都要求被互斥地共享。
		\item 同时访问方式
		系统中还有另一类资源，这类资源允许一段时间内由多个进程“同时”访问。这里所说的“同时”通常是宏观上的，而在微观上，这些进程可能是交替地对该资源进行访问，即“分时共享”的。可供多个进程“同时”访问的典型资源是磁盘设备，一些用重入代码编写的文件也可被“同时”共享，即允许若干用户同时访问该文件。
		
		注意，互斥共享要求一种资源在一段时间内（哪怕是一段很短的时间）只能满足一个请求，否则就会出现严重的问题（你能想象打印机第一行打印文档 A 的内容、第二行打印文档 B 的内容的效果吗？），而同时访问共享通常要求一个请求分几个时间片段间隔地完成，其效果与连续完成的效果相同。
	\end{enumerate}
	
	并发和共享是操作系统两个最基本的特征，两者之间互为存在的条件：①资源共享是以程序的并发为条件的，若系统不允许程序并发执行，则自然不存在资源共享问题；②若系统不能对资源共享实施有效的管理，则必将影响到程序的并发执行，甚至根本无法并发执行。
	
	\subsubsection{虚拟（Virtual）}
	虚拟是指将一个物理上的实体变为若干逻辑上的对应物。物理实体（前者）是实的，即实际存在的；而后者是虚的，是用户感觉上的事物。用于实现虚拟的技术称为虚拟技术。操作系统的虚拟技术可归纳为：时分复用技术，如虚拟处理器；空分复用技术，如虚拟存储器。
	
	通过多道程序设计技术，让多道程序并发执行，来分时使用一个处理器。此时，虽然只有一个处理器，但它能同时为多个用户服务，使每个终端用户都感觉有一个 CPU 在专门为它服务。利用多道程序设计技术将一个物理上的 CPU 虚拟为多个逻辑上的 CPU，称为虚拟处理器。
	
	采用虚拟存储器技术将一台机器的物理存储器变为虚拟存储器，以便从逻辑上扩充存储器的容量。当然，这时用户所感觉到的内存容量是虚的。我们将用户感觉到（但实际不存在）的存储器称为虚拟存储器。
	
	还可采用虚拟设备技术将一台物理 I/O 设备虚拟为多台逻辑上的 I/O 设备，并允许每个用户占用一台逻辑上的 I/O 设备，使原来仅允许在一段时间内由一个用户访问的设备（临界资源）变为在一段时间内允许多个用户同时访问的共享设备。
	
	\subsubsection{异步（Asynchronism）}
	多道程序环境允许多个程序并发执行，但由于资源有限，进程的执行并不是一贯到底的，而是走走停停的，它以不可预知的速度向前推进，这就是进程的异步性。
	
	异步性使得操作系统运行在一种随机的环境下，可能导致进程产生与时间有关的错误（就像
	对全局变量的访问顺序不当会导致程序出错一样）。然而，只要运行环境相同，操作系统就须保
	证多次运行进程后都能获得相同的结果。
	
	\section{操作系统发展历程}
	\subsection{手工操作阶段（此阶段无操作系统）}
	用户在计算机上算题的所有工作都要人工干预，如程序的装入、运行、结果的输出等。随着计算机硬件的发展，人机矛盾（速度和资源利用）越来越大，必须寻求新的解决办法。
	
	手工操作阶段有两个突出的缺点：\textcircled{1}用户独占全机，虽然不会出现因资源已被其他用户占用而等待的现象，但资源利用率低。\textcircled{2}CPU 等待手工操作，CPU 的利用不充分。
	
	唯一的解决办法就是用高速的机器代替相对较慢的手工操作来对作业进行控制。
	
	\subsection{批处理阶段（操作系统开始出现）}
	为了解决人机矛盾及 CPU 和 I/O 设备之间速度不匹配的矛盾，出现了批处理系统。按发展历程又分为单道批处理系统、多道批处理系统（多道程序设计技术出现以后）。

	\colorbox{gray!20}{命题追踪 \enspace 批处理系统的特点（2016）}
	\subsubsection{单道批处理系统}
	为实现对作业的连续处理，需要先将一批作业以脱机方式输入磁带，并在系统中配上监督程序（Monitor），在其控制下，使这批作业能一个接一个地连续处理。虽然系统对作业的处理是成批进行的，但内存中始终保持一道作业。单道批处理系统的主要特征如下：
	\begin{enumerate}
		\item 自动性。在顺利的情况下，磁带上的一批作业能自动地逐个运行，而无须人工干预。
		\item 顺序性。磁带上的各道作业顺序地进入内存，先调入内存的作业先完成。
		\item 单道性。内存中仅有一道程序运行，即监督程序每次从磁带上只调入一道程序进入内存运行，当该程序完成或发生异常情况时，才换入其后继程序进入内存运行。
	\end{enumerate}
	此时面临的问题是：每次主机内存中仅存放一道作业，每当它在运行期间（注意这里是“运行时”而不是“完成后”）发出输入/输出请求后，高速的 CPU 便处于等待低速的 I/O 完成的状态。为了进一步提高资源的利用率和系统的吞吐量，引入了多道程序技术。
	
	\subsubsection{多道批处理系统}
	用户所提交的作业都先存放在外存上并排成一个队列，作业调度程序按一定的算法从后备队列中选择若干作业调入内存，它们在管理程序的控制下相互穿插地运行，共享系统中的各种硬/软件资源。当某道程序因请求 I/O 操作而暂停运行时，CPU 便立即转去运行另一道程序，这是通过中断机制实现的。它让系统的各个组成部分都尽量的“忙”，切换任务所花费的时间很少，因而可实现系统各部件之间的并行工作，使其在单位时间内的效率翻倍。
	
	\colorbox{gray!20}{命题追踪 \enspace 多道批处理系统的特点（2017、2018、2022）}
	
	多道程序设计的特点是多道、宏观上并行、微观上串行。
	\begin{enumerate}
		\item 多道。计算机内存中同时存放多道相互独立的程序。
		\item 宏观上并行。同时进入系统的多道程序都处于运行过程中，但都未运行完毕。
		\item 微观上串行。内存中的多道程序轮流占有 CPU，交替执行。
	\end{enumerate}
	
	多道程序设计技术的实现需要解决下列问题：
	\begin{enumerate}
		\item 如何分配处理器。
		\item 多道程序的内存分配问题。
		\item I/O 设备如何分配。
		\item 如何组织和存放大量的程序和数据，以方便用户使用并保证其安全性与一致性。
	\end{enumerate}
	在批处理系统中采用多道程序设计技术就形成了多道批处理操作系统。该系统将用户提交的作业成批地送入计算机内存，然后由作业调度程序自动地选择作业运行。
	
	优点：资源利用率高，多道程序共享计算机资源，从而使各种资源得到充分利用；系统吞吐量大，CPU 和其他资源保持“忙碌”状态。缺点：用户响应的时间较长；不提供人机交互能力，用户既不能了解自己的程序的运行情况，又不能控制计算机。
	
	\colorbox{gray!20}{注意 \enspace 2018 年真题考查的多任务操作系统可视为具有交互性的多道批处理系统。}
	
	\subsection{分时操作系统}
	所谓分时技术，是指将处理器的运行时间分成很短的时间片，按时间片轮流将处理器分配给
	各联机作业使用。若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时停止运行，将处理器让给其他作业使用，等待下一轮再继续运行。计算机速度很快，作业运行轮转得也很快，因此给每个用户的感觉就像是自己独占一台计算机。
	
	分时操作系统是指多个用户通过终端同时共享一台主机，这些终端连接在主机上，用户可以同时与主机进行交互操作而互不干扰。因此，实现分时系统的关键问题是如何使用户能与自己的作业进行交互，即当用户在自己的终端上键入命令时，系统应能及时接收并及时处理该命令，再将结果返回用户。分时系统也是支持多道程序设计的系统，但它不同于多道批处理系统。多道批处理是实现作业自动控制而无须人工干预的系统，而分时系统是实现人机交互的系统，这使得分时系统具有与批处理系统不同的特征。分时系统的主要特征如下：
	\begin{enumerate}
		\item 同时性。同时性也称多路性，指允许多个终端用户同时使用一台计算机。
		\item 交互性。用户通过终端采用人机对话的方式直接控制程序运行，与同程序进行交互。
		\item 独立性。系统中多个用户可以彼此独立地进行操作，互不干扰，单个用户感觉不到别人也在使用这台计算机，好像只有自己单独使用这台计算机一样。
		\item 及时性。用户请求能在很短时间内获得响应。
	\end{enumerate}
	虽然分时操作系统较好地解决了人机交互问题，但在一些应用场合，需要系统能对外部的信息在规定的时间（比时间片的时间还短）内做出处理（比如飞机订票系统或导弹制导系统），因此，实时操作系统应运而生。
	
	\subsection{实时操作系统}
	为了能在某个时间限制内完成某些紧急任务而不需要时间片排队，诞生了实时操作系统。这里的时间限制可以分为两种情况：若某个动作必须绝对地在规定的时刻（或规定的时间范围）发生，则称为硬实时系统，如飞行器的飞行自动控制系统，这类系统必须提供绝对保证，让某个特定的动作在规定的时间内完成。若能够接受偶尔违反时间规定且不会引起任何永久性的损害，则称为软实时系统，如飞机订票系统、银行管理系统。
	
	在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，并在严格的时限内处理完接收的事件。实时操作系统的主要特点是及时性和可靠性。
	
	\subsection{网络操作系统和分布式计算机系统}
	网络操作系统是伴随着计算机网络的发展而诞生的，它把网络中的各台计算机有机地结合起来，实现各台计算机之间的通信和数据传送等功能，实现网络中各种资源的共享。
	
	分布式计算机系统是由多台计算机组成并满足下列条件的系统：系统中任意两台计算机通过通信方式交换信息；每台计算机都具有同等的地位，即没有主机也没有从机；每台计算机上的资源为所有用户共享；系统中的任意台计算机都可以构成一个子系统，并且还能重构；任何工作都可以分布在几台计算机上，由它们并行工作、协同完成。用于管理分布式计算机系统的操作系统称为分布式计算机系统。该系统的主要特点是：分布性和并行性。分布式操作系统与网络操作系统的本质不同是，分布式操作系统中的若干计算机相互协同完成同一任务。
	
	\subsection{个人计算机操作系统}
	个人计算机操作系统是目前使用最广泛的操作系统，它广泛应用于文字处理、电子表格、游戏中，常见的有 Windows、Linux 和 MacOS 等。操作系统的发展历程如图 1.1 所示。
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{5544.png} % 请将文件名替换为实际的文件名
		\caption{操作系统的发展历程}
	\end{figure}
	
	\section{操作系统的运行环境}
	\subsection{处理器运行模式}
	在计算机系统中，通常 CPU 执行两种不同性质的程序：一种是操作系统内核程序；另一种是用户自编程序（系统外层的应用程序，简称应用程序）。对操作系统而言，这两种程序的作用不同，前者是后者的管理者，因此“管理程序”（内核程序）要执行一些特权指令，而“被管理程序”（用户自编程序）出于安全考虑不能执行这些特权指令。
	
	\colorbox{gray!20}{命题追踪 \enspace 特权指令和非特权指令的特点（2022）}
	\begin{enumerate}
		\item 特权指令，是指不允许用户直接使用的指令，如 I/O 指令、关中断指令、内存清零指令，存取用于内存保护的寄存器、修改程序状态字寄存器等的指令。
		\item 非特权指令，是指允许用户直接使用的指令，它不能直接访问系统中的软硬件资源，仅限于访问用户的地址空间，这也是为了防止用户程序对系统造成破坏。
	\end{enumerate}
	
	\colorbox{gray!20}{命题追踪 \enspace 内核态执行的指令分析（2021）}
	
	\colorbox{gray!20}{命题追踪 \enspace 用户态发生或执行的事件分析（2011、2012、2014）}
	
	在具体实现上，将 CPU 的运行模式划分为用户态（目态）和内核态（也称管态、核心态）。可以理解为 CPU 内部有一个小开关，当小开关为 0 时，CPU 处于内核态，此时 CPU 可以执行特权指令，切换到用户态的指令也是特权指令。当小开关为 1 时，CPU 处于用户态，此时 CPU 只能执行非特权指令。应用程序运行在用户态，操作系统内核程序运行在内核态。应用程序向操作系统请求服务时通过使用访管指令，访管指令是在用户态执行的，因此是非特权指令。
	
	在软件工程思想和结构化程序设计方法影响下诞生的现代操作系统，几乎都是分层式的结构。操作系统的各项功能分别被设置在不同的层次上。一些与硬件关联较紧密的模块，如时钟管理、中断处理、设备驱动等处于最低层。其次是运行频率较高的程序，如进程管理、存储器管理和设备管理等。这两部分内容构成了操作系统的内核。这部分内容的指令运行在内核态。
	
	内核是计算机上配置的底层软件，它管理着系统的各种资源，可视为连接应用程序和硬件的一座桥梁，大多数操作系统的内核包括 4 方面的内容。
	
	\subsubsection{时钟管理}
	\colorbox{gray!20}{命题追踪 \enspace 时钟中断服务的内容（2018）}
	
	在计算机的各种部件中，时钟是关键设备。时钟的第一功能是计时，操作系统需要通过时钟管理，向用户提供标准的系统时间。另外，通过时钟中断的管理，可以实现进程的切换。例如，在分时操作系统中采用时间片轮转调度，在实时系统中按截止时间控制运行，在批处\footnote{先弄清楚一个问题，即计算机“指令”和高级语言“代码”是不同的。通常所说的“编写代码”指的是用高级语言（如 C、Java 等）来编写程序。但 CPU 看不懂这些高级语言程序的含义，为了让这些程序能顺利执行，就需要将它们“翻译”成 CPU 能懂的机器语言，即一条条“指令”。所谓执行程序，其实就是 CPU 根据一条条指令来执行一个个具体的操作。}理系统中通过时钟管理来衡量一个作业的运行程度等。因此，系统管理的方方面面无不依赖于时钟。
	
	\subsubsection{中断机制}
	\colorbox{gray!20}{命题追踪 \enspace 中断机制在多道程序设计中的作用（2016）}
	
	引入中断技术的初衷是提高多道程序运行时的 CPU 利用率，使 CPU 可以在 I/O 操作期间执行其他指令。后来逐步得到发展，形成了多种类型，成为操作系统各项操作的基础。例如，键盘或鼠标信息的输入、进程的管理和调度、系统功能的调用、设备驱动、文件访问等，无不依赖于中断机制。可以说，现代操作系统是靠中断驱动的软件。
	
	中断机制中，只有一小部分功能属于内核，它们负责保护和恢复中断现场的信息，转移控制权到相关的处理程序。这样可以减少中断的处理时间，提高系统的并行处理能力。
	
	\subsubsection{原语}
	按层次结构设计的操作系统，底层必然是一些可被调用的公用小程序，它们各自完成一个规定的操作，通常将具有这些特点的程序称为原语（Atomic Operation）。它们的特点如下：
	\begin{enumerate}
		\item 处于操作系统的底层，是最接近硬件的部分。
		\item 这些程序的运行具有原子性，其操作只能一气呵成（出于系统安全性和便于管理考虑）。
		\item 这些程序的运行时间都较短，而且调用频繁。
	\end{enumerate}
	定义原语的直接方法是关中断，让其所有动作不可分割地完成后再打开中断。系统中的设备驱动、CPU 切换、进程通信等功能中的部分操作都可定义为原语，使它们成为内核的组成部分。
	
	\subsubsection{系统控制的数据结构及处理}
	系统中用来登记状态信息的数据结构很多，如作业控制块、进程控制块（PCB）、设备控制块、各类链表、消息队列、缓冲区、空闲区登记表、内存分配表等。为了实现有效的管理，系统需要一些基本的操作，常见的操作有以下 3 种：
	\begin{enumerate}
		\item 进程管理。进程状态管理、进程调度和分派、创建与撤销进程控制块等。
		\item 存储器管理。存储器的空间分配和回收、内存信息保护程序、代码对换程序等。
		\item 设备管理。缓冲区管理、设备分配和回收等。
	\end{enumerate}
	可见，内核态指令实际上包括系统调用类指令和一些针对时钟、中断和原语的操作指令。
	
	\subsection{中断和异常的概念}\footnote{本节的内容较为精简，建议结合《计算机组成原理考研复习指导》中相应小节进行学习。}
	\colorbox{gray!20}{命题追踪 \enspace 用户态切换到内核态的事件分析（2013、2015）}
	
	在操作系统中引入内核态和用户态这两种工作状态后，就需要考虑这两种状态之间如何切换。操作系统内核工作在内核态，而用户程序工作在用户态。系统不允许用户程序实现内核态的功能，而它们又必须使用这些功能。因此，需要在内核态建立一些“门”，以便实现从用户态进入内核态。在实际操作系统中，CPU 运行用户程序时\underline{唯一}能进入这些“门”的途径就是通过中断或异常。发生中断或异常时，运行用户态的 CPU 会立即进入内核态，这是通过硬件实现的（例如，用一个特殊寄存器的一位来表示 CPU 所处的工作状态，0 表示内核态，1 表示用户态。若要进入内核态，则只需将该位置 0 即可）。中断是操作系统中非常重要的一个概念，对一个运行在计算机上的实用操作系统而言，缺少了中断机制，将是不可想象的。原因是，操作系统的发展过程大
	
	\subsubsection{中断和异常的定义}
	\colorbox{gray!20}{命题追踪 \enspace 可能引发中断或异常的指令分析（2013、2015）}
	
	中断（Interruption）也称外中断，是指来自 CPU 执行指令外部的事件，通常用于信息输入/输出（见第 5 章），如设备发出的 I/O 结束中断，表示设备输入/输出处理已经完成。时钟中断，表示一个固定的时间片已到，让处理机处理计时、启动定时运行的任务等。
	
	异常（Exception）也称内中断，是指来自 CPU 执行指令内部的事件，如程序的非法操作码、地址越界、运算溢出、虚存系统的缺页及专门的陷入指令等引起的事件。异常不能被屏蔽，一旦出现，就应立即处理。关于内中断和外中断的联系与区别如图 \ref{fig:t} 所示。
	
	\begin{figure}[h]
		\centering
		\label{fig:t}
		\includegraphics[width=0.3\textwidth]{2722.png}
		\caption{内中断和外中断的联系与区别}
	\end{figure}
	
	\subsubsection{中断和异常的分类}
	\colorbox{gray!20}{命题追踪 \enspace 中断和异常的分类（2016）}
	
	外中断可分为可屏蔽中断和不可屏蔽中断。可屏蔽中断是指通过 INTR 线发出的中断请求，通过改变屏蔽字可以实现多重中断，从而使得中断处理更加灵活。不可屏蔽中断是指通过 NMI 线发出的中断请求，通常是紧急的硬件故障，如电源掉电等。此外，异常也是不能被屏蔽的。
	
	
	异常可分为故障、自陷和终止。故障（Fault）通常是由指令执行引起的异常，如非法操作码、缺页故障、除数为 0、运算溢出等。自陷（Trap，也称陷入）是一种事先安排的“异常”事件，用于在用户态下调用操作系统内核程序，如条件陷阱指令、系统调用指令等。终止（Abort）是指出现了使得 CPU 无法继续执行的硬件故障，如控制器出错、存储器校验错等。故障异常和自陷异常属于软件中断（程序性异常），终止异常和外部中断属于硬件中断。
	
	\subsubsection{中断和异常的处理过程}
	\colorbox{gray!20}{命题追踪 \enspace 中断和异常的处理过程（2015、2020、2024）}
	
	中断和异常处理过程的大致描述如下：当 CPU 在执行用户程序的第 \(i\) 条指令时检测到一个异常事件，或在执行第 \(i\) 条指令后发现一个中断请求信号，则 CPU 打断当前的用户程序，然后转到相应的中断或异常处理程序去执行。若中断或异常处理程序能够解决相应的问题，则在中断或异常处理程序的最后，CPU 通过执行中断或异常返回指令，回到被打断的用户程序的第 \(i\) 条指令或第 \(i + 1\) 条指令继续执行；若中断或异常处理程序发现是不可恢复的致命错误，则终止用户程序。通常情况下，对中断和异常的具体处理过程由操作系统（和驱动程序）完成。
	
	\colorbox{gray!20}{命题追踪 \enspace 中断处理和子程序调用的比较（2012）}
	
	注意区分中断处理和子程序调用：①中断处理程序与被中断的当前程序是相互独立的，它们之间没有确定的关系；子程序与主程序是同一程序的两部分，它们属于主从关系。②通常中断的产生都是随机的；而子程序调用是通过调用指令（CALL）引起的，是由程序设计者事先安排的。③调用子程序的过程完全属于软件处理过程；而中断处理的过程还需要有专门的硬件电路才能实现。④中断处理程序的入口地址可由硬件向量法产生向量地址，再由向量地址找到入口地址；子程序的入口地址是由 CALL 指令中的地址码给出的。⑤调用中断处理程序和子程序都需要保护程序计数器（PC）的内容，前者由中断隐指令完成，后者由 CALL 指令完成（执行 CALL 指令时，处理器先将当前的 PC 值压入栈，再将 PC 设置为被调用子程序的入口地址）。⑥响应中断时，需对同时检测到的多个中断请求进行裁决，而调用子程序时没有这种操作。
	
	\subsection{系统调用}
	\colorbox{gray!20}{命题追踪 \enspace 系统调用的定义及性质（2019、2021、2024）}
	
	系统调用是操作系统提供给应用程序（程序员）使用的接口，可视为一种供应用程序调用的特殊的公共子程序。系统中的各种共享资源都由操作系统统一掌管，因此凡是与共享资源有关的操作（如存储分配、I/O 传输及文件管理等），都必须通过系统调用方式向操作系统提出服务请求，由操作系统代为完成，并将处理结果返回给应用程序。这样，就可以保证系统的稳定性和安全性，防止用户进行非法操作。通常，一个操作系统提供的系统调用命令有几十条乃至上百条之多，每个系统调用都有唯一的系统调用号。这些系统调用按功能大致可分为如下几类。
	
	\colorbox{gray!20}{命题追踪 \enspace 系统调用的功能（2021）}
	\begin{itemize}
		\item 设备管理。完成设备的请求或释放，以及设备启动等功能。
		\item 文件管理。完成文件的读、写、创建及删除等功能。
		\item 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。
		\item 进程通信。完成进程之间的消息传递或信号传递等功能。
		\item 内存管理。完成内存的分配、回收以及获取作业占用内存区大小和起始地址等功能。
	\end{itemize}
	显然，系统调用相关功能涉及系统资源管理、进程管理之类的操作，对整个系统的影响非常
	大，因此系统调用的处理需要由操作系统内核程序负责完成，要运行在内核态。
	
	\colorbox{gray!20}{命题追踪 \enspace 系统调用的处理过程及 CPU 状态的变化（2012、2017、2023）}
	
	\colorbox{gray!20}{命题追踪 \enspace 系统调用处理过程中操作系统负责的任务（2022）}
	
	下面分析系统调用的处理过程：第一步，用户程序首先将系统调用号和所需的参数压入堆栈；接着，调用实际的调用指令，然后执行一个陷入指令，将 CPU 状态从用户态转为内核态，再后由硬件和操作系统内核程序保护被中断进程的现场，将程序计数器（PC）、程序状态字（PSW）及通用寄存器内容等压入堆栈。第二步，分析系统调用类型，转入相应的系统调用处理子程序。在系统中配置了一张系统调用入口表，表中的每个表项都对应一个系统调用，根据系统调用号可以找到该系统调用处理子程序的入口地址。第三步，在系统调用处理子程序执行结束后，恢复被中断的或设置新进程的 CPU 现场，然后返回被中断进程或新进程，继续往下执行。
	
	可以这么理解，用户程序执行“陷入指令”，相当于将 CPU 的使用权主动交给操作系统内核程序（CPU 状态会从用户态进入内核态），之后操作系统内核程序再对系统调用请求做出相应处理。处理完成后，操作系统内核程序又会将 CPU 的使用权还给用户程序（CPU 状态会从内核态回到用户态）。这么设计的目的是：用户程序不能直接执行对系统影响非常大的操作，必须通过系统调用的方式请求操作系统代为执行，以便保证系统的稳定性和安全性。
	
	这样，操作系统的运行环境可以理解为：用户通过操作系统运行上层程序（如系统提供的命令解释程序或用户自编程序），而这个上层程序的运行依赖于操作系统的底层管理程序提供服务支持，当需要管理程序服务时，系统通过硬件中断机制进入内核态，运行管理程序；也可能是程序运行出现异常情况，被动地需要管理程序的服务，此时则通过异常处理进入内核态。管理程序运行结束时，退出中断或异常处理程序，返回到用户程序的断点处继续执行。系统调用的执行过程如图 \ref{fig:tt} 所示。
	
	\begin{figure}[h]
		\centering
		\centering
		\label{fig:tt}
		\includegraphics[width=0.6\textwidth]{xtdy.png}
		\caption{系统调用的执行过程}
	\end{figure}
	
	在操作系统这一层面上，我们关心的是系统内核态和用户态的软件实现与切换，对于硬件层面的具体理解，可以结合“计算机组成原理”课程中有关中断的内容进行学习。
	
	下面列举一些由用户态转向内核态的例子：
	\begin{enumerate}
		\item 用户程序要求操作系统的服务，即系统调用。
		\item 发生一次中断。
		\item 用户程序中产生了一个错误状态。
		\item 用户程序中企图执行一条特权指令。
	\end{enumerate}
	从内核态转向用户态由一条指令实现，这条指令也是特权命令，一般是中断返回指令。
	
	\section{操作系统结构}
	随着操作系统功能的不断增多和代码规模的不断扩大，提供合理的结构，对降低操作系统复杂度、提升操作系统安全与可靠性来说变得尤为重要。
	
	\subsection{分层法}
	分层法是将操作系统分为若干层，底层（层 0）为硬件，顶层（层 \(N\)）为用户接口，每层只能调用紧邻它的低层的功能和服务（单向依赖）。分层的操作系统如图 1.4 所示。
	
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
			\draw[fill=gray!20] (0,0) circle (2.9cm);
			\draw[fill=gray!10] (0,0) circle (2.2cm);
			\draw[fill=white] (0,0) circle (1.5cm);
			\draw[fill=white] (0,0) circle (0.8cm);
			\node at (0,2.5) {层\(N\)  用户接口};
			\node at (0,2) {$\vdots$};
			\node at (0,1) {层1};
			\node at (0,0) {层0  硬件};
		\end{tikzpicture}
		\caption{分层的操作系统}
	\end{figure}
		
		分层法的优点：①便于系统的调试和验证，简化了系统的设计和实现。第 1 层可先调试而无须考虑系统的其他部分，因为它只使用了基本硬件。第 1 层调试完且验证正确之后，就可以调试第 2 层，如此向上。若在调试某层时发现错误，则错误应在这一层上，这是因为它的低层都调试好了。②易扩充和易维护。在系统中增加、修改或替换一层中的模块或整层时，只要不改变相应层间的接口，就不会影响其他层。
		
		分层法的问题：①合理定义各层比较困难。因为依赖关系固定后，往往就显得不够灵活。②效率较差。操作系统每执行一个功能，通常要自上而下地穿越多层，各层之间都有相应的层间通信机制，这无疑增加了额外的开销，导致系统效率降低。
		
		\subsection{模块化}
		模块化是将操作系统按功能划分为若干具有一定独立性的模块。每个模块具有某方面的管理功能，并规定好各模块间的接口，使各模块之间能够通过接口进行通信。还可以进一步将各模块细分为若干具有一定功能的子模块，同样也规定好各子模块之间的接口。这种设计方法被称为模块 - 接口法，图 1.5 所示为由模块、子模块等组成的模块化操作系统结构。
		
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}[
			level 1/.style={sibling distance=6cm}, % 单独加大第一层间距
			level 2/.style={sibling distance=3cm}, % 可选：第二层间距适当缩小
			level distance=2.5cm,
			every node/.style={align=center, draw=black, circle, minimum size=1.8em}
			]
			\node {操作系统}
			child {
				node {进程管理}
				child {node {进程控制}}
				child {node {进程调度}}
			}
			child {
				node {存储器管理}
				child {node {内存分配}}
				child {node {内存保护}}
			}
			child {
				node {文件管理}
				child {node {磁盘管理}}
				child {node {目录管理}}
			};
		\end{tikzpicture}
		\caption{由模块、子模块等组成的模块化操作系统结构}
	\end{figure}
		
		在划分模块时，若将模块划分得太小，则虽然能降低模块本身的复杂性，但会使得模块之间的联系过多，造成系统比较混乱；若模块划分得过大，则又会增加模块内部的复杂性，显然应在两者间进行权衡。此外，在划分模块时，要充分考虑模块的独立性问题，因为模块独立性越高，各模块间的交互就越少，系统的结构也就越清晰。衡量模块的独立性主要有两个标准：
		\begin{itemize}
			\item 内聚性，模块内部各部分间联系的紧密程度。内聚性越高，模块独立性越好。
			\item 耦合度，模块间相互联系和相互影响的程度。耦合度越低，模块独立性越好。
		\end{itemize}
		
		模块化的优点：①提高了操作系统设计的正确性、可理解性和可维护性；②增强了操作系统的可适应性；③加速了操作系统的开发过程。
		
		模块化的缺点：①模块间的接口规定很难满足对接口的实际需求。②各模块设计者齐头并进，每个决定无法建立在上一个已验证的正确决定的基础上，因此无法找到一个可靠的决定顺序。
		
		\subsection{宏内核}
		从操作系统的内核架构来划分，可分为宏内核和微内核。
		
		宏内核，也称单内核或大内核，是指将系统的主要功能模块都作为一个紧密联系的整体运行在内核态，从而为用户程序提供高性能的系统服务。因为各管理模块之间共享信息，能有效利用相互之间的有效特性，所以具有无可比拟的性能优势。
		
		随着体系结构和应用需求的不断发展，需要操作系统提供的服务越来越复杂，操作系统的设计规模急剧增长，操作系统也面临着“软件危机”困境。就像一个人，越胖活动起来就越困难。所以就出现了微内核技术，就是将一些非核心的功能移到用户空间，这种设计带来的好处是方便扩展系统，所有新服务都可以在用户空间增加，内核基本不用去做改动。
		
		从操作系统的发展来看，宏内核获得了绝对的胜利，目前主流的操作系统，如 Windows、Android、iOS、macOS、Linux 等，都是基于宏内核的构架。但也应注意到，微内核和宏内核一直是同步发展的，目前主流的操作系统早已不是当年纯粹的宏内核构架了，而是广泛吸取微内核构架的优点而后揉合而成的混合内核。当今宏内核构架遇到了越来越多的困难和挑战，而微内核的优势似乎越来越明显，尤其是谷歌的 Fuchsia 和华为的鸿蒙 OS，都瞄准了微内核构架。
		
		\subsection{微内核}
		\subsubsection{微内核的基本概念}
		微内核构架，是指将内核中最基本的功能保留在内核，而将那些不需要在内核态执行的功能移到用户态执行，从而降低内核的设计复杂性。那些移出内核的操作系统代码根据分层的原则被划分成若干服务程序，它们的执行相互独立，交互则都借助于微内核进行通信。
		
		微内核结构将操作系统划分为两大部分：微内核和多个服务器。微内核是指精心设计的、能实现操作系统最基本核心功能的小型内核，通常包含：①与硬件处理紧密相关的部分；②一些较基本的功能；③客户和服务器之间的通信。这些部分只是为构建通用操作系统提供一个重要基础，这样就可以确保将内核做得很小。操作系统中的绝大部分功能都放在微内核外的一组服务器（进程）中实现，如用于提供对进程（线程）进行管理的进程（线程）服务器、提供虚拟存储器管理功能的虚拟存储器服务器等，它们都是作为进程来实现的，运行在用户态，客户与服务器之间是借助微内核提供的消息传递机制来实现交互的。图 1.6 展示了单机环境下的客户/服务器模式。
		
		\begin{figure}[h]
			\centering
			\centering
			\label{fig:ttt}
			\includegraphics[width=0.7\textwidth]{4142.png}
			\caption{单机环境下的客户/服务器模式}
		\end{figure}
		
		在微内核结构中，为了实现高可靠性，只有微内核运行在内核态，其余模块都运行在用户态，一个模块中的错误只会使这个模块崩溃，而不会使整个系统崩溃。例如，文件服务代码运行时出了问题，宏内核因为文件服务是运行在内核态的，系统直接就崩溃了。而微内核的文件服务是运行在用户态的，只要将文件服务功能强行停止，然后重启，就可以继续使用，系统不会崩溃。
		
		\subsubsection{微内核的基本功能}
		微内核结构通常利用“机制与策略分离”的原理来构造 OS 结构，将机制部分以及与硬件紧密相关的部分放入微内核。微内核通常具有如下功能：
		\begin{enumerate}
			\item 进程（线程）管理。进程（线程）之间的通信功能是微内核 OS 最基本的功能，此外还有进程的切换、进程的调度，以及多处理机之间的同步等功能，都应放入微内核。举个例子，为实现进程调度功能，需要在进程管理中设置一个或多个进程优先级队列，这部分属于调度功能的机制部分，应将它放入微内核。而对用户进程如何分类，以及优先级的确认方式，则属于策略问题，可将它们放入微内核外的进程管理服务器中。
			\item 低级存储器管理。在微内核中，只配置最基本的低级存储器管理机制，如用于实现将逻辑地址变换为物理地址等的页表机制和地址变换机制，这一部分是依赖于硬件的，因此放入微内核。而实现虚拟存储器管理的策略，则包含应采取何种页面置换算法、采用何种内存分配与回收的策略，应将这部分放在微内核外的存储器管理服务器中。
			\item 中断和陷入处理。微内核 OS 将与硬件紧密相关的一小部分放入微内核，此时微内核的主要功能是捕获所发生的中断和陷入事件，并进行中断响应处理，在识别中断或陷入的事件后，再发送给相关的服务器来处理，所以中断和陷入处理也应放入微内核。
		\end{enumerate}
		
		微内核操作系统将进程管理、存储器管理以及 I/O 管理这些功能一分为二，属于机制的很小一部分放入微内核，而绝大部分放入微内核外的各种服务器实现，大多数服务器都要比微内核大。因此，在采用客户/服务器模式时，能将微内核做得很小。
		
		\subsubsection{微内核的特点}
		\colorbox{gray!20}{命题追踪 \enspace 微内核操作系统的特点（2023）}
		
		微内核结构的主要优点如下所示。
		\begin{enumerate}
			\item 扩展性和灵活性。许多功能从内核中分离出来，当要修改某些功能或增加新功能时，只需在相应的服务器中修改或新增功能，或再增加一个专用的服务器，而无须改动内核代码。
			\item 可靠性和安全性。前面已举例说明。
			\item 可移植性。与 CPU 和 I/O 硬件有关的代码均放在内核中，而其他各种服务器均与硬件平台无关，因而将操作系统移植到另一个平台上所需做的修改是比较小的。
			\item 分布式计算。客户和服务器之间、服务器和服务器之间的通信采用消息传递机制，这就使得微内核系统能很好地支持分布式系统和网络系统。
		\end{enumerate}
		
		微内核结构的主要问题是性能问题，因为需要频繁地在内核态和用户态之间进行切换，操作系统的执行开销偏大。为了改善运行效率，可以将那些频繁使用的系统服务移回内核，从而保证系统性能，但这又会使微内核的容量明显地增大。
		
		虽然宏内核在桌面操作系统中取得了绝对的胜利，但是微内核在实时、工业、航空及军事应用中特别流行，这些领域都是关键任务，需要有高度的可靠性。
		
		\subsection{外核}
		不同于虚拟机克隆真实机器，另一种策略是对资源进行划分，为每个用户分配整个资源的一个子集。例如，某虚拟机可能得到磁盘的 0～1023 盘块，而另一虚拟机得到磁盘的 1024～2047 盘块等。在底层，一种称为外核（exokernel）的程序在内核态中运行。它的任务是为虚拟机分配资源，并检查这些资源使用的安全性，以确保没有机器会使用他人的资源。每个用户的虚拟机可以运行自己的操作系统，但限制只能使用已经申请并且获得分配的那部分资源。
		
		外核机制的优点是减少了资源的“映射层”。在其他设计中，每个虚拟机系统都认为它拥
		有完整的磁盘（或其他资源），这样虚拟机监控程序就必须维护一张表格以重映像磁盘地址，有了外核，这个重映射处理就不需要了。外核只需要记录已分配给各个虚拟机的有关资源即可。这种方法还有一个优点，它将多道程序（在外核内）与用户操作系统代码（在用户空间内）加以分离，而且相应的负载并不重，因为外核所做的只是保持多个虚拟机彼此不发生冲突。
		
		\section{操作系统引导}
		操作系统（如 Windows、Linux 等）是一种程序，程序以数据的形式存放在硬盘中，而硬盘通常分为多个区，一台计算机中又可能有多个或多种外部存储设备。操作系统引导是指计算机利用 CPU 运行特定程序，通过程序识别硬盘，识别硬盘分区，识别硬盘分区上的操作系统，最后通过程序启动操作系统，一环扣一环地完成上述过程。
		
		\colorbox{gray!20}{命题追踪 \enspace 操作系统的引导过程（2021）}
		
		常见操作系统的引导过程如下：
		\begin{enumerate}
			\item 激活 CPU。激活的 CPU 读取 ROM 中的 boot 程序，将指令寄存器置为 BIOS（基本输入/输出系统）的第一条指令，即开始执行 BIOS 的指令。
		
			\colorbox{gray!20}{命题追踪 \enspace 操作系统引导过程中创建的数据结构（2022）}
			
			\item 硬件自检。BIOS 程序在内存最开始的空间构建中断向量表，接下来的 POST（通电自检）过程要用到中断功能。然后进行通电自检，检查硬件是否出现故障。如有故障，主板会发出不同含义的蜂鸣，启动中止；如无故障，屏幕会显示 CPU、内存、硬盘等信息。
			\item 加载带有操作系统的硬盘。通电自检后，BIOS 开始读取 Boot Sequence（通过 CMOS 里保存的启动顺序，或者通过与用户交互的方式），将控制权交给启动顺序排在第一位的存储设备，然后 CPU 将该存储设备引导扇区的内容加载到内存中。
			\item 加载主引导记录（MBR）。硬盘以特定的标识符区分引导硬盘和非引导硬盘。若发现一个存储设备不是可引导盘，就检查下一个存储设备。如无其他启动设备，就会死机。主引导记录 MBR 的作用是告诉 CPU 去硬盘的哪个主分区去找操作系统。
			\item 扫描硬盘分区表，并加载硬盘活动分区。MBR 包含硬盘分区表，硬盘分区表以特定的标识符区分活动分区和非活动分区。主引导记录扫描硬盘分区表，进而识别含有操作系统的硬盘分区（活动分区）。找到硬盘活动分区后，开始加载硬盘活动分区，将控制权交给活动分区。
			\item 加载分区引导记录（PBR）。读取活动分区的第一个扇区，这个扇区称为分区引导记录（PBR），其作用是寻找并激活分区根目录下用于引导操作系统的程序（启动管理器）。
			\item 加载启动管理器。分区引导记录搜索活动分区中的启动管理器，加载启动管理器。
	
		\colorbox{gray!20}{命题追踪 \enspace 操作系统运行的存储器（2013）}
			
			\item 加载操作系统。将操作系统的初始化程序加载到内存中执行。
		\end{enumerate}
		
		\section{虚拟机}
		\subsection{虚拟机的基本概念}
		虚拟机是指利用虚拟化技术，将一台物理机器虚拟化为多台虚拟机器，通过隐藏特定计算平台的实际物理特性，为用户提供抽象的、统一的、模拟的计算环境。有两类虚拟化方法。
		
		\subsubsection{第一类虚拟机管理程序}
		从技术上讲，第一类虚拟机管理程序就像一个操作系统，因为它是唯一一个运行在最高特权级的程序。它在裸机上运行并且具备多道程序功能。虚拟机管理程序向上层提供若干虚拟机，这些虚拟机是裸机硬件的精确复制品。因为每台虚拟机都与裸机相同，所以在不同的虚拟机上可以运行任何不同的操作系统。
		
		
		\begin{figure}[h]
			\centering
			\centering
			\label{fig:tttt}
			\includegraphics[width=0.7\textwidth]{544.png}
			\caption{两类虚拟机管理程序在系统中的位置}
		\end{figure}
		
		虚拟机作为用户态的一个进程运行，不允许执行敏感指令。然而，虚拟机上的操作系统认为自己运行在内核态（实际上不是），称为虚拟内核态。虚拟机中的用户进程认为自己运行在用户态（实际上确实是）。当虚拟机操作系统执行了一条 CPU 处于内核态才允许执行的指令时，会陷入虚拟机管理程序。在支持虚拟化的 CPU 上，虚拟机管理程序检查这条指令是由虚拟机中的操作系统执行的还是由用户程序执行的。若是前者，则虚拟机管理程序将安排这条指令功能的正确执行。否则，虚拟机管理程序将模拟真实硬件面对用户态执行敏感指令时的行为。
		
		在过去不支持虚拟化的 CPU 上，真实硬件不会直接执行虚拟机中的敏感指令，这些敏感指令被转为对虚拟机管理程序的调用，由虚拟机管理程序模拟这些指令的功能。
		
		\subsubsection{第二类虚拟机管理程序}
		图 1.7(b)中显示了第二类虚拟机管理程序。它是一个依赖于 Windows、Linux 等操作系统分配和调度资源的程序，很像一个普通的进程。第二类虚拟机管理程序仍然伪装成具有 CPU 和各种设备的完整计算机。VMware Workstation 是首个 x86 平台上的第二类虚拟机管理程序。
		
		运行在两类虚拟机管理程序上的操作系统都称为客户操作系统。对于第二类虚拟机管理程序，运行在底层硬件上的操作系统称为宿主操作系统。
		
		首次启动时，第二类虚拟机管理程序像一台刚启动的计算机那样运转，期望找到的驱动器可以是虚拟设备。然后将操作系统安装到虚拟磁盘上（其实只是宿主操作系统中的一个文件）。客户操作系统安装完成后，就能启动并运行。
		
		虚拟化在 Web 主机领域很流行。没有虚拟化，服务商只能提供共享托管（不能控制服务器的软件）和独占托管（成本较高）。当服务商提供租用虚拟机时，一台物理服务器就可以运行多个虚拟机，每个虚拟机看起来都是一台完整的服务器，客户可以在虚拟机上安装自己想用的操作系统和软件，但是只需支付较低的费用，这就是市面上常见的“云”主机。
		
		有的教材将第一类虚拟化技术称为裸金属架构，将第二类虚拟化技术称为寄居架构。

	\chapter{进程与线程}
	
	\section*{【考纲内容】}
	
	\begin{enumerate}
		\item 进程与线程
		
		
进程与线程的基本概念；进程/线程的状态与转换
		
线程的实现：内核支持的线程，线程库支持的线程
		
进程与线程的组织与控制
		
进程间通信：共享内存，消息传递，管道，信号
		
		\item CPU 调度与上下文切换
		
		
调度的基本概念；调度的目标：
		
		
调度的实现：调度器/调度程序（scheduler），调度的时机与调度方式（抢占式/非抢占式），闲逛进程，内核级线程与用户级线程调度
		
		
CPU 调度算法
		
多处理器调度
		
上下文及其切换机制
		
		\item 同步与互斥
		
		
同步与互斥的基本概念
		
基本的实现方法：软件方法；硬件方法
		
锁：信号量，条件变量
		
经典同步问题：生产者-消费者问题，读者-写者问题，哲学家进餐问题
		
		\item 死锁
		
		
死锁的基本概念：死锁预防
		
死锁避免：死锁检测和解除
		
	\end{enumerate}
	
	\section*{【复习提示】}
	
	进程管理是操作系统的核心，也是每年必考的重点。其中，进程的概念、进程调度、信号量机制实现同步和互斥、进程死锁等更是重中之重，必须深入掌握。需要注意的是，除选择题外，本章还容易出综合题，其中信号量机制实现同步和互斥、进程调度算法和死锁等都可能命制综合题，如利用信号量进行进程同步就在往年的统考中频繁出现。
	
	\section{进程与线程}
	
	\textbf{在学习本节时，请读者思考以下问题：}
	
	\begin{enumerate}
		\item 为什么要引入进程？
		\item 什么是进程？进程由什么组成？
		\item 进程是如何解决问题的？
	\end{enumerate}
	
	希望读者带着上述问题去学习本节内容，并在学习的过程中多思考，从而更深入地理解本节内容。进程本身是一个比较抽象的概念，它不是实物，看不见、摸不着，初学者在理解进程概念时存在一定困难。在介绍完进程的相关知识后，我们会用比较直观的例子帮助大家理解。
	
	\subsection{进程的概念和特征}
	
	\paragraph{1. 进程的概念}
	
	在多道程序环境下，允许多个程序并发执行，此时它们将失去封闭性，并具有间断性和不可再现性的特征。为此引入了进程（Process）的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发性和共享性（最基本的两个特性）。
	
	为了使参与并发执行的每个程序（含数据）都能独立地运行，必须为之配置一个专门的数据结构，称为进程控制块（Process Control Block, PCB）。系统利用 PCB 来描述进程的基本情况和运行状态，进而控制和管理进程。相应地，由程序段、相关数据段和 PCB 三部分构成了进程实体（也称进程映像）。所谓创建进程，就是创建进程的 PCB；而撤销进程，就是撤销进程的 PCB。
	
	从不同的角度，进程可以有不同的定义，比较典型的定义有：
	\begin{enumerate}
		\item 进程是一个正在执行程序的实例。
		\item 进程是一个程序及其数据从磁盘加载到内存后，在 CPU 上的执行过程。
		\item 进程是一个具有独立功能的程序在一个数据集合上运行的过程。
	\end{enumerate}
	
	引入进程概念后，我们可将传统操作系统中的进程定义为：“进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。”
	
	读者要准确理解这里说的系统资源。它指 CPU、存储器和其他设备服务于某个进程的“时间”，例如将 CPU 资源理解为 CPU 的时间片才是准确的。因为进程是这些资源分配和调度的独立单位，即“时间片”分配的独立单位，这就决定了进程一定是一个动态的、过程性的概念。
	
	\subsection{进程的特征}
	
	进程是由多道程序的并发执行而引出的，它和程序是两个截然不同的概念。程序是静态的，进程是动态的，进程的基本特征是对比单个程序的顺序执行提出的。
	
	\begin{enumerate}
		\item \textbf{动态性}。进程是程序的一次执行，它有着创建、活动、暂停、终止等过程，具有一定的生命周期，是动态地产生、变化和消亡的。动态性是进程最基本的特征。
		\item \textbf{并发性}。指多个进程同存于内存中，能在一段时间内同时运行。引入进程的目的就是使进程能和其他进程并发执行。并发性是进程的重要特征，也是操作系统的重要特征。
		\item \textbf{独立性}。指进程是一个能独立运行、独立获得资源和独立接受调度的基本单位。凡未建立 PCB 的程序，都不能作为一个独立的单位参与运行。
		\item \textbf{异步性}。由于进程的相互制约，使得进程按各自独立的、不可预知的速度向前推进。异步性会导致执行结果的不可再现性，为此在操作系统中必须配置相应的进程同步机制。通常不会直接考查进程有什么特性，所以读者对上面的 4 个特性不必记忆，只求理解。
	\end{enumerate}
	
	\subsection{进程的组成}
	
	进程是一个独立的运行单位，也是操作系统进行资源分配和调度的基本单位。它由以下三部分组成，其中最核心的是进程控制块（PCB）。
	
	\paragraph{1. 进程控制块 (PCB)}
	
	进程创建时，操作系统为它新建一个 PCB，该结构之后常驻内存，任意时刻都可以存取，并在进程结束时删除。
	
	进程执行时，系统通过其 PCB 了解进程的现行状态，以便操作系统对其进行控制和管理；在调度到某个进程时，要根据其 PCB 中保存的 CPU 状态信息，设置该进程恢复运行的现场，并根据其 PCB 中的程序和数据的内存地址，找到其程序和数据；进程在运行过程中，当需要和与之合作的进程实现同步、通信或访问文件时，也需要访问 PCB；当进程由于某种原因暂停运行时，又需将其断点的 CPU 环境保存在 PCB 中。可见，在进程的整个生命期中，系统总是通过进程的 PCB 对进程进行控制的，亦即系统唯有通过进程的 PCB 才能感知到该进程的存在。
	
	表 2.1 是一个 PCB 的实例。PCB 主要包括进程描述信息、进程控制和管理信息、资源分配清单和 CPU 相关信息等。各部分的主要说明如下：
	
	\begin{table}[h]
		\centering
		\caption{PCB 通常包含的内容}
		\label{tab:pcb}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{1}{|c|}{\multirow{2}{*}{进程描述信息}} & \multicolumn{3}{c|}{进程控制和管理信息} \\
			\cline{2-4}
			& 资源分配清单 & 处理机相关信息 & \\
			\hline
			进程标识符 (PID) & 进程当前状态 & 代码段指针 & 通用寄存器值 \\
			用户标识符 (UID) & 进程优先级 & 数据段指针 & 控制寄存器值 \\
			\hline
			& 代码运行入口地址 & 文件描述符 & 标志寄存器值 \\
			\hline
			& 程序的外存地址 & 键盘 & 鼠标 \\
			\hline
			& 进入内存时间 & 文件打开列表 & 输入/输出设备 \\
			\hline
			& CPU 占用时间 & 状态字 & \\
			\hline
			& 信号量使用 & & \\
			\hline
		\end{tabular}
	\end{table}
	
	\begin{enumerate}
		\item \textbf{进程描述信息}。进程标识符：标志各个进程。每个进程都有一个唯一的标识号。用户标识符：进程所归属的用户，用户标识符主要用于共享和保护服务。
		\item \textbf{进程控制和管理信息}。进程当前状态：描述进程的状态信息，作为 CPU 分配调度的依据。进程优先级：描述进程抢占 CPU 的优先级，优先级高的进程可优先获得 CPU。
		\item \textbf{资源分配清单}，也称 CPU 上下文，主要指 CPU 中各寄存器的值。当进程处于执行态时，CPU 的许多信息都在寄存器中。当进程被切换时，CPU 状态信息都必须保存在相应的 PCB 中，以便在该进程重新执行时，能从中断继续执行。
		\item \textbf{处理机相关信息}。进程的外存地址、进入内存时间、CPU 占用时间、信号量使用等。
	\end{enumerate}
	
	在一个系统中，通常存在着许多进程的 PCB，有的处于就绪态，有的处于阻塞态，而且阻塞的原因各不相同。为了方便进程的调度和管理，需要将各个进程的 PCB 用适当的方法组织起来。目前，常用的组织方式有链接方式和索引方式两种。链接方式将同一状态的 PCB 链接成一个队列，不同状态对应不同的队列，也可将处于阻塞态的 PCB 根据其阻塞原因的不同，排成多个阻塞队列。索引方式将同一状态的进程组织在一个索引表中，索引表的表项指向相应的 PCB，不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。
	
	\paragraph{2. 程序段}
	
	程序段就是能被进程调度程序调度到 CPU 执行的程序代码段。注意，程序可被多个进程共享，即多个进程可以运行同一个程序。
	
	
	\begin{figure}[h]
		\centering
		\centering
		\label{fig:ttttt}
		\includegraphics[width=0.7\textwidth]{3144.png}
		\caption{}
	\end{figure}
	
	\subsection{数据段}
	
	一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。
	
	\subsubsection{进程的状态与转换}
	
	进程在其生命周期内，由于系统中各个进程之间的相互制约及系统的运行环境的变化，使得进程的状态也在不断地发生变化。通常进程有以下 5 种状态，前 3 种是进程的基本状态。
	
	\begin{enumerate}
		\item \textbf{运行态}。进程正在 CPU 上运行。在单 CPU 中，每个时刻只有一个进程处于运行态。
		\item \textbf{就绪态}。进程获得了除 CPU 之外的所有资源，一旦得到 CPU，便可立即运行。系统中处于就绪态的进程可能有多个，通常将它们排成一个队列，甚至根据阻塞原因的不同，设置多个阻塞队列。
		\item \textbf{阻塞态}，也称等待态。进程正在等待某一事件而暂停运行，如等待某个资源可用（不包括 CPU）或等待 I/O 完成。即使 CPU 空闲，该进程也不能运行。系统通常将处于阻塞态的进程也排成一个队列。
		\item \textbf{创建态}。进程正在被创建，尚未转到就绪态。创建进程需要多个步骤：首先申请一个空白 PCB，并向 PCB 中填写用于控制和管理进程的信息；然后为该进程分配运行时所必需的资源；最后将该进程转入就绪态并插入就绪队列。但是，若进程所需的资源尚不能得到满足，如内存不足，则创建工作尚未完成，进程此时所处的状态称为创建态。
		\item \textbf{终止态}。进程正从系统中消失，可能是进程正常结束或其他原因退出运行。进程需要结束运行时，系统首先将该进程置为终止态，然后进一步处理资源释放和回收等工作。
	\end{enumerate}
	
	\textbf{区分就绪态和阻塞态}：就绪态是指进程仅缺少 CPU，只要获得 CPU 就立即运行；而阻塞态是指进程需要其他资源（除了 CPU）或等待某一事件。之所以将就绪态和其他资源分开，是因为在分时系统的时间片轮转机制中，每个进程分到的时间片是若干毫秒。也就是说，进程得到 CPU 的时间很短且非常频繁，进程在运行过程中实际上是频繁地转换到就绪态的；而其他资源（如外设）的使用和分配或某一事件的发生（如 I/O 操作的完成）对应的时间相对来说很长，进程转换到阻塞态的次数也相对较少。这样看来，就绪态和阻塞态是进程生命周期中两个完全不同的状态。
	
	\textbf{命题追踪} 引起进程状态转换的事件（2014、2015、2018、2023）
	
	图 2.1 说明了 5 种进程状态的转换，而 3 种基本状态之间的转换如下：

就绪态→运行态：处于就绪态的进程被调度后，获得 CPU 资源（分派 CPU 的时间片），于是进程由就绪态转换为运行态。

运行态→就绪态：处于运行态的进程在时间片用完后，不得不让出 CPU，从而进程由运行态转换为就绪态。此外，在可剥夺的操作系统中，当有更高优先级的进程就绪时，调度程序将正在执行的进程转换为就绪态，让更高优先级的进程执行。

阻塞态→就绪态：进程等待的事件到来时，如 I/O 操作完成或中断结束时，中断处理程序必须将相应进程的状态由阻塞态转换为就绪态。
	
	需要注意的是，一个进程从运行态变为阻塞态是主动的行为，而从阻塞态变为就绪态是被动的行为，需要其他相关进程的协助。
	
	\subsection{进程控制}
	
	进程控制的主要功能是对系统中的所有进程实施有效的管理，它具有创建新进程、撤销已有进程、实现进程状态转换等功能。在操作系统中，一般将进程控制用的程序段称为原语，原语的特点是执行期间不允许中断，它是一个不可分割的基本单位。
	
	\paragraph{1. 进程的创建}
	
	\textbf{命题追踪} 父进程与子进程的关系和特点（2020、2024）
	
	允许一个进程创建另一个进程，此时创建者称为父进程，被创建的进程称为子进程。子进程可以继承父进程所拥有的资源。当子进程终止时，应将其从父进程那里获得的资源还给父进程。
	
	\textbf{命题追踪} 导致创建进程的操作（2010）
	
	在操作系统中，终端用户登录系统、作业调度、系统提供服务、用户程序的应用请求等都会引起进程的创建。操作系统创建一个新进程的过程如下（创建原语）：
	
	\begin{enumerate}
		\item 为新进程分配一个唯一的进程标识号，并申请一个空白 PCB（PCB 是有限的）。若 PCB 申请失败，则创建失败。
		\item 为进程分配其所需的资源，如内存、文件、I/O 设备和 CPU 时间等（在 PCB 中体现）。这些资源是从操作系统获得，或仅从其父进程获得。若资源不足（如内存），则并不是创建失败，而是处于创建态，等待内存资源。
		\item 初始化 PCB，主要包括初始化标志信息、初始化 CPU 状态信息和初始化 CPU 控制信息，以及设置进程的优先级等。
		\item 若进程就绪队列能够接纳新进程，则将新进程插入就绪队列，等待被调度运行。
	\end{enumerate}
	
	\paragraph{2. 进程的终止}
	
	\textbf{命题追踪} 终止进程时的操作（2024）
	
	引起进程终止的事件主要有：① 正常结束，表示进程的任务已完成并准备退出运行。② 异常结束，表示进程在运行时，发生了某种异常事件，使程序无法继续运行，如存储区越界、保护错、非法指令、特权指令、运行超时、算术运算错、I/O 故障等。③ 外界干预，指进程应外界的请求而终止运行，如操作员或操作系统干预、父进程请求和父进程终止。
	
	操作系统终止进程的过程如下（终止原语）：
	
	1. 根据被终止进程的标识符，检索出该进程的 PCB，从中读出该进程的状态。
	
	2. 若被终止进程处于运行态，立即终止该进程的执行，将 CPU 资源分配给其他进程。
	
	3. 若该进程还有子孙进程，则通常需将其所有子孙进程终止（有些系统无此要求）。
	
	4. 将该进程所拥有的全部资源，归还给其父进程，或归还给操作系统。
	
	5. 将该 PCB 从所在队列（链表）中删除。
	
	
	有些系统不允许子进程在父进程终止的情况下存在，对于这类系统，若一个进程终止，则它的所有子进程也终止，这种现象称为级联终止。然而，不是所有操作系统都是这么设计的。
	
	\subsection{进程的阻塞和唤醒}
	
	\textbf{命题追踪} I/O 事件阻塞或唤醒进程的过程（2018、2022、2023）
	
	正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新任务可做等，进程便通过调用阻塞原语（Block），使自己由运行态变为阻塞态。可见，阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得 CPU），才可能将其转为阻塞态。阻塞原语的执行过程如下：
	
	1. 找到将要被阻塞进程的标识号 (PID) 对应的 PCB。
	
	2. 若该进程为运行态，则保护其现场，将其状态转为阻塞态，停止运行。
	
	3. 将该 PCB 插入相应事件的等待队列，将 CPU 资源调度给其他就绪进程。
	
	\textbf{应当注意}，Block 原语和 Wakeup 原语是一对作用刚好相反的原语，必须成对使用。若在一个进程中调用了 Block 原语，则必须在与之合作的或其他相关的进程中安排一条相应的 Wakeup 原语，以便唤醒阻塞进程；否则，阻塞进程将因不能被唤醒而永久地处于阻塞态。
	
	\textbf{命题追踪} 进程唤醒的事件与时机（2014、2019）
	
	当被阻塞进程所期待的事件出现时，如它所期待的 I/O 操作已完成或其所期待的数据已到达，由有关进程（比如，释放该 I/O 设备的进程，或提供数据的进程）调用唤醒原语（Wakeup），将等待该事件的进程唤醒。唤醒原语的执行过程如下：
	
	1. 在该事件的等待队列中找到相应的 PCB。
	
	2. 将其从等待队列中移出，并置其状态为就绪态。
	
	3. 将该 PCB 插入就绪队列，等待调度程序调度。
	
	\subsection{进程的通信}
	
	进程通信是指进程之间的信息交换。PV 操作（见 2.3 节）是低级通信方式，高级通信方式是指以较高的效率传输大量数据的通信方式。高级通信方法主要有以下三类。
	
	\paragraph{1. 共享存储}
	
	在进程的进程之间存在一块可直接访问的共享空间，通过对这片共享空间进行读/写操作实现进程之间的信息交换，如图 2.2 所示。在对共享空间进行读/写操作时，需要使用同步互斥工具（如 P 操作、V 操作）对共享空间的读/写进行控制。共享存储又分为两种：低级方式的共享是基于数据结构的共享；高级方式的共享则是基于存储区的共享。操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具，而数据交换则由用户自己安排读/写指令完成。
	
	注意，进程空间一般都是独立的，进程运行期间一般不能访问其他进程的空间，必须通过特殊的系统调用实现，而进程内的线程是自然共享进程空间的。
	
	简单理解就是，甲和乙中间有一个大布袋，甲和乙交换物品是通过大布袋进行的，甲将物品放在大布袋里，乙拿走。但乙不能直接到甲的手中拿东西，甲也不能直接到乙的手中拿东西。
	
	\paragraph{2. 消息传递}
	
	\subsubsection{消息传递}
	
	若通信的进程之间不存在可直接访问的共享空间，则必须利用操作系统提供的消息传递方法实现进程通信。在消息传递系统中，进程间的数据交换以格式化的消息（Message）为单位。进程通过操作系统提供的发送消息和接收消息两个原语进行数据交换。这种方式隐藏了通信实现细节，简化了通信程序的设计，是当前应用最广泛的进程间通信机制。在微内核操作系统中，微内核与服务器之间的通信就采用了消息传递机制。该机制能很好地支持多用户系统、分布式系统和计算机网络，因此也成为这些领域最主要的通信工具。
	
	\paragraph{1) 直接通信方式。}
	发送进程直接将消息发送给接收进程，并将其挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中取得消息，如图 2.3 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.45\textwidth]{3756.png}
		\caption{消息传递}
		\label{fig:message_passing}
	\end{figure}
	
	\paragraph{2) 间接通信方式。}
	发送进程将消息发送到某个中间实体，接收进程从中间实体取得消息。这种中间实体一般称为信箱。该通信方式广泛应用于计算机网络中。
	
	简单理解就是，甲要将某些事情告诉乙，就要写信，然后通过邮差送给乙。直接通信就是邮差将信直接送到乙的手上；间接通信就是乙家门口有一个邮箱，邮差将信放到邮箱里。
	
	\subsection{管道通信}
	
	\textbf{命题追踪} 管道通信的特点（2014）
	
	管道是一个特殊的共享文件，也称 pipe 文件，数据在管道中是先进先出的。管道通信允许两个进程按生产者-消费者方式进行通信（见图 2.4），只要管道不满，写进程就能向管道的一端写入数据；读进程从管道的另一端读出数据。为了协调双方的通信，管道机制必须提供三方面的协调能力：①互斥，指当一个进程对管道进行读/写操作时，其他进程必须等待；②同步，指写进程向管道写入一定数量的数据后，写进程阻塞，直到读进程取走数据后再将它唤醒；读进程将管道中的数据取空后，读进程阻塞，直到写进程将数据写入后才将其唤醒；③确定对方的存在。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.45\textwidth]{3805.png}
		\caption{管道通信}
		\label{fig:pipe_communication}
	\end{figure}
	
	在 Linux 中，管道是一种使用非常频繁的通信机制。从本质上说，管道也是一种文件，但它又和一般的文件有所不同，管道可以克服使用文件进行通信的两个问题，具体表现如下：
	\begin{enumerate}
		\item 限制管道的大小。管道文件是一个固定大小的缓冲区，在 Linux 中该缓冲区的大小为 4KB。这使得它的大小不像普通文件那样不加检验地增长。使用单个固定缓冲区也会带来问题，比如在写管道时可能变满，这种情况发生时，随后对管道的 write() 调用将被阻塞，等待某些数据被读取，以便腾出足够的空间供 write() 调用。
		\item 读进程也可能工作得比写进程快。当管道内的数据已被读取时，管道变空。当这种情况发生时，一个随后的 read() 调用将被阻塞，等待某些数据的写入。
	\end{enumerate}
	
	\textbf{注意}
	
	管道只能由创建进程所访问，当父进程创建一个管道后，管道是一种特殊文件，子进程会继承父进程的打开文件，因此子进程也继承父进程的管道，并可用它来与父进程进行通信。
	
	\subsection{信号 (Signal)}
	
	信号是一种用于通知进程发生了某个事件的机制。不同的系统事件对应不同的信号类型，每类信号对应一个序号。例如，Linux 定义了 30 种信号，分别用序号 1～30 表示。
	
	在进程的 PCB 中，用至少 $n$ 位向量记录该进程的待处理信号，如 Linux 使用一个 32 位的 int 型变量表示。若给某个进程发送一个信号，则把该类信号对应的位修改为 1。一旦该信号被处理，就把对应的位修改为 0。此外，还用另一个 $n$ 位向量记录被阻塞（被屏蔽）的信号。当某个位为 1 时，表示该位对应的信号类型将被忽略，无须响应。
	
	接下来探讨信号是如何发送的，主要有两种方式：
	\begin{enumerate}
		\item 内核给某个进程发送信号。当内核检测到某个特定的系统事件时，就给进程发送信号。例如，若进程使用非法指令，则内核会给该进程发送 SIGILL 信号（信号序号为 4）。
		\item 一个进程给另一个进程发送信号。进程可以调用 kill 函数，要求内核发送一个信号给目的进程（需要指明接收进程的 PID 和信号的序号）。当然，进程也可给自己发送信号。
	\end{enumerate}
	
	当操作系统把一个进程从内核态切换到用户态时（如系统调用返回时），会检查该进程是否有未被阻塞的待处理信号，若有，则强制进程接收信号，并立即处理信号（若有多个待处理信号，则通常先处理信号序号更小的信号）。信号的处理方式有两种：
	\begin{enumerate}
		\item 执行默认的信号处理程序。操作系统为每类信号预设了默认的信号处理程序。例如，收到 SIGILL 信号的默认操作就是终止进程。
		\item 执行进程定义的信号处理程序。例如，进程可以定义收到 SIGILL 信号时输出 “hello world”，而不是终止进程。
	\end{enumerate}
	
	信号处理程序运行结束后，通常会返回进程的下一条指令继续执行。
	
	\subsection{线程和多线程模型}
	
	\paragraph{1. 线程的基本概念}
	
	引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量；而引入线程（Threads）的目的是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。
	
	线程最直接的理解就是轻量级进程，它是一个基本的 CPU 执行单元，也是程序执行流的最小单位，由线程 ID、程序计数器、寄存器集合和堆栈组成。线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，只拥有一点儿在运行中必不可少的资源，但它可与同属一个进程的其他线程共享进程所拥有的全部资源。一个线程可以创建和撤销另一个线程，同一进程中的多个线程之间可以并发执行。由于线程之间相互制约，致使线程在运行中呈现出间断性。线程也有就绪、阻塞和运行三种基本状态。
	
	引入线程后，进程的内涵发生了改变，进程只作为除 CPU 外的系统资源的分配单元，而线程则作为 CPU 的分配单元。由于一个进程内部有多个线程，若线程的切换发生在同一个进程内部，则只需要很少的时空开销。下面从几个方面对线程和进程进行比较。
	
	\paragraph{2. 线程与进程的比较}
	
	\textbf{命题追踪} 进程和线程的比较（2012）
	
	\subparagraph{1) 调度。}
	在传统的操作系统中，拥有资源和独立调度的基本单位都是进程，每次调度都要进行上下文切换，开销较大。在引入线程的操作系统中，线程是独立调度的基本单位，而线程的切换代价低于进程。在同一进程中，线程的切换不会引起进程切换。但从一个进程中的线程切换到另一个进程中的线程时，则会引起进程切换。
	
	\paragraph{2) 并发性。}
	在引入线程的操作系统中，不仅进程之间可以并发执行，一个进程中的多个线程之间也可并发执行，甚至不同进程中的线程也能并发执行，从而使操作系统具有更好的并发性，提高了系统资源的利用率和系统的吞吐量。
	
	\paragraph{3) 拥有资源。}
	进程是系统中拥有资源的基本单位，而线程不拥有系统资源（仅有一点必不可少、能保证独立运行的资源），但线程可以访问其隶属进程的系统资源，这主要表现在属于同一进程的所有线程都具有相同的地址空间。要知道，若线程也是拥有资源的单位，则切换线程就需要较大的时空开销，线程这个概念的提出就没有意义。
	
	\paragraph{4) 独立性。}
	每个进程都有独立的地址空间和资源，除了共享全局变量，不允许其他进程访问。某个进程中的线程对其他进程不可见。同一进程中的不同线程是为了提高并发性及进行相互之间的合作而创建的，它们共享进程的地址空间和资源。
	
	\paragraph{5) 系统开销。}
	在创建或撤销进程时，系统都要为之分配或回收进程控制块 (PCB) 及其他资源，如内存空间、I/O 设备等。操作系统为此所付出的开销，明显大于创建或撤销线程时的开销。类似地，在进程切换时涉及进程上下文的切换，而线程切换时只需保存和设置少量寄存器内容，开销很小。此外，同一进程内的多个线程共享进程的地址空间，因此这些线程之间的同步与通信非常容易实现，甚至无须操作系统的干预。
	
	\paragraph{6) 支持多处理器系统。}
	对于传统单线程进程，不管有多少个 CPU，进程只能运行在一个 CPU 上。对多线程进程，可将进程中的多个线程分配到多个 CPU 上执行。
	
	\subsection{线程的属性}
	
	\textbf{命题追踪} 线程所拥有资源的特点（2011、2024）
	
	线程是一个轻型实体，它不拥有系统资源，但每个线程都应有一个唯一的标识符和一个线程控制块，线程控制块记录线程执行的寄存器和栈等现场状态。
	
	不同的线程可以执行相同的程序，即同一个服务程序被不同的用户调用时，操作系统将它们创建成不同的线程。
	
	同一进程中的各个线程共享该进程所拥有的资源。
	
	线程是 CPU 的独立调度单位，多个线程是可以并发执行的。在单 CPU 的计算机系统中，各线程可交替地占用 CPU；在多 CPU 的计算机系统中，各线程可同时占用不同的 CPU，若各个 CPU 同时为一个进程内的各线程服务，则可缩短进程的处理时间。
	
	一个线程被创建后，便开始了它的生命周期，直至终止。线程在生命周期内会经历阻塞态、就绪态和运行态等各种状态变化。
	
	为什么线程的提出有利于提高系统并发性？可以这样来理解：由于有了线程，线程切换时，有可能发生进程切换，也有可能不发生进程切换，平均而言每次切换所需的开销就变小了，因此能够让更多的线程参与并发，而不会影响到响应时间等问题。
	
	\subsection{线程的状态与转换}
	
	与进程一样，各线程之间也存在共享资源和相互合作的制约关系，致使线程在运行时也具有间断性。相应地，线程在运行时也具有下面三种基本状态：
	
执行态：线程已获得 CPU 而正在运行。

就绪态：线程已具备各种执行条件，只需再获得 CPU 便可立即执行。

阻塞态：线程在执行中因某事件受阻而处于暂停状态。
	
	线程这三种基本状态之间的转换和进程基本状态之间的转换是一样的。
	
	\subsection{线程的组织与控制}
	
	\subsubsection{(1) 线程控制块}
	
	\textbf{命题追踪} 线程的特点（2019、2024）
	
	与进程类似，系统也为每个线程配置一个线程控制块 TCB，用于记录控制和管理线程的信息。线程控制块通常包括：①线程标识符；②一组寄存器，包括程序计数器、状态寄存器和通用寄存器；③线程运行状态，用于描述线程正处于何种状态；④优先级；⑤线程专有存储区，线程切换时用于保存现场等；⑥堆栈指针，用于过程调用时保存局部变量和返回地址等。
	
	同一进程中的所有线程都能访问进程的地址空间和全局变量。但是，每个线程都拥有自己的堆栈，且互不共享（可以这么理解：线程的堆栈被包含在进程的地址空间内，因此同一进程中的各个线程事实上可以访问彼此的堆栈，但编程规范通常不推荐这么做）。
	
	\subsubsection{(2) 线程的创建}
	
	线程也是具有生命周期的，它由创建而产生，由调度而执行，由终止而消亡。相应地，在操作系统中就有用于创建线程和终止线程的函数（或系统调用）。
	
	用户程序启动时，通常仅有一个称为初始化线程的线程正在执行，其主要功能是用于创建新线程。在创建新线程时，需要利用一个线程创建函数，并提供相应的参数，如指向线程主程序的入口指针、堆栈的大小、线程优先级等。线程创建函数执行完后，将返回一个线程标识符。
	
	\subsubsection{(3) 线程的终止}
	
	当一个线程完成自己的任务后，或线程在运行中出现异常而要被强制终止时，由终止线程调用相应的函数执行终止操作。但是有些线程（主要是系统线程）一旦被建立，便一直运行而不会被终止。通常，线程被终止后并不立即释放它所占有的资源，只有当进程中的其他线程执行了分离函数后，被终止线程才与资源分离，此时的资源才能被其他线程利用。
	
	被终止但尚未释放资源的线程仍可被其他线程调用，以使被终止线程重新恢复运行。
	
	\subsection{线程的实现方式}
	
	\textbf{命题追踪} 两种线程的特点与比较（2019）
	
	线程的实现可以分为两类：用户级线程（User-Level Thread，ULT）和内核级线程（Kernel-Level Thread，KLT）。内核级线程也称内核支持的线程。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.85\textwidth]{4733.png}
		\caption{管道通信}
		\label{fig:tttttt}
	\end{figure}
	
	\subsubsection{(1) 用户级线程（ULT）}
	
	通俗地说，用户级线程就是“从用户视角能看到的线程”。在用户级线程中，有关线程管理（创建、撤销和切换等）的所有工作都由应用程序在用户空间内（用户态）完成，无须操作系统干预，内核意识不到线程的存在。应用程序可以通过使用线程库设计多线程程序。通常，应用程序从单线程开始，在该线程中开始运行，在其运行的任何时刻，可以通过调用线程库中的派生例程创建一个在相同进程中运行的新线程。图 2.5(a) 说明了用户级线程的实现方式。
	
	对于设置了用户级线程的系统，其调度仍然以进程为单位进行，各个进程轮流执行一个时间片。假设进程 A 包含 1 个用户级线程，进程 B 包含 100 个用户级线程，这样，进程 A 中线程的运行时间将是进程 B 中各线程运行时间的 100 倍，因此对线程来说是不公平的。
	
	这种实现方式的优点如下：
	
线程切换不需要转换到内核空间，节省了模式切换的开销。

调度算法可以是进程专用的，不同的进程可根据自身的需要，对自己的线程选择不同的调度算法。

用户级线程的实现与操作系统平台无关，对线程管理的代码是属于用户程序的一部分。
	
	这种实现方式的缺点如下：
	
系统调用的阻塞问题，当线程执行一个系统调用时，不仅该线程被阻塞，进程内的所有线程也都被阻塞。

不能发挥多 CPU 的优势，内核每次分配给一个进程的仅有一个 CPU，因此进程中仅有一个线程能执行。
	
	\subsubsection{(2) 内核级线程（KLT）}
	
	在操作系统中，无论是系统进程还是用户进程，都是在操作系统内核的支持下运行的，与内核紧密相关。内核级线程同样也是在内核的支持下运行的，线程管理的所有工作也是在内核空间（内核态）实现的。操作系统也为每个线程设置一个线程控制块 TCB，内核根据该控制块感知某线程的存在，并对其加以控制。图 2.5(b) 说明了内核级线程的实现方式。
	
	这种实现方式的优点如下：
	
能发挥多 CPU 的优势，内核能同时调度同一进程中的多个线程占用 CPU，也可运行其他进程中的线程。

若进程中的一个线程被阻塞，则内核可以调度该进程中的其他线程占用 CPU。

内核支持线程具有很小的数据结构和堆栈，线程切换比较快、开销小。

内核本身也可采用多线程技术，可以提高系统的执行速度和效率。
	
	这种实现方式的缺点如下：
	
同一进程中的线程切换，需要从用户态转到内核态进行，系统开销较大。这是因为用户进程的线程在用户态运行，而线程调度和管理是在内核实现的。
	
	\subsubsection{(3) 组合方式}
	
	有些系统使用组合方式的多线程实现。在组合实现方式中，内核支持多个内核级线程的建立、调度和管理，同时允许用户程序建立、调度和管理用户级线程。一些内核级线程对应多个用户级线程，这是用户级线程通过时分复用内核级线程实现的。同一进程中的多个线程可以同时在多 CPU 上并行执行，且在阻塞一个线程时不需要将整个进程阻塞，所以组合方式能结合 KLT 和 ULT 的优点，并且克服各自的不足。图 2.5(c) 展示了这种组合实现方式。
	
	在线程实现方式的介绍中，提到了通过线程库来创建和管理线程。线程库（thread library）是为程序员提供创建和管理线程的 API。实现线程库主要的方法有如下两种：
	
在用户空间中提供一个没有内核支持的库。这种库的所有代码和数据结构都位于用户空间中。这意味着，调用库中的一个函数只导致用户空间中的一个本地函数的调用。

实现由操作系统直接支持的内核级的库。对于这种情况，库内的代码和数据结构位于内核空间中。调用库中的一个 API 函数通常会导致对内核的系统调用。
	
	目前使用的三种主要线程库是：POSIX Pthreads、Windows API、Java。Pthreads 作为 POSIX 标准
	
	\subsection{多线程模型}
	
	在同时支持用户级线程和内核级线程的系统中，用户级线程和内核级线程连接方式的不同，形成了下面三种不同的多线程模型。
	
	\begin{enumerate}
		\item \textbf{多对一模型。} 将多个用户级线程映射到一个内核级线程，如图 2.6(a) 所示。每个进程只被分配一个内核级线程，线程的调度和管理在用户空间完成。仅当用户线程需要访问内核时，才将其映射到一个内核级线程上，但每次只允许一个线程进行映射。
		
		
	\textbf{优点：} 线程管理是在用户空间进行的，无须切换到内核态，因此效率比较高。
		
	\textbf{缺点：} 若一个线程在访问内核时发生阻塞，则整个进程都会被阻塞；在任何时刻，只有一个线程能够访问内核，多个线程不能同时在多个 CPU 上运行。
		
		\item \textbf{一对一模型。} 将每个用户级线程映射到一个内核级线程，如图 2.6(b) 所示。每个进程有与用户级线程数量相同的内核级线程，线程切换由内核完成，需要切换到内核态。
		
		
	\textbf{优点：} 当一个线程被阻塞后，允许调度另一个线程运行，所以并发能力较强。
		
	\textbf{缺点：} 每创建一个用户线程，相应地就需要创建一个内核线程，开销较大。
		
		\item \textbf{多对多模型。} 将 $n$ 个用户级线程映射到 $m$ 个内核级线程上，要求 $n \geq m$，如图 2.6(c) 所示。
		
		
	\textbf{特点：} 既克服了多对一模型并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程而开销太大的缺点。此外，还拥有上述两种模型各自的优点。
		
	\end{enumerate}
	
	\subsection{本节小结}
	
	本节开头提出的问题的参考答案如下。
	
	\begin{enumerate}
		\item \textbf{为什么要引入进程？}
		
		在多道程序设计的背景下，进程之间需要共享系统资源，因此会导致各程序在执行过程中出现相互制约的关系，程序的执行会表现出间断性等特征。这些特征都是在程序的执行过程中发生的，是动态的过程，而传统的程序本身是一组指令的集合，是静态的概念，无法描述程序在内存中的执行情况，即无法从程序的字面上看出它何时执行、何时停顿，也无法看出它与其他执行程序的关系，因此，程序这个静态概念已不能如实反映程序并发执行过程的特征。为了深刻描述程序动态执行过程的性质乃至更好地支持和管理多道程序的并发执行，便引入了进程的概念。
		
		\item \textbf{什么是进程？进程由什么组成？}
		
		进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码本身，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。
		
		一个进程实体由程序段、相关数据段和 PCB 三部分构成，其中 PCB 是标志一个进程存在的唯一标识，程序段是进程运行的程序的代码，数据段则存储程序运行过程中相关的一些数据。
		
		\item \textbf{进程是如何解决问题的？}
		
		进程将能够识别程序运行状态的一些变量存放在 PCB 中，通过这些变量系统能够更好地了解进程的状况，并在适当机进行进程的切换，以避免一些资源的浪费，甚至划分为更小的调度单位——线程来提高系统的并发度。
		
		本节主要介绍什么是进程，并围绕这个问题进行一些阐述和讨论，为下一节讨论的内容做铺垫，但之前未学过相关课程的读者可能比较费解，到现在为止对进程这个概念还未形成比较清晰的认识。接下来，我们再用一个比较熟悉的概念来类比进程，以便大家能彻底理解本节的内容到底在讲什么，到底解决了什么问题。
		
		我们用“人的生命历程”来类比进程。首先，人的生命历程一定是一个动态的、过程性的概念，要研究人的生命历程，先要介绍经历这个历程的主体是什么。主体当然是人，相当于经历进程的主体是进程映像，人有自己的身份，相当于进程映像里有 PCB；人生历程会经历好几种状态：出生的时候、弥留的时候、充满斗志的时候、发奋图强的时候及失落的时候，相当于进程有创建、撤销、就绪、运行、阻塞等状态，这几种状态会发生改变，人会充满斗志而转向发奋图强，或者发奋图强获得进步之后又会充满斗志预备下一次发奋图强，发奋图强后遇到阻碍会进入失落状态，然后在别人的开导之下又重新充满斗志。类比进程，会由就绪态转向运行态，运行态转向就绪态，或者运行态转向阻塞态，然后在别的进程帮助下返回就绪态。若我们用“人生历程”这个过程的概念去类比进程，则对进程的理解就更深一层。前面生活化的例子可以帮助我们理解进程的实质，但它毕竟有不严谨的地方。一种较好的方式是，在类比进程和人生历程后，再看一遍前面较为严谨的书面阐述和讨论，这样对知识的掌握会更加准确而全面。
		
		这里再给出一些学习计算机科学知识的建议。学习时，很多同学会陷入一个误区，即只注重对定理、公式的应用，而忽视对基础概念的理解。这是我们从小到大应付考试而培养出的一个毛病，因为熟练应用公式和定理对考试有立竿见影的效果。公式、定理的应用固然重要，但基础概念的理解能让我们透彻地理解一门学科，更利于我们产生兴趣，培养创造性思维。
	\end{enumerate}
	
	\section{CPU 调度}
	
	在学习本节时，请读者思考以下问题：
	\begin{enumerate}
		\item 为什么要进行 CPU 调度？
		\item 调度算法有哪几种？结合第 1 章学习的分时操作系统和实时操作系统，思考哪种调度算法比较适合这两种操作系统。
	\end{enumerate}
	
	希望读者能够在学习调度算法前，先自己思考一些调度算法，在学习的过程中注意将自己的想法与这些经典的算法进行对比，并学会计算一些调度算法的周转时间。
	
	\subsection{调度的概念}
	
	\subsubsection{调度的基本概念}
	
	在多道程序系统中，进程的数量往往多于 CPU 的个数，因此进程争用 CPU 的情况在所难免。CPU 调度是对 CPU 进行分配，即从就绪队列中按照一定的算法（公平、高效的原则）选择一个进程并将 CPU 分配给它运行，以实现进程并发地执行。
	
	CPU 调度是多道程序操作系统的基础，是操作系统设计的核心问题。
	
	\subsubsection{调度的层次}
	
	一个作业从提交开始直到完成，往往要经历以下三级调度，如图 2.7 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{1501.png}
		\caption{CPU 的三级调度}
		\label{fig:cpu_scheduling}
	\end{figure}
	
	\begin{enumerate}
		\item \textbf{高级调度（作业调度）}
		
		按照某种规则从外存处于后备队列的作业中挑选一个（或多个），给它（们）分配内存、I/O 设备等必要的资源，并建立相应的进程，以使它（们）获得竞争 CPU 的权利。简言之，作业调度就是内存与辅存之间的调度。每个作业只调入一次、调出一次。
		
		多道批处理系统中大多配有作业调度，而其他系统中通常不需要配置作业调度。
		
		\item \textbf{中级调度（内存调度）}
		
		引入中级调度的目的是提高内存利用率和系统吞吐量。为此，将那些暂时不能运行的进程调至外存等待，此时进程的状态称为挂起态。当它们已具备运行条件且内存又稍有空闲时，由中级调度来决定将外存上的那些已具备运行条件的挂起进程再重新调入内存，并修改其状态为就绪态，挂在就绪队列上等待。中级调度实际上是存储器管理中的对换功能。
		
		\item \textbf{低级调度（进程调度）}
		
		按照某种算法从就绪队列中选取一个进程，将 CPU 分配给它。进程调度是最基本的一种调度，在各种操作系统中都必须配置这级调度。进程调度的频率很高，一般几十毫秒一次。
	\end{enumerate}
	
	\subsubsection{三级调度的联系}
	
	作业调度从外存的后备队列中选择一批作业进入内存，为它们建立进程，这些进程被送入就绪队列，进程调度从就绪队列中选出一个进程，并将其状态改为运行态，将 CPU 分配给它。中级调度是为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。
	
	1) 作业调度为进程活动做准备，进程调度使进程正常活动起来。
	2) 中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。
	3) 作业调度次数少，中级调度次数略多，进程调度频率最高。
	4) 进程调度是最基本的，不可或缺。
	
	\subsection{调度的实现}
	
	\subsubsection{调度程序（调度器）}
	
	用于调度和分派 CPU 的组件称为调度程序，它通常由三部分组成，如图 2.8 所示。
	
	\subsubsection{调度程序的结构}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{3518.png}
		\caption{调度程序的结构}
		\label{fig:scheduler_structure}
	\end{figure}
	
	调度程序通常由三部分组成：
	
	1. 排队器。将系统中的所有就绪进程按照一定的策略排成一个或多个队列，以便于调度程序选择。每当有一个进程转变为就绪态时，排队器便将它插入相应的就绪队列。
	
	2. 分派器。依据调度程序所选的进程，将其从就绪队列中取出，将 CPU 分配给新进程。
	
	3. 上下文切换器。在对 CPU 进行切换时，会发生两对上下文的切换操作：第一对，将当前进程的上下文保存到其 PCB 中，再装入分派程序的上下文，以便分派程序运行；第二对，移出分派程序的上下文，将新选进程的 CPU 现场信息装入 CPU 的各个相应寄存器。
	
	在上下文切换时，需要执行大量 load 和 store 指令，以保存寄存器的内容，因此会花费较长时间。现在已有硬件实现的方法来减少上下文切换时间。通常采用两组寄存器，其中一组供内核使用，一组供用户使用。这样，上下文切换时，只需改变指针，让其指向当前寄存器组即可。
	
	\subsubsection{调度的时机、切换与过程}
	
	调度程序是操作系统内核程序。请求调度的事件发生后，才可能运行调度程序，调度了新的就绪进程后，才会进行进程切换。理论上这三件事情应该顺序执行，但在实际的操作系统内核程序运行中，若某时刻发生了引起进程调度的因素，则不一定能马上进行调度与切换。
	
	\textbf{命题追踪} 可以进行 CPU 调度的事件或时机（2012、2021）
	
	现代操作系统中，应该进行进程调度与切换的情况如下：
	
	1. 创建新进程后，父进程和子进程都处于就绪态，因此需要决定是运行父进程还是运行子进程，调度程序可以合法地决定其中一个进程先运行。
	2. 进程正常结束或异常终止后，必须从就绪队列中选择某个进程运行。若没有就绪进程，则通常运行一个系统提供的闲逛进程。
	3. 当进程因 I/O 请求、信号量操作或其他原因被阻塞时，必须调度其他进程运行。
	4. 当 I/O 设备准备就绪后，发出 I/O 中断，原先等待 I/O 的进程从阻塞态变为就绪态，此时需要决定是让新的就绪进程投入运行，还是让中断发生时运行的进程继续执行。
	
	此外，在有些系统中，当有更紧急的任务（如更高优先级的进程进入就绪队列）需要处理时，或者当前进程的时间片用完时，也会被强行剥夺 CPU。
	
	进程切换往往在调度完成后立刻发生，它要求保存原进程当前点的现场信息，恢复被调度进程的现场信息。现场切换时，操作系统内核将原进程的现场信息推入当前进程的内核堆栈来保存它们，并更新堆栈指针。内核完成从新进程的内核栈中装入新进程的现场信息、更新当前运行进程空间指针、重设 PC 寄存器等相关工作之后，开始运行新的进程。
	
	不能进行进程的调度与切换的情况如下：
	
	1. 在处理中断的过程中。中断处理过程复杂，在实现上很难做到进程切换，而且中断处理是系统工作的一部分，逻辑上不属于某一进程，不应被剥夺 CPU 资源。
	2. 需要完全屏蔽中断的原子操作过程中。如加锁、解锁、中断现场保护、恢复等原子操作。在原子过程中，连中断都要屏蔽，更不应该进行进程调度与切换。
	
	若在上述过程中发生了引起调度的条件，则不能马上进行调度和切换，应置系统的请求调度标志，直到上述过程结束后才进行相应的调度与切换。
	
	\subsubsection{进程调度的方式}
	
	所谓进程调度方式，是指当某个进程正在 CPU 上执行时，若有某个更为重要或紧迫的进程需要处理，即有优先权更高的进程进入就绪队列，此时应如何分配 CPU。
	
	通常有以下两种进程调度方式：
	
	1. 非抢占调度方式，也称非剥夺方式。是指当一个进程正在 CPU 上执行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在执行的进程继续执行，直到该进程运行完成（如正常结束、异常终止）或发生某种事件（如等待 I/O 操作、在进程通信或同步中执行了 Block 原语）而进入阻塞态时，才将 CPU 分配给其他进程。
	
	非抢占调度方式的优点是实现简单、系统开销小，适用于早期的批处理系统，但它不能用于分时系统和大多数的实时系统。
	
	2. 抢占调度方式，也称剥夺方式。是指当一个进程正在 CPU 上执行时，若有某个更为重要或紧迫的进程需要使用 CPU，则允许调度程序根据某种原则去暂停正在执行的进程，将 CPU 分配给这个更为重要或紧迫的进程。
	
	抢占调度方式对提高系统吞吐率和响应效率都有明显的好处。但“抢占”不是一种任意性行为，必须遵循一定的原则，主要有优先权、短进程优先和时间片原则等。
	
	\subsubsection{闲逛进程}
	
	当进程切换时，若系统中没有就绪进程，则会调度闲逛进程（Idle Process）运行，它的 PID 为 0。若没有其他进程就绪，则该进程就一直运行，并在指令周期后测试中断。闲逛进程的优先级最低，没有就绪进程时才会运行闲逛进程，只要有进程就绪，就会立即让出 CPU。
	
	闲逛进程不需要 CPU 之外的资源，它不会被阻塞。
	
	\subsubsection{两种线程的调度}
	
	1. 用户级线程调度。因为内核并不知道线程的存在，所以内核还是和以前一样，选择一个进程，并给予时间控制。由进程中的调度程序决定哪个线程运行。
	
	2. 内核级线程调度。内核选择一个特定线程运行，通常不用考虑该线程属于哪个进程。对被选择的线程赋予一个时间片，若超过了时间片，则会强制挂起该线程。
	
	用户级线程的线程切换在同一进程中进行，仅需少量的机器指令；内核级线程的线程切换需要完整的上下文切换、修改内存映像、使高速缓存失效，这就导致了若干数量级的延迟。
	
	\subsection{调度的目标}
	
	不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法的特性。为了比较 CPU 调度算法的性能，人们提出了很多评价标准，下面介绍其中主要的几种：
	
	\textbf{命题追踪} 作业执行的相关计算（2012、2016、2018、2019、2023、2024）
	
	1. CPU 利用率。CPU 是计算机系统中最重要和昂贵的资源之一，所以应尽可能使 CPU 保持“忙”状态，使这一资源利用率最高。CPU 利用率的计算方法如下：
	\[
	\text{CPU的利用率} = \frac{\text{CPU有效工作时间}}{\text{CPU有效工作时间} + \text{CPU空闲等待时间}}
	\]
	
	2. 系统吞吐量。表示单位时间内 CPU 完成作业的数量。长作业需要消耗较长的 CPU 时间，因此会降低系统的吞吐量。而对于短作业，需要消耗的 CPU 时间较短，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞叶量产生较大的影响。
	
	3. 周转时间。指从作业提交到作业完成所经历的时间，是作业等待、在就绪队列中排队、在 CPU 上运行及 I/O 操作所花费时间的总和。周转时间的计算方法如下：
	\[
	\text{周转时间} = \text{作业完成时间} - \text{作业提交时间}
	\]
	平均周转时间是指多个作业周转时间的平均值：
	\[
	\text{平均周转时间} = \frac{\text{作业 1 的周转时间} + \cdots + \text{作业 } n \text{ 的周转时间}}{n}
	\]
	带权周转时间是指作业周转时间与作业实际运行时间的比值：
	
	\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
		计算作业完成时间时，要注意 CPU 与设备、设备与设备之间是可以并行的。
	\end{tcolorbox}
	
	平均带权周转时间是指多个作业带权周转时间的平均值：
	\[
	\text{平均带权周转时间} = \frac{\text{作业 1 的带权周转时间} + \cdots + \text{作业 } n \text{ 的带权周转时间}}{n}
	\]
	
	4) 等待时间。指进程处于等待 CPU 的时间之和，等待时间越长，用户满意度越低。CPU 调度算法实际上并不影响作业执行或 I/O 操作的时间，只影响作业在就绪队列中等待所花的时间。因此，衡量一个调度算法的优劣，常常只需简单地考察等待时间。
	
	5) 响应时间。指从用户提交请求到系统首次产生响应所用的时间。在交互式系统中，周转时间不是最好的评价准则，一般采用响应时间作为衡量调度算法的重要准则之一。从用户角度来看，调度策略应尽量降低响应时间，使响应时间处在用户能接受的范围之内。
	
	要想得到一个满足所有用户和系统要求的算法几乎是不可能的。设计调度程序，一方面要满足特定系统用户的要求（如某些实时和交互进程的快速响应要求），另一方面要考虑系统整体效率（如减少整个系统的进程平均周转时间），同时还要考虑调度算法的开销。
	
	\subsection{进程切换}
	
	\textbf{命题追踪} 进程调度前后 CPU 模式的的变化（2023）
	
	对通常的进程而言，其创建、撤销及要求由系统设备完成的 I/O 操作，都是利用系统调用而进入内核，再由内核中的相应处理程序予以完成的。进程切换同样是在内核的支持下实现的，因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。
	
	\paragraph{(1) 上下文切换}
	
	\textbf{命题追踪} 切换进程时的操作（2024）
	
	切换 CPU 到另一个进程需要保存当前进程状态并恢复另一个进程的状态，这个任务称为上下文切换。进程上下文采用进程 PCB 表示，包括 CPU 寄存器的值、进程状态和内存管理信息等。
	
	当进行上下文切换时，内核将旧进程状态保存在其 PCB 中，然后加载经调度而要执行的新进程的上下文。在切换过程中，进程的运行环境产生实质性的变化。上下文切换的流程如下：
	
	\begin{enumerate}
		\item 挂起一个进程，将 CPU 上下文保存到 PCB 中，包括程序计数器和其他寄存器。
		\item 将进程的 PCB 移入相应的队列，如就绪、在某事件阻塞等队列。
		\item 选择另一个进程执行，并更新其 PCB。
		\item 恢复新进程的 CPU 上下文。
		\item 跳转到新进程 PCB 中的程序计数器所指向的位置执行。
	\end{enumerate}
	
	\paragraph{(2) 上下文切换的消耗}
	
	上下文切换通常是计算密集型的，即它需要相当可观的 CPU 时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间，所以上下文切换对系统来说意味着消耗大量的 CPU 时间。有些 CPU 提供多个寄存器组，这样，上下文切换就只需要简单改变当前寄存器组的指针。
	
	\paragraph{(3) 上下文切换与模式切换}
	
	模式切换与上下文切换是不同的，模式切换时，CPU 逻辑上可能还在执行同一进程。用户进程最开始都运行在用户态，若进程因中断或异常进入内核态运行，执行完后又回到用户态刚被中断的进程运行。用户态和内核态之间的切换称为模式切换，而不是上下文切换，因为没有改变当前的进程。上下文切换只能发生在内核态，它是多任务操作系统中的一个必需的特性。
	
	\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
		调度和切换的区别：调度是指决定资源分配给哪个进程的行为，是一种决策行为；切换是指实际分配的行为，是执行行为。一般来说，先有资源的调度，然后才有进程的切换。
	\end{tcolorbox}
	
	\subsection{CPU 调度算法}
	
	\textbf{命题追踪} 各种调度算法的特点与对比（2009、2011、2014）
	
	操作系统中存在多种调度算法，有的调度算法适用于作业调度，有的调度算法适用于进程调度，有的调度算法两者都适用。下面介绍几种常用的调度算法。
	
	\subsubsection{先来先服务 (FCFS) 调度算法}
	
	\textbf{FCFS 调度算法的思想（2017）}
	
	在作业调度中，FCFS 调度算法每次从后备作业队列中选择最先进入该队列的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。
	
	在进程调度中，FCFS 调度算法每次从就绪队列中选择最先进入该队列的进程，将 CPU 分配给它，使之投入运行，直到运行完成或因某种原因此阻塞时才释放 CPU。
	
	\textbf{命题追踪} 批处理系统中作业完成时间的分析（2012、2016）
	
	下面通过一个实例来说明 FCFS 调度算法的性能。假设系统中有 4 个作业，它们的提交时间分别是 8、8.4、8.8、9，运行时间依次是 2、1、0.5、0.2，系统采用 FCFS 调度算法，这组作业的平均等待时间、平均周转时间和平均带权周转时间见表 2.2。
	
	\begin{table}[h]
		\centering
		\caption{FCFS 调度算法的性能}
		\label{tab:fcfs_performance}
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			作业号 & 提交时间 & 运行时间 & 开始时间 & 等待时间 & 完成时间 & 周转时间 & 带权周转时间 \\
			\hline
			1 & 8 & 2 & 8 & 0 & 10 & 2 & 1 \\
			\hline
			2 & 8.4 & 1 & 10 & 1.6 & 11 & 2.6 & 2.6 \\
			\hline
			3 & 8.8 & 0.5 & 11 & 2.2 & 11.5 & 2.7 & 5.4 \\
			\hline
			4 & 9 & 0.2 & 11.5 & 2.5 & 11.7 & 2.7 & 13.5 \\
			\hline
			
		\end{tabular}
	\end{table}
	
	平均等待时间 $t=(0+1.6+2.2+2.5)/4=1.575$; 平均周转时间 $T=(2+2.6+2.7+2.7)/4=2.5$; 
	
	平均带权周转时间 $W=(1+2.6+5.4+13.5)/4=5.625$. 
	
	FCFS 调度算法属于不可剥夺算法。从表面上看，它对所有作业都是公平的，但若一个长作业先到达系统，就会使后面的许多短作业等待很长时间，因此它不能作为分时系统和实时系统的主调度策略。但它常被结合在其他调度策略中使用。例如，在使用优先级作为调度策略的系统中，往往对多个具有相同优先级的进程按 FCFS 原则处理。
	
	\subsubsection{短作业优先 (SJF) 调度算法}
	
	\textbf{命题追踪} SJF 调度算法的思想（2017）
	
	短作业（进程）优先调度算法是指对短作业（进程）优先调度的算法。短作业优先 (SJF) 调度算法从后备队列中选择一个或几个估计运行时间最短的作业，将它们调入内存运行；短进程
	
	\textbf{命题追踪} 饥饿现象的含义（2016）
	
	SJF 算法也存在不容忽视的缺点：
	1) 该算法对长作业不利，由表 2.2 和表 2.3 可知，SJF 调度算法中长作业的周转时间会增加。更严重的是，若有一长作业进入系统的后备队列，由于调度程序总是优先调度那些（即使是后进来的）短作业，将导致长作业长期不被调度，产生饥饿现象（注意区分死锁，后者是系统环形等待，前者是调度策略问题）。
	2) 该算法完全未考虑作业的紧迫程度，因此不能保证紧迫性作业会被及时处理。
	3) 由于作业的长短是根据用户所提供的估计执行时间而定的，而用户又可能有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。
	
	SPF 算法也可以是抢占式的（若未特别说明，则默认为非抢占式）。当一个新进程到达就绪队列时，若其估计执行时间比当前进程的剩余时间小，则立即暂停当前进程，将 CPU 分配给新进程。因此，抢占式 SPF 调度算法也称最短剩余时间优先调度算法。
	
	\begin{table}[h]
		\centering
		\caption{SJF 调度算法的性能}
		\label{tab:sjf_performance}
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			作业号 & 提交时间 & 运行时间 & 开始时间 & 等待时间 & 完成时间 & 周转时间 & 带权周转时间 \\
			\hline
			1 & 8 & 2 & 8 & 0 & 10 & 2 & 1 \\
			\hline
			2 & 8.4 & 1 & 10.7 & 2.3 & 11.7 & 3.3 & 3.3 \\
			\hline
			3 & 8.8 & 0.5 & 10.2 & 1.4 & 10.7 & 1.9 & 3.8 \\
			\hline
			4 & 9 & 0.2 & 10 & 1 & 10.2 & 1.2 & 6 \\
			\hline
			
		\end{tabular}
	\end{table}
	
	平均等待时间 $t=(0+2.3+1.4+1)/4=1.175$; 
	
	平均周转时间 $T=(2+3.3+1.9+1.2)/4=2.1$; 
	
	平均带权周转时间 $W=(1+3.3+3.8+6)/4=3.525$. 
	
	\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
		短作业 (SJF) 调度算法的平均等待时间、平均周转时间是最优的。
	\end{tcolorbox}
	
	\subsubsection{高响应比优先调度算法}
	
	高响应比优先调度算法主要用于作业调度，是对 FCFS 调度算法和 SJF 调度算法的一种综合平衡，同时考虑了每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。
	
	响应比的变化规律可描述为
	\[
	R_{p} = \frac{\text{等待时间} + \text{要求服务时间}}{\text{要求服务时间}}
	\]
	
	根据公式可知：
	1. 作业的等待时间相同时，要求服务时间越短，响应比越高，有利于短作业，因此类似于 SJF。
	2. 要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高，因此类似于 FCFS。
	3. 对于长作业，作业的响应比可以随等待时间的增加而提高，当其等待时间足够长时，也可获得 CPU，克服了“饥饿”现象。
	
	\subsubsection{优先级调度算法}
	
	优先级调度算法既可用于作业调度，又可用于进程调度。该算法中的优先级用于描述作业的紧迫程度。
	
	在作业调度中，优先级调度算法每次从后备作业队列中选择优先级最高的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。在进程调度中，优先级调度算法每次从就绪队列中选择优先级最高的进程，将 CPU 分配给它，使之投入运行。
	
	根据新的更高优先级进程能否抢占正在执行的进程，可将该调度算法分为如下两种：
	
	\textbf{命题追踪} 非抢占式优先级调度算法的应用分析（2018）
	
	1. 非抢占式优先级调度算法。当一个进程正在 CPU 上运行时，即使有某个优先级更高的进程进入就绪队列，仍让正在运行的进程继续运行，直到由于其自身的原因让出 CPU 时（任务完成或等待事件），才将 CPU 分配给就绪队列中优先级最高的进程。
	
	\textbf{命题追踪} 抢占式优先级调度算法的应用分析（2022、2023）
	
	2. 抢占式优先级调度算法。当一个进程正在 CPU 上运行时，若有某个优先级更高的进程进入就绪队列，则立即暂停正在运行的进程，将 CPU 分配给优先级更高的进程。
	
	而根据进程创建后其优先级是否可以改变，可将进程优先级分为以下两种：
	
	\textbf{命题追踪} 静态优先级和动态优先级的分析（2016）
	
	1. 静态优先级。优先级是在创建进程时确定的，且在进程的整个运行期间保持不变。确定静态优先级的主要依据有进程类型、进程对资源的要求、用户要求。优点是简单易行，系统开销小；缺点是不够精确，可能出现优先级低的进程长期得不到调度的情况。
	
	\textbf{命题追踪} 调整进程优先级的合理时机（2010）
	
	2. 动态优先级。创建进程时先赋予进程一个优先级，但优先级会随进程的推进或等待时间的增加而改变，以便获得更好的调度性能。例如，规定优先级随等待时间的增加而提高，于是，对于优先级初值较低的进程，在等待足够长的时间后也可获得 CPU。
	
	一般来说，进程优先级的设置可以参照以下原则：
	
	1. 系统进程 > 用户进程。系统进程作为系统的管理者，理应拥有更高的优先级。
	
	2. 交互型进程 > 非交互型进程（或前台进程 > 后台进程）。大家平时在使用手机时，在前台运行的正在和你交互的进程应该更快速地响应你，因此自然需要被优先处理。
	
	3. I/O 型进程 > 计算型进程。所谓 I/O 型进程，是指那些会频繁使用 I/O 设备的进程，而计算型进程是那些频繁使用 CPU 的进程（很少使用 I/O 设备）。我们知道，I/O 设备（如打印机）的处理速度要比 CPU 慢得多，因此若将 I/O 型进程的优先级设置得更高，就更有可能让 I/O 设备尽早开始工作，进而提升系统的整体效率。
	
	\subsubsection{时间片轮转 (RR) 调度算法}
	
	\textbf{命题追踪} 时间片轮转调度算法的原理（2021、2024）
	
	时间片轮转 (RR) 调度算法主要适用于分时系统。这种算法最大的特点是公平，系统将所有的就绪进程按 FCFS 策略排成一个就绪队列，每隔一定的时间（如 30ms）便产生一次时钟中断，激活调度程序进行调度，将 CPU 分配给就绪队列的队首进程，并令其执行一个时间片。在执行完一个时间片后，即使进程并未运行完成，它也必须释放出 (被剥夺) CPU 给就绪队列的新队首进程，而被剥夺的进程返回到就绪队列的末尾重新排队，等候再次运行。
	
	在 RR 调度算法中，若一个时间片尚未用完而当前进程已运行完成，则调度程序会被立即激活；若一个时间片用完，则产生一个时钟中断，由时钟中断处理程序来激活调度程序。
	
	\textbf{命题追踪} 时间片轮转调度算法的特点（2017）
	
	在 RR 调度算法中，时间片的大小对系统性能的影响很大。若时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则时间片轮转调度算法就退化为先来先服务调度算法。若时间片很小，则 CPU 将在进程间过于频繁地切换，使 CPU 的开销增大，而真正用于运行用户进程的时间将减少。因此，时间片的大小应选择适当，时间片的长短通常由以下因素确定：系统的响应时间、就绪队列中的进程数目和系统的处理能力。
	
	\subsubsection{多级队列调度算法}
	
	前述的各种调度算法，由于系统中仅设置一个进程的就绪队列，即调度算法是固定且单一的，无法满足系统中不同用户对进程调度策略的不同要求。在多 CPU 系统中，这种单一调度策略实现机制的缺点更为突出，多级队列调度算法能在一定程度上弥补这一缺点。
	
	该算法在系统中设置多个就绪队列，将不同类型或性质的进程固定分配到不同的就绪队列。每个队列可实施不同的调度算法，因此，系统针对不同用户进程的需求，很容易提供多种调度策略。同一队列中的进程可以设置不同的优先级，不同的队列本身也可以设置不同的优先级。在多 CPU 系统中，可以很方便为每个 CPU 设置一个单独的就绪队列，每个 CPU 可实施各自的调度策略，这样就能根据用户需求将多个线程分配到一个或多个 CPU 上运行。
	
	\subsubsection{多级反馈队列调度算法（融合了前几种算法的优点）}
	
	\textbf{命题追踪} 多级反馈队列调度算法的应用分析（2019）
	
	多级反馈队列调度算法是时间片轮转调度算法和优先级调度算法的综合与发展，如图 2.9 所示。通过动态调整进程优先级和时间片大小，多级反馈队列调度算法可以兼顾多方面的系统目标。例如，为提高系统吞吐量和缩短平均周转时间而照顾短进程；为获得较好的 I/O 设备利用率和缩短响应时间而照顾 I/O 型进程；同时，也不必事先估计进程的执行时间。
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{5125.png}
		\caption{多级反馈队列调度算法}
		\label{fig:multilevel_feedback_queue}
	\end{figure}
	
	\textbf{命题追踪} 多级反馈队列调度算法的实现思想（2020）
	
	多级反馈队列调度算法的实现思想如下：
	
	(1)设置多个就绪队列，并为每个队列赋予不同的优先级。第 1 级队列的优先级最高，第 2 级队列的优先级次之，其余队列的优先级逐个降低。
	
	(2) 赋予各个队列的时间片大小各不相同。在优先级越高的队列中，每个进程的时间片就越小。例如，第 $i+1$ 级队列的时间片要比第 $i$ 级队列的时间片长 1 倍。
	
	(3) 每个队列都采用 FCFS 算法。新进程进入内存后，首先将它放入第 1 级队列的末尾，按 FCFS 原则等待调度。当轮到该进程执行时，如它能在该时间片内完成，便可撤离系统。若它在一个时间片结束时尚未完成，调度程序将其转入第 2 级队列的末尾等待调度；若它在第 2 级队列中运行一个时间片后仍未完成，再将它放入第 3 级队列，以此类推。当进程最后被降到第 $n$ 级队列后，在第 $n$ 级队列中便采用时间片轮转方式运行。
	
	(4) 按队列优先级调度。仅当第 1 级队列为空时，才调度第 2 级队列中的进程运行；仅当第 $1 \sim i-1$ 级队列均为空时，才会调度第 $i$ 级队列中的进程运行。若 CPU 正在执行第 $i$ 级队列中的某个进程时，又有新进程进入任何一个优先级较高的队列，此时须立即将正在运行的进程放回到第 $i$ 级队列的末尾，而将 CPU 分配给新到的高优先级进程。
	
	多级反馈队列的优势有以下几点：
	\begin{enumerate}
		\item 终端型作业用户：短作业优先。
		\item 短批处理作业用户：周转时间较短。
		\item 长批处理作业用户：经过前面几个队列得到部分执行，不会长期得不到处理。
	\end{enumerate}
	
	\subsubsection{基于公平原则的调度算法}
	
	前面介绍的几种调度算法都只能保证满足要求的进程优先运行，但不能保证进程占用了多少 CPU 时间，也未考虑调度的公平性。本节介绍两种相对公平的调度算法。
	
	\paragraph{(1) 保证调度算法}
	
	保证调度算法向用户做出明确的性能保证，而非优先运行保证。一种很实际且很容易实现的保证是：若系统中有 $n$ 个用户登录，则每个用户都保证获得 $\frac{1}{n}$ 的 CPU 时间；又如，若在单用户系统中有 $n$ 个进程正在运行，则每个进程都保证获得 $\frac{1}{n}$ 的 CPU 时间。
	
	为了实现保证调度算法，系统必须具有下列功能：
	\begin{enumerate}
		\item 跟踪各个进程自创建以来已获得了多少 CPU 时间。
		\item 计算各个进程应获得的 CPU 时间，即自创建以来的时间除以 $n$。
		\item 计算各个进程真正获得的 CPU 时间和应获得的 CPU 时间之比。若比率为 0.5，则说明一个进程只获得了应得时间的一半，而若比率为 2.0，则说明它获得了应得时间的 2 倍。
		\item 调度比率最小的进程持续运行，直到该进程的比率超过最接近它的进程的比率为止。
	\end{enumerate}
	
	\paragraph{(2) 公平分享调度算法}
	
	保证对进程公平，但并不意味着对用户也公平。假设各个用户所拥有的进程数不同，如用户 1 启动 4 个进程而用户 2 只启动 1 个进程，采用 RR 调度，那么对每个进程而言很公平，用户 1 得到 80\% 的 CPU 时间，而用户 2 只得到 20\% 的 CPU 时间，显然对用户 2 有失公平。
	
	公平分享调度算法保证所有用户能获得相同的 CPU 时间，或所要求的时间比例。在这种方式下，不论用户启动多少进程，都能保证每个用户分配到应得的 CPU 份额。例如，系统中有两个用户，用户 1 有 4 个进程 A、B、C 和 D，而用户 2 只有 1 个进程 E，若采用 RR 调度，为保证两个用户能获得相同的 CPU 时间，一个满足条件的调度序列是
	\[ AEBECEDEAEBECEDE \cdots \]
	若用户 1 获得的 CPU 时间是用户 2 的两倍，则可能的调度序列是
	\[ ABECDEABECDE \cdots \]
	
	表 2.4 总结了几种常见进程调度算法的特点，读者要在理解的基础上掌握。
	
	\begin{table}[h]
		\centering
		\caption{几种常见进程调度算法的特点}
		\label{tab:algorithm_features}
		\resizebox{\textwidth}{!}{% 调整为文本宽度，自动缩放
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline
				& 先来先服务 & 短作业优先 & 高响应比优先 & 时间片轮转 & 多级反馈队列 \\
				\hline
				能否可抢占 & 否 & 可以 & 否 & 可以 & 队列内算法不一定 \\
				\hline
				优点 & 公平，实现简单 & 平均等待时间、平均周转时间最优 & 兼顾长短作业 & 兼顾长短作业 & 兼顾长短作业，有更好的响应时间，可行性强 \\
				\hline
				缺点 & 不利于短作业 & 长作业会饥饿，估计时间不易确定 & 计算响应比的开销大 & 平均等待时间较长，上下文切换浪费时间 & 最复杂 \\
				\hline
				适用于 & 无 & 批处理系统 & 无 & 分时系统 & 相当通用 \\
				\hline
			\end{tabular}
		}
	\end{table}
	
	\subsection{多处理机调度}
	
	多处理机系统的调度较单处理机系统复杂，它与系统结构有关。
	
	非对称多处理机 (Asymmetric MultiProcessing, AMP) 大多采用主从式操作系统，内核驻留在主机上，而从机上只运行用户程序，进程调度由主机负责。当从机空闲时，便向主机发送一个索求进程的信号，在主机中有一个就绪队列，只要队列不为空，主机便从队首摘下一个进程分配给索求进程的从机。这种分配方式实现简单，缺点是主机太忙，容易成为系统瓶颈。
	
	对称多处理机 (Symmetric MultiProcessing, SMP) 的所有处理机都是相同的，因此由调度程序将任何一个进程分配给任何一个 CPU。本节主要讨论 SMP 系统的调度问题。
	
	\paragraph{1. 亲和性和负载平衡}
	
	当一个进程从一个 CPU 移到其他 CPU 上时，应将第一个 CPU 的缓存设置为无效，然后重新填充第二个 CPU 的缓存，这种操作的代价较高，因此系统应尽量避免将进程从一个 CPU 移到另一个 CPU，而应试图让一个进程运行在同一 CPU 上，这称为处理器亲和性。
	
	对于 SMP 系统，应尽量保证所有 CPU 的负载平衡（也称负载均衡），以便充分利用多处理机的优点，否则，一个或多个 CPU 会空闲，而其他 CPU 会处于高负载状态，且有一些进程处于等待状态。负载平衡应设法将负载平均分配到 SMP 系统的所有 CPU 上。
	
	然而，负载平衡通常会抵消处理器亲和性带来的好处，保持一个进程运行在同一 CPU 上的好处是可以利用它在该 CPU 的缓存。而将进程从一个 CPU 迁移到另一个 CPU 会失去这个好处。因此，在某些系统中，只有当不平衡达到一定程度后才移动进程。
	
	\paragraph{2. 多处理机调度方案}
	
	\textbf{方案一：公共就绪队列}
	
	系统中仅设置一个公共就绪队列，所有 CPU 共享同一个就绪队列。这种方案很好地实现了负载平衡，因为 CPU 一旦空闲，它就立刻从公共就绪队列中选择一个进程运行。缺点是各进程可能频繁地在不同的 CPU 上运行，处理器亲和性不好。
	
	提升处理器亲和性的方法有两种：软亲和和硬亲和。软亲和是指由调度程序尽量保持一个进程到某个 CPU 上，但这个进程也可以迁移到其他 CPU 上。硬亲和是指由用户进程通过系统调用，主动请求系统分配到固定的 CPU 上。例如，Linux 系统实现了软亲和，也支持硬亲和的系统调用。
	
	\textbf{方案二：私有就绪队列}
	
	系统为每个 CPU 设置一个私有就绪队列，当 CPU 空闲时，就从各自的私有就绪队列中选择一个进程运行。这种方案很好地实现了处理器亲和性，缺点是必须进行负载平衡。
	
	平衡负载的方法通常有两种：推迁移和拉迁移。对于推迁移，一个特定的系统程序周期性检查每个 CPU 的负载，若发现不平衡，则从超载 CPU 的就绪队列中“推”一些进程到空闲 CPU 的就绪队列，从而平均分配负载。若一个 CPU 负载很低，则从超载 CPU 的就绪队列中“拉”一些进程到自己的就绪队列，发生拉迁移。在系统中，推迁移和拉迁移常被并行实现。
	
	\subsubsection{本节小结}
	
	本节开头提出的问题的参考答案如下。
	
	\begin{enumerate}
		\item 为什么要进行 CPU 调度？
		
		若没有 CPU 调度，则意味着要等到当前运行的进程执行完毕后，下一个进程才能执行，而实际情况中，进程时常需要等待一些外部设备的输入，而外部设备的速度与 CPU 相比是非常缓慢的，若让 CPU 总是等待外部设备，则对 CPU 的资源是极大的浪费。而引进 CPU 调度后，可在运行进程等待外部设备时，将 CPU 调度给其他进程，从而提高 CPU 的利用率。用一句简单的话说，就是为了合理地处理计算机的软/硬件资源。
		
		\item 调度算法有哪几种？结合第 1 章学习的分时操作系统和实时操作系统，思考有没有哪种调度算法比较适合这两种操作系统。
		
		本节介绍的调度算法有先来先服务调度、短作业优先调度、优先级调度、高响应比优先调度、时间片轮转调度、多级队列调度、多级反馈队列调度 7 种。
		
		先来先服务算法和短作业优先算法无法保证及时地接收和处理问题，因此无法保证在规定的时间间隔内响应每个用户的需求，也同样无法达到实时操作系统的及时性需求。优先级调度算法按照任务的优先级进行调度，对于更紧急的任务给予更高的优先级，适合实时操作系统。
		
		高响应比优先调度算法、时间片轮转调度算法、多级反馈队列调度算法都能保证每个任务在一定时间内分配到时间片，并轮流占用 CPU，适合分时操作系统。
		
		本节主要介绍了 CPU 调度的概念。操作系统主要管理 CPU、内存、文件、设备几种资源，只要对资源的请求大于资源本身的数量，就会涉及调度。例如，在单处理器系统中，CPU 只有一个，而请求的进程却有多个，因此就需要 CPU 调度。出现调度的概念后，又有了一个问题，即如何调度、应该满足谁、应该让谁等待，这是调度算法所面对的问题，而应该满足谁、应该让谁等待，要遵循一定的准则。调度这一概念贯穿于操作系统的始终，读者在接下来的学习中，将接触到几种资源的调度问题。将它们与 CPU 调度的内容相对比，将发现有异曲同工之妙。
	\end{enumerate}
	
	
	\section{同步与互斥}
	
	在学习本节时，请读者思考以下问题：
	\begin{enumerate}
		\item 为什么要引入进程同步的概念？
		\item 不同的进程之间会存在什么关系？
		\item 当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗？
	\end{enumerate}
	
	用 PV 操作解决进程之间的同步互斥问题是这一节的重点，统考中频繁考查这一内容，请读者务必多加练习，掌握好求解该类问题的方法。
	
	\subsection{同步与互斥的基本概念}
	
	在多道程序环境下，进程是并发执行的，不同进程之间存在着不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。下面举一个简单的例子来帮大家理解这个概念。例如，让系统计算 $1 + 2 \times 3$，假设系统产生两个进程：一个是加法进程，一个是乘法进程。要让计算结果是正确的，一定要让加法进程发生在乘法进程之后，但实际上操作系统具有异步性，若不加以制约，加法进程发生在乘法进程之前是绝对有可能的，因此要制定一定的机制去约束加法进程，让它在乘法进程完成之后才发生，而这种机制就是本节要讨论的内容。
	
	\paragraph{1. 临界资源}
	
	\textbf{命题追踪} 给定代码的同步互斥分析（2016、2021、2023）
	
	虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所用，我们将一次仅允许一个进程使用的资源称为临界资源。许多物理设备都属于临界资源，如打印机等。此外，还有许多变量、数据等都可以被若干进程共享，也属于临界资源。
	
	\textbf{命题追踪} 临界区和临界资源的分析（2024）
	
	对临界资源的访问，必须互斥地进行，在每个进程中，访问临界资源的那段代码称为临界区。为了保证临界资源的正确使用，可将临界资源的访问过程分成 4 个部分：
	\begin{enumerate}
		\item 进入区。为了进入临界区使用临界资源，在进入区要检查可否进入临界区，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。
		\item 临界区。进程中访问临界资源的那段代码，也称临界段。
		\item 退出区。将正在访问临界区的标志清除。
		\item 剩余区。代码中的其余部分。
		\begin{lstlisting}
			while(true){
				entry section;       //进入区
				critical section;    //临界区
				exit section;        //退出区
				remainder section;   //剩余区
			}
		\end{lstlisting}
	\end{enumerate}
	
	\paragraph{2. 同步}
	
	同步亦称直接制约关系，是指为完成某种任务而建立的两个或多个进程，这些进程因为需要协调它们的运行次序而等待、传递信息所产生的制约关系。同步关系源于进程之间的相互合作。
	
	例如，输入进程 A 通过单缓冲向进程 B 提供数据。当该缓冲区空时，进程 B 不能获得所需数据而阻塞，一旦进程 A 将数据送入缓冲区，进程 B 就被唤醒。反之，当缓冲区满时，进程 A 被阻塞，仅当进程 B 取走缓冲数据时，才唤醒进程 A。
	
	\paragraph{3. 互斥}
	
	互斥也称间接制约关系。当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一个进程才允许去访问此临界资源。
	
	例如，在仅有一台打印机的系统中，有两个进程 A 和进程 B，若当进程 A 需要打印时，系统已将打印机分配给进程 B，则进程 A 必须阻塞。一旦进程 B 将打印机释放，系统便将进程 A 唤醒，并将其由阻塞态变为就绪态。
	
	\textbf{命题追踪} 实现临界区互斥必须遵循的准则（2020）
	
	为禁止两个进程同时进入临界区，同步机制应遵循以下准则：
	\begin{enumerate}
		\item 空闲让进。临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区。
		\item 忙则等待。当已有进程进入临界区时，其他试图进入临界区的进程必须等待。
		\item 有限等待。对请求访问的进程，应保证能在有限时间内进入临界区，防止进程无限等待。
		\item 让权等待（原则上应该遵循，但非必须）。当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。
	\end{enumerate}
	
	\subsection{实现临界区互斥的基本方法}
	
	\textbf{命题追踪} 实现互斥的软/硬件方法的特点（2018）
	
	
	\paragraph{1. 软件实现方法}
	
	在进入区设置并检查一些标志来标明是否有进程在临界区中，若已有进程在临界区，则在进入区通过循环检查进行等待，进程离开临界区后则在退出区修改标志。
	
	\subparagraph{(1) 算法一：单标志法}
	
	该算法设置一个公用整型变量 $\text{turn}$，指示允许进入临界区的进程编号，当 $\text{turn}=0$ 时，表示允许 $P_0$ 进入临界区；当 $\text{turn}=1$ 时，表示允许 $P_1$ 进入临界区。进程退出临界区时将临界区的使用权赋予另一个进程，当 $P_i$ 退出临界区时，将 $\text{turn}$ 置为 $j$ ($i=0, j=1$ 或 $i=1, j=0$)。
	
	\begin{lstlisting}[caption={进程 P0}, label={lst:p0}]
		while(turn!=0);
		critical section;
		turn=1;
		remainder section;
	\end{lstlisting}
	
	\begin{lstlisting}[caption={进程 P1}, label={lst:p1}]
		while(turn!=1);
		critical section;
		turn=0;
		remainder section;
	\end{lstlisting}
	
	该算法可以实现每次只允许一个进程进入临界区。但两个进程必须交替进入临界区，若某个进程不再进入临界区，则另一个进程也将无法进入临界区（违背“空闲让进”准则）。这样很容易造成资源利用不充分。若 $P_0$ 顺利进入临界区并从临界区离开，则此时临界区是空闲的，但 $P_1$ 并没有进入临界区的打算，而 $\text{turn}=1$ 一直成立，则 $P_0$ 就无法再次进入临界区。
	
	\subparagraph{(2) 算法二：双标志先检查法}
	
	该算法设置一个布尔型数组 $\text{flag}[2]$，用来标记各个进程想进入临界区的意愿，$\text{flag}[i]=\text{true}$ 表示 $P_i$ 想要进入临界区 ($i=0$ 或 $1$)。$P_i$ 进入临界区前，先检查对方是否想进入临界区，若想，则等待；否则，将 $\text{flag}[i]$ 置为 $\text{true}$ 后，再进入临界区；当 $P_i$ 退出临界区时，将 $\text{flag}[i]$ 置为 $\text{false}$。
	
	\begin{lstlisting}[caption={进程 P0}, label={lst:p0}]
		while(flag[1]);         //1
		flag[0]=true;           //3
		critical section;
		flag[0]=false;
		remainder section;
	\end{lstlisting}
	
	\begin{lstlisting}[caption={进程 P1}, label={lst:p1}]
		while(flag[0]);         //2  //进入区
		flag[1]=true;           //4  //临界区
		critical section;
		flag[1]=false;          //退出区
		remainder section;       //剩余区
	\end{lstlisting}
	
	优点：不用交替进入，可连续使用。缺点：$P_0$ 和 $P_1$ 可能同时进入临界区。按序列①②③④执行时，即检查对方标志后和设置自己的标志前可能发生进程切换，结果双方都检查通过，会同时进入临界区（违背“忙则等待”准则）。原因在于检查和设置操作不是一气呵成的。
	
	\subparagraph{(3) 算法三：双标志后检查法}
	
	算法二先检查对方的标志，再设置自己的标志，但这两个操作又无法一气呵成，于是导致两个进程出现同时进入临界区的问题。因此，想到先设置后检查的方法，以避免上述问题。算法三先设置自己的标志，再检查对方的标志，若对方的标志为 $\text{true}$，则等待；否则，进入临界区。
	
	\subparagraph{(4) 算法四：Peterson 算法}
	
	\textbf{命题追踪} Peterson 算法实现互斥访问问题的分析（2010）
	
	Peterson 算法结合了算法一和算法三的思想，利用 $\text{flag}[]$ 解决互斥访问问题，而利用 $\text{turn}$ 解决“饥饿”问题。若双方都争着进入临界区，则可让进程将进入临界区的机会谦让给对方。即每个进程进入临界区之前，先设置自己的 $\text{flag}$ 标志，再设置允许进入 $\text{turn}$ 标志；之后，再同时检测对方的 $\text{flag}$ 和 $\text{turn}$ 标志，以保证双方同时要求进入临界区时，只允许一个进程进入。
	
	\begin{lstlisting}[caption={进程 P0}, label={lst:p0}]
		flag[0]=true;
		turn=1;
		while(flag[1] && turn==1);
		critical section;
		flag[0]=false;
		remainder section;
	\end{lstlisting}
	
	\begin{lstlisting}[caption={进程 P1}, label={lst:p1}]
		flag[1]=true;
		turn=0;
		while(flag[0] && turn==0);
		critical section;
		flag[1]=false;
		remainder section;
	\end{lstlisting}
	
	为进入临界区，$P_i$ 先将 $\text{flag}[j]$ 置为 $\text{true}$，并将 $\text{turn}$ 置为 $j$，表示优先让对方 $P_j$ 进入临界区 ($i=0, j=1$ 或 $i=1, j=0$)。若双方试图同时进入，则 $\text{turn}$ 几乎同时被置为 $i$ 和 $j$，但只有一个赋值语句的结果会保持，另一个也会执行，但会被立即重写。变量 $\text{turn}$ 的最终值决定了哪个进程被允许先进入临界区，若 $\text{turn}$ 的值为 $i$，则 $P_i$ 进入临界区。当 $P_i$ 退出临界区时，将 $\text{flag}[i]$ 置为 $\text{false}$，以允许 $P_j$ 进入临界区，则 $P_j$ 在 $P_i$ 退出临界区后很快就能进入临界区。若 $P_i$ 不想进入临界区，即 $\text{flag}[i] = \text{false}$ ($P_j$ 不会陷入 while 循环)，则 $P_j$ 就可进入临界区。
	
	由此可见，Peterson 算法很好地遵循了“空闲让进”、“忙则等待”、“有限等待”三个准则，但依然未遵循“让权等待”准则。相比于前三种算法，该算法是最好的，但依然不够好。
	
	\paragraph{2. 硬件实现方法}
	
	理解本节介绍的硬件实现，对学习后面的信号量很有帮助。计算机提供了特殊的硬件指令，允许对一个字的内容进行检测和修正，或对两个字的内容进行交换等。
	
	\subparagraph{(1) 中断屏蔽方法}
	
	\textbf{命题追踪} 关中断指令实现互斥的分析（2021）
	
	当一个进程正在执行它的临界区代码时，防止其他进程进入其临界区的最简单方法是关中断。因为 CPU 只在发生中断时引起进程切换，因此屏蔽中断能够保证当前运行的进程让临界区代码顺利地执行完，进而保证互斥的正确实现，然后执行开中断。其典型模式为
	
	关中断；
	
	临界区；
	
	开中断；
	
	...
	
	这种方法的缺点：①限制了 CPU 交替执行程序的能力，因此系统效率会明显降低。②对内核来说，在它执行更新变量的几条指令期间，关中断是很方便的，但将关中断的权限交给用户则是很不明智的，若一个进程关中断后不再开中断，则系统可能因此终止。③不适用于多处理器系统，因为在多个 CPU 上关中断并不能防止进程在其他 CPU 上执行相同的临界区代码。
	
	\subparagraph{(2) 硬件指令方法——TestAndSet 指令}
	
	借助一条硬件指令——TestAndSet 指令（简称 TS 指令）实现互斥，这条指令是原子操作。其功能是读出指定标志后将该标志设置为真。指令的功能描述如下：
	
	\begin{lstlisting}
	boolean TestAndSet(boolean *lock) {
		boolean old;
		old=*lock;                 //old 用来存放 lock 的旧值
		*lock=true;               //无论之前是否已加锁，都将 lock 置为 true
		return old;                //返回 lock 的旧值
	}
	\end{lstlisting}
	\textbf{命题追踪} TestAndSet 指令实现互斥的分析（2016）
	
	用 TS 指令管理临界区时，为每个临界资源设置一个共享布尔变量 lock ，表示该资源的两种状态：true 表示正被占用（已加锁）；false 表示空闲（未加锁），初值为 false ，因此可将 lock 视为一把锁。进程在进入临界区之前，先用 TS 指令检查 lock 值：①若为 false ，则表示没有进程在临界区，可以进入，并将 lock 置为 true ，这意味着关闭了临界资源（加锁），使任何进程都不能进入临界区；②若为 true ，则表示有进程在临界区，进入循环等待，直到当前访问临界区的进程退出时解锁（将 lock 置为 false ）。利用 TS 指令实现互斥的过程描述如下：
	
	\begin{lstlisting}
	while (TestAndSet(&lock));    //加锁并检查
	进程的临界区代码段;
	lock=false;                    //解锁
	进程的其他代码;
	\end{lstlisting}

	相比于软件实现方法，TS 指令将“加锁”和“检查”操作用硬件的方式变成了一气呵成的原子操作。相比于关中断方法，由于“锁”是共享的，这种方法适用于多处理器系统。缺点是，暂时无法进入临界区的进程会占用 CPU 循环执行 TS 指令，因此不能实现“让权等待”。
	
	\subparagraph{(3) 硬件指令方法——Swap 指令}
	
	Swap 指令的功能是交换两个字（字节）的内容。其功能描述如下：
	\begin{lstlisting}
	Swap(boolean *a, boolean *b){
		boolean temp=*a;
		*a=*b;
		*b=temp;
	}
\end{lstlisting}

	\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
		以上对 TS 和 Swap 指令的描述仅为功能描述，它们由硬件逻辑实现，不会被中断。
	\end{tcolorbox}
	
	\textbf{命题追踪} Swap 指令与函数实现的分析（2023）
	
	用 Swap 指令管理临界区时，为每个临界资源设置一个共享布尔变量 lock ，初值为 false ；在每个进程中再设置一个局部布尔变量 key ，初值为 true ，用于与 lock 交换信息。从逻辑上看，Swap 指令和 TS 指令实现互斥的方法并无太大区别，都是先记录此时临界区是否已加锁（记录在变量 key 中），再将锁标志 lock 置为 true ，最后检查 key ，若 key 为 false ，则说明之前没有其他进程对临界区加锁，于是跳出循环，进入临界区。其处理过程描述如下：
	\begin{lstlisting}
	boolean key=true;
	while(key!=false)
	Swap(&lock, &key);
	进程的临界区代码段;
	lock=false;
	进程的其他代码;
\end{lstlisting}

	用硬件指令方法实现互斥的优点：①简单、容易验证其正确性；②适用于任意数量的进程，支持多处理器系统；③支持系统中有多个临界区，只需为每个临界区设立一个布尔变量。缺点：①等待进入临界区的进程会占用 CPU 执行 while 循环，不能实现“让权等待”；②从等待进程中随机选择一个进程进入临界区，有的进程可能一直选不上，从而导致“饥饿”现象。
	
	无论是软件实现方法还是硬件实现方法，读者都需要理解它的执行过程，特别是软件实现方法。以上的代码实现与我们平时在编译器上写的代码意义不同，以上的代码实现是为了描述进程实现同步和互斥的过程，并不是说计算机内部实现同步互斥的就是这些代码。
	
	\subsection{互斥锁}
	
	解决临界区最简单的工具就是互斥锁（mutex lock）。一个进程在进入临界区时调用 acquire() 函数，以获得锁；在退出临界区时调用 release() 函数，以释放锁。每个互斥锁有一个布尔变量 available ，表示锁是否可用。若锁是可用的，则调用 acquire() 会成功，且锁不再可用。当一个进程试图获取不可用的锁时，会被阻塞，直到锁被释放。其过程描述如下：
	\begin{lstlisting}
	acquire() {           //获得锁的定义
		while (!available)
		;             //忙等待
		available=false;   //获得锁
	}
	release() {           //释放锁的定义
		available=true;    //释放锁
	}
\end{lstlisting}

	acquire() 或 release() 的执行必须是原子操作，因此互斥锁通常采用硬件机制来实现。
	
	上面描述的互斥锁也称自旋锁，其主要缺点是忙等待，当有一个进程在临界区时，任何其他进程在进入临界区前必须连续循环调用 acquire() 。类似的还有前面介绍的单标志法、TS 指令和 Swap 指令。当多个进程共享同一 CPU 时，这种连续循环显然浪费了 CPU 周期。因此，互斥锁通常用于多处理器系统，一个线程可以在一个处理器上旋转，而不影响其他线程的执行。自旋锁的优点是，进程在等待锁期间，没有上下文切换，若上锁的时间较短，则等待代价不高。
	
	\subsection{信号量}
	
	信号量机制是一种功能较强的机制，可用来解决互斥与同步问题，它只能被两个标准的原语 "wait()" 和 "signal()" 访问，也可简写为 "P()" 和 "V()"，或者简称 P 操作和 V 操作。
	
	原语是指完成某种功能且不被分割、不被中断执行的操作序列，通常可由硬件来实现。例如，前述的 TS 指令和 Swap 指令就是由硬件实现的原子操作。原语功能的不被中断执行特性在单处理器上可由软件通过屏蔽中断方法实现。原语之所以不能被中断执行，是因为原语对变量的操作过程若被打断，可能去运行另一个对同一变量的操作过程，从而出现临界段问题。
	
	\textbf{命题追踪} 信号量的含义（2010）
	
	\paragraph{1. 整型信号量}
	
	整型信号量被定义为一个用于表示资源数量的整型量 $S$，相比于普通整型变量，对整型信号量的操作只有三种：初始化、"wait" 操作和 "signal" 操作。"wait" 操作和 "signal" 操作可描述为
	
	\begin{lstlisting}
	wait(S) {
		while(S <= 0);          // 若资源数不够，则一直循环等待
		S = S - 1;              // 若资源数够，则占用一个资源
	}
	signal(S) {
		S = S + 1;              // 使用完后，就释放一个资源
	}
	\end{lstlisting}
	在整型信号量机制中的 wait 操作，只要信号量 S≤0 ，就会不断循环测试。因此，该机制并未遵循“让权等待”的准则，而是使进程处于“忙等”的状态。
	
	\paragraph{2. 记录型信号量}
	
	记录型信号量机制是一种不存在“忙等”现象的进程同步机制。除了需要一个用于代表资源数量的整型变量 value 外，再增加一个进程链表 L，用于链接所有等待该资源的进程。记录型信号量得名于采用了记录型的数据结构。记录型信号量可描述为
	\begin{lstlisting}
	typedef struct {
		int value;
		struct process *L;
	} semaphore;
\end{lstlisting}

	\textbf{命题追踪} wait()操作导致线程状态的变化（2023）
	
	\textbf{命题追踪} 遵循“让权等待”的互斥方法（2018）
	
	相应的 wait(S) 和 signal(S) 的操作如下。
	\begin{lstlisting}
	void wait(semaphore S) {         // 相当于申请资源
		S.value--;
		if (S.value < 0) {
			add this process to S.L;
			block(S.L);
		}
	}
\end{lstlisting}
	对信号量 S 的一次 P 操作，表示进程请求一个该类资源，因此执行 S.value−− ，使系统中可供分配的该类资源数减 1。当 S.value<0 时，表示该类资源已分配完毕，因此应调用 block 原语进行自我阻塞（当前运行的进程：运行态 → 阻塞态），主动放弃 CPU，并插入该类资源的等待队列 S.L ，可见该机制遵循了“让权等待”准则。
	\begin{lstlisting}
	void signal(semaphore S) {       // 相当于释放资源
		S.value++;
		if (S.value <= 0) {
			remove a process P from S.L;
			wakeup(P);
		}
	}
\end{lstlisting}

	对信号量 S 的一次 V 操作，表示进程释放一个该类资源，因此执行 S.value++ ，使系统中可供分配的该类资源数加 1。若加 1 后仍是 S.value≤0 ，则表示仍有进程在等待该类资源，因此应调用 wakeup 原语将 S.L 中的第一个进程唤醒（被唤醒进程：阻塞态 → 就绪态）。
	
	\paragraph{3. 利用信号量实现进程互斥}
	
	\textbf{命题追踪} 利用信号量实现互斥的实现（2024）
	
	为了使多个进程能互斥地访问某个临界资源，需要为该资源设置一个互斥信号量 S ，其初值为 1（可用资源数为 1），然后将各个进程访问该资源的临界区置于 P(S) 和 V(S) 之间。这样，每个要访问该资源的进程在进入临界区之前，都要先对 S 执行 P 操作，若该资源此刻未被访问，则本次 P 操作必然成功，进程便可进入自己的临界区。这时，若再有其他进程也要进入自己的临界区，对 S 执行 P 操作必然失败，因此主动阻塞，从而保证了该资源能被互斥访问。当访问该资源的进程退出临界区后，要对 S 执行 V 操作，以便释放该临界资源。其实现如下：
	\begin{lstlisting}
	semaphore S = 1;                // 初始化信号量，初值为 1
	
	P1() {
		...
		P(S);                       // 准备访问临界资源，加锁
		进程 P1 的临界区；
		V(S);                       // 访问结束，解锁
		...
	}
	
	P2() {
		...
		P(S);                       // 准备访问临界资源，加锁
		进程 P2 的临界区；
		V(S);                       // 访问结束，解锁
		...
	}
\end{lstlisting}

S 的取值范围为 (−1,0,1) 。当 S=1 时，表示两个进程都未进入临界区；当 S=0 时，表示有一个进程已进入临界区；当 S=−1 时，表示有一个进程正在临界区，另一个进程因等待而阻塞在阻塞队列中，需要被当前已在临界区运行的进程退出时唤醒。

\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
	① 对不同的临界资源需要设置不同的互斥信号量。② P(S) 和 V(S) 必须成对出现，缺少 P(S) 就不能保证对临界资源的互斥访问；缺少 V(S) 会使临界资源永远不被释放，从而使因等待该资源而阻塞的进程永远不能被唤醒。③ 考试还可能考查多个资源的问题，有多少资源就将信号量初值设为多少，申请资源时执行 P 操作，释放资源时执行 V 操作。
\end{tcolorbox}

\paragraph{4. 利用信号量实现同步}

\textbf{命题追踪} 利用信号量实现同步（2024）

同步源于进程之间的相互合作，要让本来异步的并发进程相互配合，有序推进。例如，进程 $P_1$
和 $P_2$
并发执行，存在异步性，因此二者交替推进的次序是不确定的。若 $P_2$
的语句 y 要使用 $P_1$

若先执行到 $V(S)$，则执行 $S++$ 后 $S=1$。之后 $P_2$ 执行到 $P(S)$ 时，由于 $S=1$，表示此时有可用资源，执行 $S--$ 后 $S=0$，因此 $P$ 操作中不会执行 $\text{block}$ 原语，而继续往下执行语句 $y$。

若先执行到 $P(S)$，执行 $S--$ 后 $S=-1$，表示此时没有可用资源，因此 $P$ 操作中会执行 $\text{block}$ 原语，$P_2$ 请求阻塞。$P_1$ 的语句 $x$ 执行完后，执行 $V(S)$，执行 $S++$ 后 $S=0$，因此 $V$ 操作中会执行 $\text{wakeup}$ 原语，唤醒在该信号量对应的阻塞队列中的 $P_2$，这样 $P_2$ 就可以继续执行语句 $y$。

PV 操作实现同步互斥的简单总结：在同步问题中，若某个行为会提供某种资源，则在这个行为之后 $V$ 这种资源；若某个行为要用到这种资源，则在这个行为之前 $P$ 这种资源。在互斥问题中，$P$、$V$ 操作要紧夹使用临界资源的那个行为，中间不能有其他冗余代码。

\paragraph{5. 利用信号量实现前驱关系}

\textbf{命题追踪} 信号量实现前驱关系的应用题（2020、2022）

信号量也可用来描述程序或语句之间的前驱关系。图 2.10 给出了一个前驱关系举例，其中 $S_1, S_2, S_3, \dots, S_6$ 是简单的程序段（只有一条语句）。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\textwidth]{2037.png}
	\caption{前驱关系举例}
	\label{fig:precedence_relationship}
\end{figure}

其实，每对前驱关系都是一个同步问题，因此要为每对前驱关系设置一个同步信号量，其初值均为 0。在“前继操作”之后，对相应的同步信号量执行 $V$ 操作，在“后继操作”之前，对相应的同步信号量执行 $P$ 操作。以图 2.10 为例，$S_2$ 是 $S_1$ 的后继，要用到 $S_1$ 的资源；$S_2$ 是 $S_4, S_5$ 的前驱，给 $S_4, S_5$ 提供资源。为保证 $S_1 \to S_2$, $S_1 \to S_3$, $S_2 \to S_4$, $S_2 \to S_5$, $S_3 \to S_6$, $S_4 \to S_6$, $S_5 \to S_6$ 的前驱关系，需分别设置同步信号量 $a_{12}, a_{13}, a_{24}, a_{25}, a_{36}, a_{46}, a_{56}$。其实现如下：

\begin{lstlisting}
semaphore a12=a13=a24=a25=a36=a46=a56=0; // 初始化信号量
S1() {
	...
	V(a12); V(a13);                     // S1 已经运行完成
}
S2() {
	P(a12);
	...
	V(a24); V(a25);                     // S2 已经运行完成
}
S3() {
	P(a13);
	...
	V(a36);                             // S3 已经运行完成
}
S4() {
	P(a24);
	...
	V(a46);                             // S4 已经运行完成
}
S5() {
	P(a25);
	...
	V(a56);                             // S5 已经运行完成
}
S6() {
	P(a36);
	P(a46);
	P(a56);
	...
}
\end{lstlisting}
\paragraph{6. 分析进程同步和互斥问题的方法步骤}

关系分析 。找出问题中的进程数，并分析它们之间的同步和互斥关系。同步、互斥、前驱关系直接按照上面例子中的经典范式改写。

整理思路 。找出解决问题的关键点，并根据做过的题目找出求解的思路。根据进程的操作流程确定 P 操作、V 操作的大致顺序。

设置信号量 。根据上面的两步，设置需要的信号量，确定初值，完善整理。
\subsection{经典同步问题}

\textbf{命题追踪} 程序并发执行的分析（2011、2018）

\textbf{命题追踪} PV 操作的应用题（2009、2011、2013、2014、2015、2017、2019）

\paragraph{1. 生产者-消费者问题}

\textbf{问题描述}：

系统中有一组生产者进程和一组消费者进程，生产者每次生产一个产品并放入缓冲区，消费者每次从缓冲区中取出一个产品并消费。生产者和消费者共享一个初始为空、大小为 n 的缓冲区。只有当缓冲区不满时，生产者才能将产品放入缓冲区；否则必须等待。只有当缓冲区不空时，消费者才能从中取出产品；否则必须等待。缓冲区是临界资源，各进程必须互斥访问。

\textbf{问题分析}：

关系分析 。生产者和消费者对缓冲区的访问是互斥关系，同时生产者和消费者又是一个相互协作的关系，只有生产者生产之后，消费者才能消费，它们也是同步关系。
整理思路 。这里比较简单，只有生产者和消费者两个进程，正好是这两个进程存在着互斥关系和同步关系。那么需要解决的是互斥和同步 PV 操作的位置。

\textbf{信号量设置}：

信号量 mutex=1 ；// 临界区互斥信号量
信号量 empty=n ；// 空闲缓冲区
信号量 full=0 ；// 缓冲区初始化为空

我们对同步互斥问题的介绍是一个循序渐进的过程。上面介绍了一个同步问题的例子和一个互斥问题的例子，下面来看生产者-消费者问题的例子是什么样的。

生产者-消费者进程的描述如下：

\begin{lstlisting}
semaphore mutex=1;     // 临界区互斥信号量
semaphore empty=n;     // 空闲缓冲区
semaphore full=0;      // 缓冲区初始化为空
producer() {           // 生产者进程
	while(1) {
		生产一个产品
		P(empty);      // (要用什么，P 一下)
		P(mutex);      // (互斥夹紧)
		将产品放入缓冲区
		V(mutex);      // (离开临界区，释放互斥信号量)
		V(full); // 满缓冲区数加 1
	}
}
consumer() {
	while(1) {
		P(full);       // 获取满缓冲区单元
		P(mutex);      // 进入临界区
		从缓冲区中取出一个产品
		V(mutex);      // 离开临界区，释放互斥信号量
		V(empty);      // 空缓冲区数加 1
		消费产品
	}
}
\end{lstlisting}

该类问题要注意对缓冲区大小为 n 的处理，当缓冲区中有空时，便可对 empty 变量执行 P 操作，一旦取走一个产品便要执行 V 操作以释放空闲区。对 empty 和 full 变量的 P 操作必须放在对 mutex 的 P 操作之前。若生产者进程先执行 P(mutex) ，然后执行 P(empty) ，消费者进程执行 P(mutex) ，然后执行 P(full) ，这样可不可以？答案是否定的。设想生产者进程已将缓冲区放满，消费者进程并没有取产品，即 empty=0 ，当下次仍然是生产者进程运行时，它先执行 P(mutex) 封锁信号量，再执行 P(empty) 时将被阻塞，希望消费者取出产品后将其唤醒。轮到消费者进程运行时，它先执行 P(mutex) ，然而生产者进程已经封锁 mutex 信号量，消费者进程也会被阻塞，这样一来生产者、消费者进程都将阻塞，都指望对方唤醒自己，因此陷入了无休止的等待。同理，若消费者进程已将缓冲区取空，即 full=0 ，下次若还是消费者先运行，也会出现类似的死锁。不过生产者释放信号量时，mutex 、full 先释放哪一个无所谓，消费者先释放 mutex 或 empty 都可以。

其实生产者-消费者问题只是一个同步互斥问题的综合而已。

下面再看一个较为复杂的生产者-消费者问题。

\textbf{问题描述}：

桌子上有一个盘子，每次只能向其中放入一个水果。爸爸专向盘子中放苹果，妈妈专向盘子中放橘子，儿子专等吃盘子中的橘子，女儿专等吃盘子中的苹果。只有盘子为空时，爸爸或妈妈才可向盘子中放一个水果；仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。

\textbf{问题分析}：

关系分析 。由每次只能向盘中放一个水果可知，爸爸和妈妈是互斥关系。爸爸和女儿、妈妈和儿子是同步关系，而且这两对进程必须连起来，儿子和女儿之间没有互斥和同步关系，因为他们是选择条件执行，不可能并发。进程之间的关系如图 2.11 所示。
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{3234.png}
	\caption{进程之间的关系}
	\label{fig:process_relationships}
\end{figure}

整理思路 。这里有 4 个进程，实际上可抽象为两个生产者和两个消费者被连接到大小为 1 的缓冲区上。
信号量设置 。首先将信号量 plate 设置互斥信号量，表示是否允许向盘子放入水果，初值为 1 表示允许放入，且只允许放入一个。信号量 apple 表示盘子中是否有苹果，初值为 0 表示盘子为空，不许取，apple=1 表示可以取。信号量 orange 表示盘子中是否有橘子，初值为 0 表示盘子为空，不许取，orange=1 表示可以取。
解决该问题的代码如下：
\begin{lstlisting}
semaphore plate=1, apple=0, orange=0;
dad() {           // 爸爸进程
	while(1) {
		准备一个苹果
		P(plate);      // 互斥向盘中取、放水果
		把苹果放入盘子
		V(apple);      // 允许取苹果
	}
}
mom() {           // 妈妈进程
	while(1) {
		准备一个橘子
		P(plate);      // 互斥向盘中取、放水果
		把橘子放入盘子
		V(orange);     // 允许取橘子
	}
}
son() {           // 儿子进程
	while(1) {
		P(orange);     // 互斥向盘中取橘子
		从盘子中取出橘子
		V(plate);      // 允许向盘中取、放水果
		吃掉橘子
	}
}
daughter() {      // 女儿进程
	while(1) {
		P(apple);      // 互斥向盘中取苹果
		从盘子中取出苹果
		V(plate);      // 允许向盘中取、放水果
		吃掉苹果
	}
}
\end{lstlisting}

进程间的关系如图 2.11 所示。dad() 和 daughter() 、mom() 和 son() 必须连续执行，正因为如此，也只能在女儿拿走苹果后或儿子拿走橘子后才能释放盘子，即 V(plate) 操作。

\paragraph{2. 读者-写者问题}

\textbf{问题描述}：

有读者和写者两组并发进程，共享一个文件。当两个或以上的读进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。因此要求：① 允许多个读者可以同时对文件执行读操作；② 只允许一个写者往文件中写信息；③ 任意一个写者在完成写操作之前不允许其他读者或写者工作；④ 写者执行写操作前，应让已有的读者和写者全部退出。

\textbf{问题分析}：

关系分析 。由题目分析读者和写者是互斥的，写者和写者也是互斥的，而读者和读者不存在互斥问题。
整理思路 。两个进程，即读者和写者。写者是比较简单的，它和任何进程互斥，用互斥的 P 操作、V 操作即可解决。读者的问题比较复杂，它必须在实现与写者互斥的同时，实现与其他读者的同步，因此简单的一对 P 操作、V 操作是无法解决问题的。这里用到了一个计数器，用它来判断当前是否有读者读文件。当有读者时，写者是无法写文件的，此时读者会一直占用文件，当没有读者时，写者才可以写文件。同时，这里不同读者对计数器的访问也应该是互斥的。

信号量设置 。首先设置信号量 count 为计数器，用于记录当前读者的数量，初值为 0；设置 mutex 为互斥信号量，用于保护更新 count 变量时的互斥；设置互斥信号量 rw ，用于保证读者和写者的互斥访问。
代码如下：

\begin{lstlisting}
int count=0;
semaphore mutex=1;          // 用于保护更新 count 变量时的互斥
semaphore rw=1;             // 用于保证读者和写者的互斥访问
writer() {
	while(1) {
		P(rw);              // 互斥访问共享文件
		写文件
		V(rw);              // 释放共享文件
	}
}
reader() {
	while(1) {
		P(mutex);           // 互斥访问 count 变量
		if(count==0)
		P(rw);          // 阻止写进程写
		count++;            // 读者计数器加 1
		V(mutex);           // 释放互斥变量 count
		读文件
		P(mutex);           // 互斥访问 count 变量
		count--;            // 读者计数器减 1
		if(count==0)
		V(rw);          // 允许写进程写
		V(mutex);           // 释放互斥变量 count
	}
}
\end{lstlisting}
在上面的算法中，读进程是优先的，即当存在读进程时，写操作将被延迟，且只要有一个读
过程活跃，随后而来的读进程都将被允许访问文件。这样的方式会导致写进程可能长时间等待，且存在写进程“饿死”的情况。

若希望写进程优先，即当有读进程正在读共享文件时，有写进程请求访问，这时应禁止后续读进程的请求，等到已在共享文件的读进程执行完毕，立即让写进程执行，只有在无写进程执行的情况下才允许读进程再次运行。为此，增加一个信号量并在上面程序的 $\text{writer()}$ 和 $\text{reader()}$ 函数中各增加一对 $P$ 操作、$V$ 操作，就可以得到写进程优先的解决方案。
\begin{lstlisting}
	int count=0;               // 用于记录当前的读者数量
	semaphore mutex=1;         // 用于保护更新 count 变量时的互斥
	semaphore rw=1;            // 用于保证读者和写者的互斥访问
	semaphore w=1;             // 用于实现“写优先”
	writer() {
		while(1) {
			P(w);              // 在无写进程请求时进入
			P(rw);             // 互斥访问共享文件
			写文件
			V(rw);             // 释放共享文件
			V(w);              // 恢复对共享文件的访问
		}
	}
	reader() {
		while(1) {
			P(w);              // 在无写进程请求时进入
			P(mutex);          // 互斥访问 count 变量
			if(count==0)
			P(rw);         // 阻止写进程写
			count++;           // 读者计数器加 1
			V(mutex);          // 释放互斥变量 count
			读文件
			P(mutex);          // 互斥访问 count 变量
			count--;           // 读者计数器减 1
			if(count==0)
			V(rw);         // 允许写进程写
			V(mutex);          // 释放互斥变量 count
		}
	}
\end{lstlisting}
这里的写进程优先是相对而言的，有些书上将这个算法称为读/写公平法，即读/写进程具有一样的优先级。当一个写进程访问文件时，若先有一些读进程要求访问文件，后有另一个写进程要求访问文件，则当前访问文件的进程结束对文件的写操作时，会是一个读进程而不是一个写进程占用文件（在信号量 w 的阻塞队列上，因为读进程先来，因此排在阻塞队列首，而 V 操作唤醒进程时唤醒的是队首进程），所以说这里的写优先是相对的，想要了解如何做到真正写者优先，可参考其他相关资料。

读者-写者问题有一个关键的特征，即有一个互斥访问的计数器 count ，因此遇到一个不太好解决的同步互斥问题时，要想一想用互斥访问的计数器 count 能否解决问题。

\paragraph{3. 哲学家进餐问题}

\textbf{问题描述}：

一张圆桌边上坐着 5 名哲学家，每两名哲学家之间的桌上摆一根筷子，两根筷子中间是一碗米饭，如图 2.12 所示。哲学家们倾注毕生精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子（一根一根地拿起）。若筷子已在他人手上，则需要等待。饥饿的哲学家只有同时拿到了两根筷子才可以开始进餐，进餐完毕后，放下筷子继续思考。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.2\textwidth]{3450.png}
	\caption{5 名哲学家进餐}
	\label{fig:philosophers_dining}
\end{figure}

\textbf{问题分析}：

关系分析 。5 名哲学家与左右邻居对其中间筷子的访问是互斥关系。

整理思路 。显然，这里有 5 个进程。本题的关键是如何让一名哲学家拿到左右两根筷子而不造成死锁或饥饿现象。解决方法有两个：一是让他们同时拿两根筷子；二是对每名哲学家的动作制定规则，避免饥饿或死锁现象的发生。

信号量设置 。定义互斥信号量数组 chopstick[5]={1,1,1,1,1} ，用于对 5 个筷子的互斥访问。哲学家按顺序编号为 0∼4 ，哲学家 i 左边筷子的编号为 i ，哲学家右边筷子的编号为 (i+1)\%5 。

\begin{lstlisting}
	semaphore chopstick[5] = {1, 1, 1, 1, 1}; // 初始化信号量数组 chopstick[5]
	Pi() {
		do {
			P(chopstick[i]);       // 取左边筷子
			P(chopstick[(i+1)%5]); // 取右边筷子
			进餐
			V(chopstick[i]);       // 放回左边筷子
			V(chopstick[(i+1)%5]); // 放回右边筷子
			思考
		} while(1);
	}
\end{lstlisting}

该算法存在以下问题：当 5 名哲学家都想要进餐并分别拿起左边的筷子时（都恰好执行完 wait(chopstick[i]) ），筷子已被拿光，等到他们再想拿右边的筷子时（执行 wait(chopstick[(i+1)\%5]) ）就全被阻塞，因此出现了死锁。

为防止死锁发生，可对哲学家进程施加一些限制条件，比如：① 至多允许 4 名哲学家同时进餐，以保证至少有一名哲学家能拿到左右两边的筷子；② 仅当一名哲学家左右两边的筷子都可用时，才允许他拿起筷子；③ 对哲学家顺序编号，要求奇数号哲学家先拿左边的筷子，然后拿右边的筷子，而偶数号哲学家刚好相反，以保证相邻的两名哲学家都想进餐时，只有一名哲学家可以拿起第一根筷子，而另一名哲学家会被阻塞。

制定的正确规则如下：假设采用第二种方法，当一名哲学家左右两边的筷子都可用时，才允许他抓起筷子。

\begin{lstlisting}
semaphore chopstick[5] = {1, 1, 1, 1, 1}; // 初始化信号量
semaphore mutex=1;                         // 设置取筷子的信号量
Pi() {
	do {
		P(mutex);                          // 在取筷子前获得互斥量
		P(chopstick[i]);                   // 取左边筷子
		P(chopstick[(i+1)%5]);             // 取右边筷子
		V(mutex);                          // 释放取筷子的信号量
		进餐
		V(chopstick[i]);                   // 放回左边筷子
		V(chopstick[(i+1)%5]);             // 放回右边筷子
		思考
	} while(1);
}
\end{lstlisting}

熟悉 ACM 或有过相关训练的读者都应知道贪心算法，哲学家进餐问题的思想其实与贪心算法的思想截然相反。贪心算法强调争取眼前认为最好的，而不考虑后续会有什么后果。若哲学家进餐问题用贪心算法来解决，即只要眼前有筷子能拿起就拿起的话，就会出现死锁。然而，若不仅考虑眼前的一步，还考虑下一步，即不因为有筷子能拿起就拿起，而考虑能不能一次拿起两根筷子才做决定的话，就会避免死锁问题，这就是哲学家进餐问题的思维精髓。
大部分习题和真题用消费者 - 生产者模型或读者 - 写者问题就能解决，但对哲学家进餐问题仍然要熟悉。考研复习的关键在于反复多次和全面，“偷工减料”是要吃亏的。

\subsection{管程}
在信号量机制中，每个要访问临界资源的进程都必须自备同步的 PV 操作，大量分散的同步操作给系统管理带来了麻烦，且容易因同步操作不当而导致系统死锁。于是，便产生了一种新的进程同步工具——管程。管程的特性保证了进程互斥，无须程序员自己实现互斥，从而降低了死锁发生的可能性。同时管程提供了条件变量，可以让程序员灵活地实现进程同步。

\paragraph{管程的定义}
\textbf{命题追踪：管程的特点（2016）}

系统中的各种硬件资源和软件资源，均可用数据结构抽象地描述其资源特性，即用少量信息和对资源所执行的操作来表征该资源，而忽略它们的内部结构和实现细节。

利用共享数据结构抽象地表示系统中的共享资源，而将对该数据结构实施的操作定义为一组过程。进程对共享资源的申请、释放等操作，都通过这组过程来实现，这组过程还可以根据资源情况，或接受或阻塞进程的访问，确保每次仅有一个进程使用共享资源，这样就可以统一管理对共享资源的所有访问，实现进程互斥。这个代表共享资源的数据结构，以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序，称为管程（monitor）。管程定义了一个数据结构和能为并发进程所执行（在该数据结构上）的一组操作，这组操作能同步进程和改变管程中的数据。

由上述定义可知，管程由 4 部分组成：
\begin{enumerate}
	\item 管程的名称；
	\item 局部于管程内部的共享数据结构说明；
	\item 对该数据结构进行操作的一组过程（或函数）；
	\item 对局部于管程内部的共享数据设置初始值的语句。
\end{enumerate}

管程的定义描述举例如下：
\begin{verbatim}
	monitor Demo{        //①定义一个名称为Demo的管程
		//②定义共享数据结构，对应系统中的某种共享资源
		共享数据结构S;
		//④对共享数据结构初始化的语句
	init_code(){
		S=5;          //初始资源数等于5
	}
	take_away(){    //③过程1：申请一个资源
		对共享数据结构S的一系列处理;
		S--;          //可用资源数-1
		...
	}
	give_back(){    //③过程2：归还一个资源
		对共享数据结构S的一系列处理;
		S++;          //可用资源数+1
		...
	}
	}
\end{verbatim}

熟悉面向对象程序设计的读者看到管程的组成后，会立即联想到管程很像一个类（class）。

1）管程将对共享资源的操作封装起来，管程内的共享数据结构只能被管程内的过程所访问。。一个进程只有通过调用管程内的过程才能进入管程访问共享资源。对于上例，外部进程只能通过调用"take\_away()"过程来申请一个资源；归还资源也类似。

2）每次仅允许一个进程进入管程，从而实现进程互斥。若多个进程同时调用"take\_away()"、"give\_back()"，则只有某个进程运行完它调用的过程后，下一进程才能开始运行它调用的过程。即各进程只能串行执行管程内的过程，这一特性保证了进程互斥访问"S"。

\subsubsection{条件变量}
当一个进程进入管程后被阻塞，直到阻塞的原因解除时，在此期间，若该进程不释放管程，则其他进程无法进入管程。为此，将阻塞原因定义为条件变量"condition"。通常，一个进程被阻塞的原因可以有多个，因此在管程中设置了多个条件变量。每个条件变量保存了一个等待队列，用于记录因该条件变量而阻塞的所有进程，对条件变量只能进行两种操作，即"wait"和"signal"。

"x.wait"：当"x"对应的条件不满足时，正在调用管程的进程调用"x.wait"将自己插入"x"条件的等待队列，并释放管程。此时其他进程可以使用该管程。

"x.signal"："x"对应的条件发生了变化，则调用"x.signal"，唤醒一个因"x"条件而阻塞的进程。

下面给出条件变量的定义和使用：
\begin{verbatim}
	monitor Demo{
		共享数据结构S;
		condition x;              //定义一个条件变量x
		init_code(){... }
		take_away(){
			if(S<=0) x.wait();     //资源不够，在条件变量x上阻塞等待
			资源足够，分配资源，做一系列相应处理;
		}
		give_back(){
			归还资源，做一系列相应处理;
			if(有进程在等待) x.signal();//唤醒一个阻塞进程
		}
	}
\end{verbatim}

\subsubsection{条件变量和信号量的比较}
相似点：条件变量的"wait/signal"操作类似于信号量的"P/V"操作，可以实现进程的阻塞/唤醒。

不同点：条件变量是“没有值”的，仅实现了“排队等待”功能；而信号量是“有值”的，信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。

\subsection{本节小结}

本节开头提出的问题的参考答案如下。

\begin{enumerate}
	\item \textbf{为什么要引入进程同步的概念？}
	
	在多道程序共同执行的条件下，进程与进程是并发执行的，不同进程之间存在不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。
	
	\item \textbf{不同的进程之间会存在什么关系？}
	
	进程之间存在同步与互斥的制约关系。
	
	同步是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。
	
	互斥是指当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。
	
	\item \textbf{当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗？}
	
	当两个或两个以上的进程在执行过程中，因占有某些资源而又需要对方的资源时，会因为争夺资源而造成一种互相等待的现象，若无外力作用，它们都将无法推进下去。这种现象称为死锁，具体介绍和解决方案请参考下一节。
\end{enumerate}

\section{死锁}

在学习本节时，请读者思考以下问题：
\begin{enumerate}
	\item 为什么会产生死锁？产生死锁有什么条件？
	\item 有什么办法可以解决死锁问题？
\end{enumerate}

学完本节，读者应了解死锁的由来、产生条件及基本解决方法，区分避免死锁和预防死锁。

\subsubsection{2.4.1 死锁的概念}

\paragraph{1. 死锁的定义}

在多道程序系统中，由于进程的并发执行，极大提升了系统效率。然而，多个进程的并发执行也带来了新的问题——死锁。所谓死锁，是指多个进程因竞争资源而造成的一种僵局（互相等待对方手里的资源），使得各个进程都被阻塞，若无外力干涉，这些进程都无法向前推进。

下面通过一些实例来说明死锁现象。

先看生活中的一个实例。在一条河上有一座桥，桥面很窄，只能容纳一辆汽车通行。若有两辆汽车分别从桥的左右两端驶上该桥，则会出现下述冲突情况：此时，左边的汽车占有桥面左边的一段，右边的汽车占有桥面右边的一段，要想过桥则需等待左边或右边的汽车向后行驶以退出桥面。但若左右两边的汽车都只想向前行驶，则两辆汽车都无法继续过桥。

在计算机系统中也存在类似的情况。例如，某计算机系统中只有一台打印机和一台输入设备，进程 $P_1$ 正占用输入设备，同时又提出使用打印机的请求，但此时打印机正被进程 $P_2$ 所占用，而 $P_2$ 在未释放打印机之前，又提出请求使用 $P_1$ 占用的输入设备。这样，两个进程相互无休止地等待下去，均无法继续向前推进，此时两个进程陷入死锁状态。

\paragraph{2. 死锁与饥饿}

一组进程处于死锁状态是指组内的每个进程都在等待一个事件，而该事件只能由组内的另一个进程产生。与死锁相关的另一个问题是饥饿，即进程在信号量内无穷等待的情况。

产生饥饿的主要原因是：当系统中有多个进程同时申请某类资源时，由分配策略确定资源分配给进程的次序，有的分配策略可能是不公平的，即不能保证等待时间上界的存在。在这种情况下，即使系统未发生死锁，某些进程也可能长时间等待。当等待时间给进程的推进带来明显影响时，称发生了饥饿。例如，当有多个进程需要打印文件时，若系统分配打印机的策略是最短文件优先，则长文件的打印任务将因短文件的源源不断地到来而被无限期推迟，最终导致饥饿，甚至“饿死”。饥饿并不表示系统一定会死锁，但至少有一个进程的执行被无限期推迟。

死锁和饥饿的主要差别是：
1. 发生死锁的进程可以只有一个；而死锁是因循环等待对方手里的资源而导致的，因此，若有死锁现象，则发生死锁的进程必然大于或等于两个。
2. 发生死锁的进程可能处于就绪态（长期得不到 CPU，如 SIF 算法的问题），也可能处于阻塞态（如长期得不到所需的 I/O 设备，如上述例子）；而发生死锁的进程必定处于阻塞态。

\paragraph{3. 死锁产生的原因}

\textbf{命题追踪} 单类资源竞争时发生死锁的临界条件的分析（2009、2014）

(1) 系统资源的竞争

通常系统中拥有的不可剥夺资源（如磁带机、打印机等），其数量不足以满足多个进程运行的需要，使得进程在运行过程中，会因争夺资源而陷入僵局。只有对不可剥夺资源的竞争才可能产生死锁，对可剥夺资源（如 CPU 和主存）的竞争是不会引起死锁的。

(2) 进程推进顺序非法

请求和释放资源的顺序不当，也同样会导致死锁。例如，进程 $P_1$、$P_2$ 分别保持了资源 $R_1$、$R_2$，而 $P_1$ 申请资源 $R_2$、$P_2$ 申请资源 $R_1$ 时，两者都会因为所需资源被占用而阻塞，于是导致死锁。

信号量使用不当也会造成死锁。进程间彼此相互等待对方发来的消息，也会使得这些进程无法继续向前推进。例如，进程 A 等待进程 B 发的消息，进程 B 又在等待进程 A 发的消息，可以看出进程 A 和 B 不是因为竞争同一资源，而是在等待对方的资源导致死锁。

(3) 请求并保持条件

进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。

(4) 循环等待条件

存在一种进程资源的循环等待链，链中每个进程已获得的资源同时被链中下一个进程所请求。即存在一个处于等待态的进程集合 $\{P_0, P_1, \cdots, P_n\}$，其中 $P_i$ 等待的资源被 $P_{i+1}$ 占有，$P_n$ 等待的资源被 $P_0$ 占有，如图 2.13 所示。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.24\textwidth]{5425.png}
	\caption{循环等待}
	\label{fig:cycle_waiting}
\end{figure}

直观上看，循环等待条件似乎和死锁的定义一样，其实不然。按死锁定义构成等待环所要求的条件更严，它要求 $P_i$ 等待的资源必须由 $P_{i+1}$ 来满足，而循环等待条件则无此限制。例如，系统中有两台输出设备，$P_0$ 和 $P_K$ 各占有一台，且 $K$ 不属于集合 $\{0, 1, \cdots, n\}$。$P_n$ 等待一台输出设备，它可从 $P_0$ 或 $P_K$ 获得。因此，虽然 $P_n$、$P_0$ 和其他一些进程形成了等待环，但 $P_K$ 不在圈内，若 $P_K$ 释放了输出设备，则可打破循环等待，如图 2.14 所示。因此循环等待只是死锁的必要条件。

\begin{figure}[h]
	\centering
	\includegraphics[width=0.24\textwidth]{5435.png}
	\caption{满足条件但无死锁}
	\label{fig:satisfy_condition_no_deadlock}
\end{figure}

资源分配图含圈而系统又不一定有死锁的原因是，同类资源数大于 1。但若系统中每类资源都只有一个资源，则资源分配图含圈就变成了系统出现死锁的充分必要条件。

要注意区分不可剥夺条件与请求并保持条件。下面用一个简单的例子进行说明：若你手上拿着一个苹果（即便你不打算吃），别人不能将你手上的苹果拿走，则这就是不可剥夺条件；若你左手拿着一个苹果，允许你右手再去拿一个苹果，则这就是请求并保持条件。

\paragraph{5. 死锁的处理策略}

为使系统不发生死锁，必须设法破坏产生死锁的 4 个必要条件之一，或允许死锁产生，但当死锁发生时能检测出死锁，并有能力实现恢复。

1) 死锁预防。设置某些限制条件，破坏产生死锁的 4 个必要条件中的一个或几个。

2) 避免死锁。在资源的动态分配过程中，用某种方法防止系统进入不安全状态。

3) 死锁的检测及解除。无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。

预防死锁和避免死锁都属于事先预防策略，预防死锁的限制条件比较严格，实现起来较为简单，但往往导致系统的效率低，资源利用率低；避免死锁的限制条件相对宽松，资源分配后需要通过算法来判断是否进入不安全状态，实现起来较为复杂。

死锁的几种处理策略的比较见表 2.5。
\begin{table}[h]
	\centering
	\caption{死锁处理策略的比较}
	\label{tab:deadlock_strategies}
	\scalebox{0.79}{  % 保留原缩放比例
		\begin{tabular}{lcccc}  % 移除垂直框线，列格式改为lcccc
			\toprule  % 顶线（粗线）
			& 资源分配策略 & 各种可能模式 & 主要优点 & 主要缺点 \\
			\midrule  % 中线（细线）
			死锁预防 & 保守，宁可资源闲置 & 一次请求所有资源， & 适用于突发式处理的进程， & 效率低，初始化时间延长； \\
			& & 资源剥夺，资源顺序分配 & 不必进行剥夺 & 剥夺次数过多；不便灵活申请新资源\\
			\midrule  % 中线（细线）
			死锁避免 & 是预防和检测的折中 & 寻找可能的安全允许顺序 & 不必进行剥夺 & 必须知道将来的资源需求； \\
			& （在运行时判断是否可能死锁） & & & 进程不能被长时间阻塞\\
			\midrule  % 中线（细线）
			死锁检测 & 宽松，只要允许就分配资源 & 定期检查死锁是否已经发生 & 允许对死锁进行现场处理 & 通过剥夺解除死锁，造成损失 \\
			\bottomrule  % 底线（粗线）
		\end{tabular}
	}
\end{table}

	\subsubsection{2.4.2 死锁预防}
	
	\textbf{命题追踪} 死锁预防的特点（2019）
	
	预防死锁的发生只需破坏死锁产生的 4 个必要条件之一即可。
	
	\paragraph{1. 破坏互斥条件}
	
	若将只能互斥使用的资源改造为允许共享使用，则系统不会进入死锁状态。但有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且为了系统安全，很多时候还必须保护这种互斥性。
	
	\paragraph{2. 破坏不可剥夺条件}
	
	当一个已经保持了某些不可剥夺资源的进程，请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，进程已占有的资源会被暂时释放，或者说是被剥夺了，从而破坏了不可剥夺条件。
	
	该策略实现起来比较复杂。释放已获得的资源可能造成前一阶段工作的失效，因此这种方法常用于状态易于保存和恢复的资源，如 CPU 的寄存器及内存资源，一般不能用于打印机之类的资源。反复地申请和释放资源既影响进程推进速度，又增加系统开销，进而降低了系统吞吐量。
	
	\paragraph{3. 破坏请求并保持条件}
	
	要求进程在请求资源时不能持有不可剥夺资源，可以通过两种方法实现：
	1. 采用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源。在它的资源未满足前，不让它投入运行。在进程的运行期间，不会再提出资源请求，从而破坏了“请求”条件。在等待期间，进程不占有任何资源，从而破坏了“保持”条件。
	2. 允许进程只获得运行初期所需的资源后，便可开始运行。进程在运行过程中再逐步释放已分配给自己且已使用完毕的全部资源后，才能请求新的资源。
	
	方法一的实现简单，但缺点也显而易见，系统资源被严重浪费，其中有些资源可能仅在运行初期或快结束时才使用，而且会导致“饥饿”现象，由于个别资源长期被其他进程占用，将导致等待该资源的进程迟迟不能开始运行。方法二则改进了这些缺点。
	
	\paragraph{4. 破坏循环等待条件}
	
	为了破坏循环等待条件，可以采用顺序资源分配法。首先给系统中的各类资源编号，规定每个进程必须按编号递增的顺序请求资源，同类资源（编号相同的资源）一次申请完。也就是说，一个进程只在已经占有小编号的资源时，才有资格申请更大编号的资源。按此规则，已持有大编号资源的进程不可能再逆向申请小编号的资源，因此不会产生循环等待的现象。
	
	这种方法的缺点：编号必须相对稳定，因此不便于增加新类型设备；尽管在编号时已考虑到大多数进程使用这些资源的顺序，但是进程实际使用资源的顺序还是可能和编号的次序不一致，这就会造成资源的浪费；此外，必须按规定次序申请资源，也会给用户编程带来麻烦。
	
	\subsubsection{2.4.3 死锁避免}
	
	避免死锁同样属于事先预防策略，但并不是事先采取某种限制措施破坏死锁的必要条件，而是在每次分配资源的过程中，都要分析此次分配是否会带来死锁风险，只有在不产生死锁的情况下，系统才会为其分配资源。这种方法所施加的限制条件较弱，可以获得较好的系统性能。
	
	\paragraph{1. 系统安全状态}
	
	避免死锁的方法中，允许进程动态地申请资源，但系统在进行资源分配之前，应先计算此次分配是否会导致系统进入不安全状态。若此次分配不会导致系统进入不安全状态，则允许分配；否则让进程等待。
	
	所谓安全状态，是指系统能按某种进程推进顺序 $(P_1, P_2, \dots, P_n)$ 为每个进程 $P_i$ 分配其所需的资源，直至满足每个进程对资源的最大需求，使每个进程都可顺利完成。此时称 $P_1, P_2, \dots, P_n$ 为安全序列（可能有多个）。若系统无法找到一个安全序列，则称系统处于不安全状态。
	
	\textbf{命题追踪} 系统安全状态的分析（2018）
	
	假设系统有三个进程 $P_1, P_2, P_3$，共有 12 台磁带机。$P_1$ 需要 10 台，$P_2$ 和 $P_3$ 分别需要 4 台和 9 台。假设在 $T_0$ 时刻，$P_1, P_2$ 和 $P_3$ 已分别获得 5 台、2 台和 2 台，尚有 3 台未分配，见表 2.6。
	
	\begin{table}[h]
		\centering
		\caption{资源分配}
		\label{tab:resource_allocation}
		\begin{tabular}{|c|c|c|c|}
			\hline
			进程名 & 最大需求 & 已分配 & 可用 \\
			\hline
			$P_1$ & 10 & 5 & 3 \\
			\hline
			$P_2$ & 4 & 2 & 2 \\
			\hline
			$P_3$ & 9 & 2 & 7 \\
			\hline
		\end{tabular}
	\end{table}
	
	在 $T_0$ 时刻是安全的，因为存在一个安全序列 $P_2, P_1, P_3$，只要系统按此进程序列分配资源，那么每个进程都能顺利完成。也就是说，当前可用资源数为 3，先将 2 台分配给 $P_2$ 以满足其最大需求，$P_2$ 结束并归还资源后，系统有 5 台可用；接下来给 $P_1$ 分配 5 台以满足其最大需求，$P_1$ 结束并归还资源后，剩余 10 台可用；最后分配 7 台给 $P_3$，这样 $P_3$ 也能顺利完成。
	
	若在 $T_0$ 时刻后，系统分配 1 台给 $P_3$，剩余可用资源数为 2，此时系统进入不安全状态，因为此时已无法再找到一个安全序列。当系统进入不安全状态后，便可能导致死锁。例如，将剩下的 2 台分配给 $P_2$，这样，$P_2$ 完成后只能释放 4 台，既不能满足 $P_1$ 又不能满足 $P_3$，致使它们都无法推进到完成，彼此都在等待对方释放资源，陷入僵局，即导致死锁。
	
	若系统处于安全状态，则一定不会发生死锁；若系统进入不安全状态，则有可能发生死锁（处于不安全状态未必就会发生死锁，但发生死锁时一定是处于不安全状态）。
	
	\paragraph{2. 银行家算法}
	
	\textbf{命题追踪} 银行家算法的特点（2013、2019）
	
	银行家算法是最著名的死锁避免算法，其思想是：将操作系统视为银行家，操作系统管理的资源视为银行家管理的资金，进程请求资源相当于用户向银行家贷款。进程运行之前先声明对各种资源的最大需求量，其数量不应超过系统的资源总量。当进程在运行中申请资源时，系统必须先确定是否有足够的资源分配给该进程。若有，再进一步试探在将这些资源分配给进程后，是否会使系统处于不安全状态。若不会，则将资源分配给它，否则让进程等待。
	
	\subparagraph{(1) 数据结构描述}
	
	假设系统中有 $n$ 个进程，$m$ 类资源，在银行家算法中需要定义下面 4 个数据结构：
	1. 可利用资源向量 Available：含有 $m$ 个元素的数组，其中每个元素代表一类可用的资源数量。Available[$j$] = $K$ 表示此时系统中有 $K$ 个 $R_j$ 类资源可用。
	2. 最大需求矩阵 Max：$n \times m$ 矩阵，定义系统中 $n$ 个进程中的每个进程对 $m$ 类资源的最大需求。Max[$i, j$] = $K$ 表示进程 $P_i$ 需要 $R_j$ 类资源的最大数量为 $K$。
	3. 分配矩阵 Allocation：$n \times m$ 矩阵，定义系统中每类资源当前已分配给每个进程的资源数。Allocation[$i, j$] = $K$ 表示进程 $P_i$ 当前已分得 $R_j$ 类资源的数量为 $K$。
	4. 需求矩阵 Need：$n \times m$ 矩阵，表示每个进程接下来最多还需要多少资源。Need[$i, j$] = $K$ 表示进程 $P_i$ 还需要 $R_j$ 类资源的数量为 $K$。
	
	上述三个矩阵间存在下述关系：
	\[
	\text{Need} = \text{Max} - \text{Allocation}
	\]
	
	通常，Max 矩阵和 Allocation 矩阵是题中的已知条件，而求出 Need 矩阵是解题的第一步。
	
	\subparagraph{(2) 银行家算法描述}
	
	设 Request$_i$ 是进程 $P_i$ 的请求向量，Request$_i[j]$ = $K$ 表示进程 $P_i$ 需要 $j$ 类资源 $K$ 个。当 $P_i$ 发出资源请求后，系统按下述步骤进行检查：
	1. 若 Request$_i[j]$ ≤ Need[$i, j$]，则转向步骤②；否则认为出错，因为它所需要的资源数已超过它所宣布的最大值。
	2. 若 Request$_i[j]$ ≤ Available[$j$]，则转向步骤③；否则，表示尚无足够资源，$P_i$ 必须等待。
	3. 系统试探着将资源分配给进程 $P_i$，并修改下面数据结构中的数值：
	\[
	\text{Available} = \text{Available} - \text{Request}_i;
	\]
	\[
	\text{Allocation}[i, j] = \text{Allocation}[i, j] + \text{Request}_i[j];
	\]
	\[
	\text{Need}[i, j] = \text{Need}[i, j] - \text{Request}_i[j].
	\]
	4. 系统执行安全性算法，检查此次资源分配后，系统是否处于安全状态。若安全，才正式将资源分配给进程 $P_i$，以完成本次分配；否则，将本次的试探分配作废，恢复原来的资源分配状态，让进程 $P_i$ 等待。
	
	\subparagraph{(3) 安全性算法}
	
	设置工作向量 Work，表示系统中的剩余可用资源数量，它有 $m$ 个元素，在执行安全性算法前，令 Work = Available。
	1. 初始时安全序列为空。
	2. 从 Need 矩阵中找出符合下面条件的行：该行对应的进程不在安全序列中，而且该行小于或等于 Work 向量，找到后，将对应的进程加入安全序列；若找不到，则执行步骤④。
	3. 进程 $P_i$ 进入安全序列后，可顺利执行，直至完成，并释放分配给它的资源，所以应执行 Work = Work + Allocation[$j$]，其中 Allocation[$j$] 是 Allocation 矩阵中对应的行，返回步骤②。
	4. 若此时安全序列中已有所有进程，则系统处于安全状态，否则系统处于不安全状态。
	
	看完上面对银行家算法的过程描述后，可能有眼花缭乱的感觉，现在通过举例来加深理解。
	
	\paragraph{3. 安全性算法举例}
	
	\textbf{命题追踪} 银行家算法的安全序列分析（2011、2012、2018、2020、2022）
	
	假定系统中有 5 个进程 $\{P_0, P_1, P_2, P_3, P_4\}$ 和三类资源 $\{A, B, C\}$，各种资源的数量分别为 10、5、7，在 $T_0$ 时刻的资源分配情况见表 2.7。
	再用更新的 Work 向量和 Need 矩阵重复步骤②。利用安全性算法分析 $T_0$ 时刻的资源分配情况如表 2.8 所示，最后得到一个安全序列 $\{P_1, P_3, P_4, P_2, P_0\}$。
	
	\begin{table}[h]
		\centering
		\caption{$T_0$ 时刻的安全序列的分析}
		\label{tab:t0_safe_sequence}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			进程名 & Work & Need & Allocation & Work+Allocation \\
			\hline
			$P_1$ & $3 \ 3 \ 2$ & $1 \ 2 \ 2$ & $2 \ 0 \ 0$ & $5 \ 3 \ 2$ \\
			\hline
			$P_3$ & $5 \ 3 \ 2$ & $0 \ 1 \ 1$ & $2 \ 1 \ 1$ & $7 \ 4 \ 3$ \\
			\hline
			$P_4$ & $7 \ 4 \ 3$ & $4 \ 3 \ 1$ & $0 \ 0 \ 2$ & $7 \ 4 \ 5$ \\
			\hline
			$P_2$ & $7 \ 4 \ 5$ & $6 \ 0 \ 0$ & $3 \ 0 \ 2$ & $10 \ 4 \ 7$ \\
			\hline
			$P_0$ & $10 \ 5 \ 7$ & $7 \ 4 \ 3$ & $0 \ 1 \ 0$ & $10 \ 5 \ 7$ \\
			\hline
		\end{tabular}
	\end{table}
	
	\paragraph{4. 银行家算法举例}
	
	安全性算法是银行家算法的核心，在银行家算法的题目中，一般会有某个进程的一个资源请求向量，读者只要执行上面所介绍的银行家算法的前三步，马上就会得到更新的 Allocation 矩阵和 Need 矩阵，再按照上例的安全性算法判断，就能知道系统能否满足进程提出的资源请求。
	
	假设当前系统中资源的分配和剩余情况如表 2.7 所示。
	
	(1) $P_1$ 请求资源：$P_1$ 发出请求向量 $\text{Request}_1(1, 0, 2)$，系统按银行家算法进行检查：
	\[
	\text{Request}_1(1, 0, 2) \leq \text{Need}_1(1, 2, 2)
	\]
	\[
	\text{Request}_1(1, 0, 2) \leq \text{Available}(3, 3, 2)
	\]
	系统先假定可为 $P_1$ 分配资源，并修改
		\[ 
		\text{Available} = \text{Available} - \text{Request}_1 = (2, 3, 0)
		\]
		\[ 
		\text{Allocation}_1 = \text{Allocation}_1 + \text{Request}_1 = (3, 0, 2)
		\]
		\[ 
		\text{Need}_1 = \text{Need}_1 - \text{Request}_1 = (0, 2, 0)
		\]
		
		由此形成的资源变化情况如表 2.7 中的圆括号所示。
		
		令 $\text{Work} = \text{Available} = (2, 3, 0)$，再利用安全性算法检查此时系统是否安全，如表 2.9 所示。
		
		\begin{table}[h]
			\centering
			\caption{$P_1$ 申请资源时的安全性检查}
			\label{tab:p1_security_check}
			\begin{tabular}{|c|c|c|c|c|}
				\hline
				进程名 & \multicolumn{4}{c|}{资源情况} \\
				\cline{2-5}
				& Work & Need & Allocation & Work+Allocation \\
				\hline
				$P_1$ & $2 \ 3 \ 0$ & $0 \ 2 \ 0$ & $3 \ 0 \ 2$ & $5 \ 3 \ 2$ \\
				\hline
				$P_3$ & $5 \ 3 \ 2$ & $0 \ 1 \ 1$ & $2 \ 1 \ 1$ & $7 \ 4 \ 3$ \\
				\hline
				$P_4$ & $7 \ 4 \ 3$ & $4 \ 3 \ 1$ & $0 \ 1 \ 0$ & $7 \ 5 \ 5$ \\
				\hline
				$P_2$ & $7 \ 5 \ 5$ & $6 \ 0 \ 0$ & $3 \ 0 \ 2$ & $10 \ 5 \ 7$ \\
				\hline
			\end{tabular}
		\end{table}
		
		由所进行的安全性检查得知，可找到一个安全序列 $\{P_1, P_3, P_4, P_0, P_2\}$。因此，系统是安全的，可以立即将 $P_1$ 所申请的资源分配给它。分配后系统中的资源情况如表 2.10 所示。
		
		\begin{table}[h]
			\centering
			\caption{为 $P_1$ 分配资源后的有关资源数据}
			\label{tab:p1_allocation_after}
			\begin{tabular}{|c|c|c|c|}
				\hline
				进程名 & \multicolumn{3}{c|}{资源情况} \\
				\cline{2-4}
				& Allocation & Need & Available \\
				\hline
				$P_0$ & $0 \ 1 \ 0$ & $7 \ 4 \ 3$ & $2 \ 3 \ 0$ \\
				\hline
				$P_1$ & $3 \ 0 \ 2$ & $0 \ 2 \ 0$ & \\
				\hline
				$P_2$ & $3 \ 0 \ 2$ & $6 \ 0 \ 0$ & \\
				\hline
				$P_3$ & $2 \ 1 \ 1$ & $0 \ 1 \ 1$ & \\
				\hline
				$P_4$ & $0 \ 0 \ 2$ & $4 \ 3 \ 1$ & \\
				\hline
			\end{tabular}
		\end{table}
		
		(2) $P_4$ 请求资源：$P_4$ 发出请求向量 $\text{Request}_4(3, 3, 0)$，系统按银行家算法进行检查：
		\[ 
		\text{Request}_4(3, 3, 0) \leq \text{Need}_4(4, 3, 1);
		\]
		\[ 
		\text{Request}_4(3, 3, 0) > \text{Available}(2, 3, 0),
		\]
		让 $P_4$ 等待。
		
		(3) $P_0$ 请求资源：$P_0$ 发出请求向量 $\text{Request}_0(0, 2, 0)$，系统按银行家算法进行检查：
		\[ 
		\text{Request}_0(0, 2, 0) \leq \text{Need}_0(7, 4, 3);
		\]
		\[ 
		\text{Request}_0(0, 2, 0) \leq \text{Available}(2, 3, 0).
		\]
		
		系统暂时先假定可为 $P_0$ 分配资源，并修改有关数据：
		\[ 
		\text{Available} = \text{Available} - \text{Request}_0 = (2, 1, 0)
		\]
		\[ 
		\text{Allocation}_0 = \text{Allocation}_0 + \text{Request}_0 = (0, 3, 0)
		\]
		\[ 
		\text{Need}_0 = \text{Need}_0 - \text{Request}_0 = (7, 2, 3).
		\]
		结果如表 2.11 所示。
		
		\begin{table}[h]
			\centering
			\caption{为 $P_0$ 分配资源后的有关资源数据}
			\label{tab:p0_allocation_after}
			\begin{tabular}{|c|c|c|c|}
				\hline
				进程名 & \multicolumn{3}{c|}{资源情况} \\
				\cline{2-4}
				& Allocation & Need & Available \\
				\hline
				$P_0$ & $0 \ 3 \ 0$ & $7 \ 2 \ 3$ & $2 \ 1 \ 0$ \\
				\hline
				$P_1$ & $3 \ 0 \ 2$ & $0 \ 2 \ 0$ & \\
				\hline
				$P_2$ & $3 \ 0 \ 2$ & $6 \ 0 \ 0$ & \\
				\hline
				$P_3$ & $2 \ 1 \ 1$ & $0 \ 1 \ 1$ & \\
				\hline
				$P_4$ & $0 \ 0 \ 2$ & $4 \ 3 \ 1$ & \\
				\hline
			\end{tabular}
		\end{table}
		
		进行安全性检查：可用资源 $\text{Available}(2, 1, 0)$ 已不能满足任何进程的需要，系统进入不安全状态，因此拒绝 $P_0$ 的请求，并将 $\text{Available}, \text{Allocation}_0, \text{Need}_0$ 恢复为之前的值。
		
		\subsubsection{2.4.4 死锁检测和解除}
		
		\textbf{命题追踪} 死锁避免和死锁检测的区分（2015）
		
		前面介绍的死锁预防和避免算法，都是在为进程分配资源时施加限制条件或进行检测，若系统为进程分配资源时不采取任何预防或避免措施，则应该提供死锁检测和解除的手段。
		
		\paragraph{1. 死锁检测}
		
		\textbf{命题追踪} 死锁避免和死锁检测对比（2015） 公众号：小兔网盘免费分享无水印PDF
		
		死锁避免和死锁检测的对比。死锁避免需要在进程的运行过程中一直保证之后不可能出现死锁，因此需要知道进程从开始到结束的所有资源请求。而死锁检测是检测某个时刻是否发生死锁，不需要知道进程在整个生命周期中的资源请求，只需知道对应时刻的资源请求。
		
		\textbf{命题追踪} 多在资源竞争时发生死锁的临界条件分析（2016、2021）
		
		可用资源分配图来检测系统所处的状态是否为死锁状态。如图 2.15(a) 所示，用圆圈表示一个进程，用框表示一类资源。一种类型的资源可能有多个，因此用框中的一个圆表示一类资源中的一个资源。从进程到资源的有向边称为请求边，表示该进程申请一个单位的该类资源；从资源到进程的有向边称为分配边，表示该类资源已有一个资源分配给了该进程。
		
		在图 2.15(a) 所示的资源分配图中，进程 $P_1$ 已分得了两个 $R_1$ 资源，并又请求一个 $R_2$ 资源；进程 $P_2$ 分得了一个 $R_1$ 资源和一个 $R_2$ 资源，并又请求一个 $R_1$ 资源。
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.8\textwidth]{3427.png}
			\caption{资源分配图及其化简过程}
			\label{fig:resource_allocation_graph}
		\end{figure}
		
		简化资源分配图可检测系统状态 $S$ 是否为死锁状态。简化方法如下：
		
		1. 在资源分配图中，找出既不阻塞又不孤立的进程 $P_i$（找出一条有向边与它相连，且该有向边对应资源的申请数量小于或等于系统中已有的空闲资源数量，如在图 2.15(a) 中，$R_1$ 没有空闲资源，$R_2$ 有一个空闲资源。若所有连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源）。消去它所有的请求边和分配边，使之成为孤立的节点。在图 2.15(a) 中，$P_1$ 是满足这一条件的进程节点，将 $P_1$ 的所有边消去，便得到图 2.15(b) 所示的情况。
		
		这里要注意一个问题，判断某种资源是否有空闲，应该用它的资源数量减去它在资源分配图中的出度，例如在图 2.15(a) 中，$R_1$ 的资源数为 3，而出度也为 3，所以 $R_1$ 没有空闲资源，$R_2$ 的资源数为 2，出度为 1，所以 $R_2$ 有一个空闲资源。
		
		2. 进程 $P_j$ 所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可变为非阻塞进程。在图 2.15(a) 中，$P_2$ 就满足这样的条件。根据 1) 中的方法进行一系列简化后，若能消去图中所有的边，则称该图是可完全简化的，如图 2.15(c) 所示。
		
		$S$ 为死锁的条件是当且仅当 $S$ 状态的资源分配图是不可完全简化的，该条件为死锁定理。
		
		\paragraph{2. 死锁解除}
		
		一旦检测出死锁，就应立即采取相应的措施来解除死锁。死锁解除的主要方法有：
		
		1. 资源剥夺法。挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源而处于资源匮乏的状态。
		
		\begin{tcolorbox}[colback=gray!10, colframe=black!50, title=注意]
			在资源分配图中，用死锁定理化简后，还有边相连的那些进程就是死锁进程。
		\end{tcolorbox}
		
		2. 撤销进程法。强制撤销部分、甚至全部死锁进程，并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。这种方式实现简单，但付出的代价可能很大，因为有些进程可能已经接近结束，一旦被终止，以后还得从头再来。
		
		3. 进程回退法。让一个或多个死锁进程回退到足以回避死锁的地步，进程回退时自愿释放资源而非被剥夺。要求系统保持进程的历史信息，设置还原点。
		
		\subsubsection{2.4.5 本节小结}
		
		本节开头提出的问题的参考答案如下。
		
		1. 为什么会产生死锁？产生死锁有什么条件？
		
		由于系统中存在一些不可剥夺资源，当两个或两个以上的进程占有自身的资源并请求对方的资源时，会导致每个进程都无法向前推进，这就是死锁。死锁产生的必要条件有 4 个，分别是互斥条件、不剥夺条件、请求并保持条件和循环等待条件。
		
		互斥条件是指进程要求分配的资源是排他性的，即最多只能同时供一个进程使用。
		
		不剥夺条件是指进程在使用完资源之前，资源不能被强制夺走。
		
		请求并保持条件是指进程占有自身拥有的资源并要求其他资源。
		
		循环等待条件是指存在一种进程资源的循环等待链。
		
		2. 有什么办法可以解决死锁问题？
		
		死锁的处理策略可以分为预防死锁、避免死锁及死锁的检测与解除。
		
		死锁预防是指通过设立一些限制条件，破坏死锁的一些必要条件，让死锁无法发生。
		
		死锁避免指在动态分配资源的过程中，用一些算法防止系统进入不安全状态，从而避免死锁。
		
		死锁的检测和解除是指在死锁产生前不采取任何措施，只检测当前系统有没有发生死锁，若有，则采取一些措施解除死锁。
	
	\chapter{内存管理}
	
	\section*{考纲内容}
	
	\begin{enumerate}
		\item \textbf{内存管理基础}
		
		
	内存管理概念：逻辑地址与物理地址空间，地址变换，内存共享，内存保护，内存分配与回收
		
	连续分配管理方式；页式管理；段式管理；段页式管理
		
		\item \textbf{虚拟内存管理}
		
		
	虚拟内存基本概念；请求页式管理；页框分配与回收；页置换算法
		
	内存映射文件（Memory-Mapped Files）；虚拟存储器性能的影响因素及改进方式
		
	\end{enumerate}
	
	\section*{复习提示}
	
	内存管理和进程管理是操作系统的核心内容，需要重点复习。本章围绕分页机制展开：通过分页管理方式在物理内存大小的基础上提高内存的利用率，再进一步引入请求分页管理方式，实现虚拟内存，使内存脱离物理大小的限制，从而提高处理器的利用率。
	
	\section{内存管理概念}
	
	在学习本节时，请读者思考以下问题：
	\begin{enumerate}
		\item 为什么要进行内存管理？
		\item 多级页表解决了什么问题？又会带来什么问题？
	\end{enumerate}
	
	在学习经典的管理方法前，同样希望读者先思考，自己给出一些内存管理的想法，并在学习过程中和经典方案进行比较。注意本节给出的内存管理是循序渐进的，后一种方法通常会解决前一种方法的不足。希望读者多多思考，比较每种方法的异同，着重掌握页式管理。
	
	\subsection{内存管理的基本原理和要求}
	
	内存管理（Memory Management）是操作系统设计中最重要和最复杂的内容之一。虽然计算机硬件技术一直在飞速发展，内存容量也在不断增大，但仍然不可能将所有用户进程和系统所需要的全部程序与数据放入主存，因此操作系统必须对内存空间进行合理的划分和有效的动态分配。操作系统对内存的划分和动态分配，就是内存管理的概念。
	
	有效的内存管理在多道程序设计中非常重要，它不仅可以方便用户使用存储器、提高内存利用率，还可以通过虚拟技术从逻辑上扩充存储器。内存管理的主要功能有：
	
	
内存空间的分配与回收。由操作系统负责内存空间的分配和管理，记录内存的空闲空间、内存的分配情况，并回收已结束进程所占用的内存空间。
	
	\paragraph{逻辑地址与物理地址}
	
	\textbf{命题追踪 > 进程虚拟地址空间的特点（2023）}
	
	编译后，每个目标模块都从 0 号单元开始编号，这称为该目标模块的相对地址（或逻辑地址）。当链接程序将各个模块链接成一个完整的可执行程序时，链接程序顺序依次按各个模块的相对地址构成统一的从 0 号单元开始编址的逻辑地址空间（或虚拟地址空间）。对于 32 位系统，逻辑地址空间的范围为 $0 \sim 2^{32} - 1$。进程在运行时，看到和使用的地址都是逻辑地址。用户程序和程序员只需知道逻辑地址，而内存管理的具体机制则是完全透明的。不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。
	
	物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址。进程在运行时执行指令和访问数据，最后都要通过物理地址从主存中取存。当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为地址重定位。
	
	操作系统通过内存管理部件（MMU）将进程使用的逻辑地址转换为物理地址。操作系统在相关硬件的协助下，将它“转换”成真正的物理地址。逻辑地址通过页表映射到物理内存，页表由操作系统维护并被处理器引用。
	
	\textbf{命题追踪 > 编译、链接和装入阶段的工作内容（2011）}
	
	创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤：
	
	
	
\textbf{编译}：由编译程序将用户源代码编译成若干目标模块。
	
\textbf{链接}：由链接程序将编译后形成的一组目标模块，以及它们所需的库函数链接在一起，形成一个完整的装入模块。
	
\textbf{装入}：由装入程序将装入模块装入内存运行。
	
	
	将用户程序变为可在内存中执行的程序的步骤如图 3.1 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{4748.png}
		\label{tttttt}
	\end{figure}
	
	\subsubsection{将一个装入模块装入内存时，有以下三种装入方式}
	
	\paragraph{(1) 绝对装入}
	
	绝对装入方式只适用于单道程序环境。在编译时，若知道程序将放到内存的哪个位置，则编译程序将产生绝对地址的目标代码。装入程序按照装入模块的地址，将程序和数据装入内存。程序中的逻辑地址与实际内存地址完全相同，因此不需对程序和数据的地址进行修改。
	
	程序中使用的绝对地址，可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中采用的是符号地址，编译或汇编时再转换为绝对地址。
	
	\paragraph{(2) 可重定位装入}
	
	可重定位装入也称静态重定位。经过编译、链接后的装入模块的始址（起始地址）通常都是从 0 开始的，程序中使用的指令和数据的地址都是相对于始址而言的逻辑地址。可根据内存的当前情况，将装入模块装入内存的适当位置。在装入时对目标程序中的相对地址的修改过程称为重定位，地址转换通常是在进程装入时一次完成的，如图 \ref{fig:relocatable_load} 所示。
	
	当一个作业装入内存时，必须给它分配要求的全部内存空间，若没有足够的内存，则无法装入。作业一旦进入内存，整个运行期间就不能在内存中移动，也不能再申请内存空间。

	
	\paragraph{(3) 动态运行时装入}
	
	动态运行时装入也称动态重定位。程序若要在内存中发生移动，则要采用动态的装入方式。装入程序将装入模块装入内存后，并不会立即将装入模块中的相对地址转换为绝对地址，而是将这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器（存放装入模块的起始位置）的支持，如图 \ref{fig:dynamic_load} 所示。
	
	动态重定位的优点：可以将程序分配到不连续的存储区；在程序运行前只需装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{5213.png}
		\label{fig:dynamic_load}
	\end{figure}
	
		\subsubsection{对目标模块进行链接时，根据链接的时间不同，分为以下三种链接方式}
	
	\paragraph{(1) 静态链接}
	
	在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的装入模块，以后不再拆开。将几个目标模块装配成一个装入模块时，需要解决两个问题：
	1. 修改相对地址，编译后的所有目标模块都是从 0 开始的相对地址，当链接成一个装入模块时要修改相对地址。
	2. 变换外部调用符号，将每个模块中所用的外部调用符号也都变换为相对地址。
	
	\paragraph{(2) 装入时动态链接}
	
	将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的方式。其优点是便于修改和更新，便于实现对目标模块的共享。
	
	\paragraph{(3) 运行时动态链接}
	
	在程序执行中需要某目标模块时，才对它进行链接。凡在程序执行中未用到的目标模块，都不会被调入内存和链接到装入模块上。其优点是能加快程序的装入过程，还可节省内存空间。
	
	\subsubsection{进程的内存映像}
	
	不同于存放在硬盘上的可执行程序文件，当一个程序调入内存运行时，就构成了进程的内存映像。一个进程的内存映像一般有几个要素
	
	
\textbf{代码段}：即程序的二进制代码，代码段是只读的，可以被多个进程共享。
	
\textbf{数据段}：即程序运行时加工处理的对象，包括全局变量和静态变量。
	
\textbf{进程控制块 (PCB)}：存放在系统区。操作系统通过 PCB 来控制和管理进程。
	
\textbf{堆}：用来存放动态分配的变量。通过调用 malloc 函数动态地向高地址分配空间。
	
\textbf{栈}：用来实现函数调用。从用户空间的最大地址往低地址方向增长。
	
	
	代码段和数据段在程序调入内存时就指定了大小，而堆和栈不一样。当调用像 malloc 和 free 这样的 C 标准库函数时，堆可以在运行时动态地扩展和收缩。用户栈在程序运行期间也可以动态地扩展和收缩，每次调用一个函数，栈就会增长；从一个函数返回时，栈就会收缩。
	
	图 \ref{fig:process_memory_map} 是一个进程在内存中的映像。其中，共享库用来存放进程用到的共享函数库代码，如 printf() 函数等。在只读代码段中，.init 是程序初始化时调用的 \_init 函数；.text 是用户程序的机器代码；.rodata 是只读数据。在读/写数据段中，.data 是已初始化的全局变量和静态变量；.bss 是未初始化及所有初始化为 0 的全局变量和静态变量。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\textwidth]{0118.png}
		\caption{一个进程在内存中的映像}
		\label{fig:process_memory_map}
	\end{figure}
	
	\subsubsection{内存保护}
	
	\textbf{命题追踪 > 分区分配内存保护的措施（2009）}
	
	确保每个进程都有一个单独的内存空间。内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。内存保护可采取两种方法：
	
	\begin{enumerate}
		\item 在 CPU 中设置一对上、下限寄存器，存放用户进程在主存中的下限和上限地址，每当 CPU 要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。
		\item 采用重定位寄存器（也称基地址寄存器）和界地址寄存器（也称限长寄存器）进行越界检查。
	\end{enumerate}
	
	\paragraph{重定位寄存器和界地址寄存器的硬件支持}
	
	重定位寄存器中存放的是进程的起始物理地址，界地址寄存器中存放的是进程的最大逻辑地址。内存管理部件将逻辑地址与界地址寄存器进行比较，若未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元，如图 \ref{fig:hardware_support} 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.6\textwidth]{1530.png}
		\caption{重定位寄存器和界地址寄存器的硬件支持}
		\label{fig:hardware_support}
	\end{figure}
	
	实现内存保护需要重定位寄存器和界地址寄存器，因此要注意两者的区别。重定位寄存器是用来“加”的，逻辑地址加上重定位寄存器中的值就能得到物理地址；界地址寄存器是用来“比”的，通过比较界地址寄存器中的值与逻辑地址的值来判断是否越界。
	
	加载重定位寄存器和界地址寄存器时必须使用特权指令，只有操作系统内核才可以加载这两个寄存器的值，而不允许用户程序修改。这种方案允许操作系统内核修改这两个寄存器的值，而不允许用户程序修改。
	
	
	\subsubsection{内存共享}
	
	并不是所有的进程内存空间都适合共享，只有那些只读的区域才可以共享。可重入代码也称纯代码，是一种允许多个进程同时访问但不允许被任何进程修改的代码。但在实际执行时，也可以为每个进程配以局部数据区，将在执行中可能改变的部分复制到该数据区，这样，程序在执行时只需对该私有数据区中的内存进行修改，并不去修改共享的代码。
	
	下面用一个例子来说明内存共享的实现。考虑一个可以同时容纳 40 个用户的多用户系统，他们同时执行一个文本编辑程序，若该程序有 $160\text{KB}$ 代码区和 $40\text{KB}$ 数据区，则共需 $80\text{KB} \times 40 + 160\text{KB} = 1760\text{KB}$ 的内存空间来支持 40 个用户。若 $160\text{KB}$ 代码是可共享的纯代码，则不论是在分页系统中还是在分段系统中，整个系统只需保留一份副本即可，此时所需的内存空间仅为 $40\text{KB} \times 40 + 160\text{KB} = 1760\text{KB}$。对于分页系统，假设页面大小为 $4\text{KB}$，则代码区占用 $40$ 个页面、数据区占用 $10$ 个页面。为实现代码共享，应在每个进程的页表中都建立 $40$ 个页表项，它们都指向共享代码区的物理页号。此外，每个进程还要为自己的数据区建立 $10$ 个页表项，指向私有数据区的物理页号。对于分段系统，由于是以段为分配单位的，不管该段有多大，都只需为该段设置一个段表项（指向共享代码段始址，以及段长 $160\text{KB}$）。由此可见，段的共享非常简单易行。
	
	此外，在第 2 章中我们介绍过基于共享内存的进程通信，由操作系统提供同步互斥工具。在本章的后面，还将介绍一种内存共享的实现——内存映射文件。
	
	\subsubsection{内存分配与回收}
	存储管理方式随着操作系统的发展而发展。在操作系统由单道向多道发展时，存储管理方式便由单一连续分配发展为固定分区分配。为了能更好地适应不同大小的程序要求，又从固定分区分配发展到动态分区分配。为了更好地提高内存的利用率，进而从连续分配方式发展到离散分配方式——页式存储管理。引入分段存储管理的目的，主要是满足用户在编程和使用方面的要求，其中某些要求是其他几种存储管理方式难以满足的。
	
	\subsection{连续分配管理方式}
	连续分配方式是指为一个用户程序分配一个连续的内存空间，譬如某用户需要 100MB 的内存空间，连续分配方式就在内存空间中为用户分配一块连续的 100MB 空间。连续分配方式主要包括单一连续分配、固定分区分配和动态分区分配。
	
	\subsubsection{1. 单一连续分配}
	在单一连续分配方式中，内存被分为系统区和用户区，系统区仅供操作系统使用，通常在低地址部分；用户区内存中仅有一道用户程序，即用户程序独占整个用户区。
	
	这种方式的优点是简单、无外部碎片；不需要进行内存保护，因为内存中永远只有一道程序。缺点是只能用于单用户、单任务的操作系统中；有内部碎片；内存的利用率极低。
	
	\subsubsection{2. 固定分区分配}
	固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干固定大小的分区，每个分区只装入一道作业。当有空闲分区时，便可再从外存的后备作业队列中选择适当大小的作业装入该分区，如此循环。在划分分区时有两种不同的方法：
	\begin{itemize}
		\item 分区大小相等。程序太小会造成浪费，程序太大又无法装入，缺乏灵活性。
		\item 分区大小不等。划分为多个较小的分区、适量的中等分区和少量大分区。
	\end{itemize}
	
	为了便于分配和回收，建立一张分区使用表，通常按分区大小排队，各表项包括对应分区的始址、大小及状态（是否已分配），如图 3.5 所示。分配内存时，便检索该表，以找到一个能满足要求且尚未分配的分区分配给装入程序，并将对应表项的状态置为“已分配”；若找不到这样的分区，则拒绝分配。回收内存时，只需将对应表项的状态置为“未分配”即可。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.45\textwidth]{2554.png}
		\label{fig:ttttttt}
	\end{figure}
	
	这种方式存在两个问题：①程序太大而放不进任何一个分区；②当程序小于固定分区大小时，也要占用一个完整的内存分区，这样分区内部就存在空间浪费，这种现象称为内部碎片。固定分区方式无外部碎片，但不能实现多进程共享一个主存区，所以内存的利用率低。
	
	\subsubsection{3. 动态分区分配}
	\paragraph{（1）动态分区分配的基本原理}
	动态分区分配也称可变分区分配，是指在进程装入内存时，根据进程的实际需要，动态地为之分配内存，并使分区的大小正好适合进程的需要。因此，系统中分区的大小和数量是可变的。
	
	如图 3.6 所示，系统有 64MB 内存空间，其中低 8MB 固定分配给操作系统，其余为用户可用内存。开始时装入前三个进程，它们分别分配到所需的空间后，内存仅剩 4MB，进程 4 无法装入。在某个时刻，内存中没有一个就绪进程，CPU 出现空闲，操作系统就换出进程 2，换入进程 4。由于进程 4 比进程 2 小，这样在主存中就产生了一个 6MB 的内存块。之后 CPU 又出现空闲，需要换入进程 2，而主存无法容纳进程 2，操作系统就换出进程 1，换入进程 2。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.65\textwidth]{3246.png}
		\label{fig:tttttttt}
	\end{figure}
	
	动态分区在开始时是很好的，但是随着时间的推移，内存中会产生越来越多的小内存块，内存的利用率也随之下降。这些小内存块被称为外部碎片，它存在于所有分区的外部，与固定分区中的内部碎片正好相对。外部碎片可通过紧凑技术来克服，即操作系统不时地对进程进行移动和整理。但是，这需要动态重定位寄存器的支持，且相对费时。紧凑过程实际上类似于 Windows 系统中的磁盘碎片整理程序，只不过后者是对外存空间的紧凑。
	
	\textbf{命题追踪：动态分区分配的内存回收方法（2017）}
	
	在动态分区分配中，与固定分区分配类似，设置一张空闲分区链（表），可以按始址排序。分配内存时，检索空闲分区链，找到所需的分区，若其大小大于请求大小，则从该分区中按请求大小分割一块空间分配给装入进程（若剩余部分小到不足以划分，则不需要分割），余下部分仍然留在空闲分区链中。回收内存时，系统根据回收分区的始址，从空闲分区链中找到相应的插入点，此时可能出现四种情况：
	\begin{enumerate}
		\item 回收区与插入点的前一空闲分区相邻，此时将这两个分区合并，并修改前一分区表项的大小为两者之和；
		\item 回收区与插入点的后一空闲分区相邻，此时将这两个分区合并，并修改后一分区表项的始址和大小；
		\item 回收区同时与插入点的前、后两个分区相邻，此时将这三个分区合并，修改前一分区表项的大小为三者之和，并取消后一分区表项；
		\item 回收区没有相邻的空闲分区，此时应该为回收区新建一个表项，填写始址和大小，并插入空闲分区链。
	\end{enumerate}
	
	以上三种内存分区管理方法有一个共同特点，即用户程序在主存中都是连续存放的。
	
	\paragraph{（2）基于顺序搜索的分配算法}
	将作业装入主存时，需要按照一定的分配算法从空闲分区链（表）中选出一个分区，以分配给该作业。按分区检索方式，可分为顺序分配算法和索引分配算法。顺序分配算法是指依次搜索空闲分区链上的空闲分区，以寻找一个大小满足要求的分区，顺序分配算法有以下四种。
	
	\subparagraph{命题追踪：各种动态分区分配算法的特点（2019、2024）}
	\begin{enumerate}
		\item 首次适应（First Fit）算法。空闲分区按地址递增的次序排列。每次分配内存时，顺序查找到第一个能满足大小的空闲分区，分配给作业。首次适应算法保留了内存高地址部分的大空闲分区，有利于后续大作业的装入。但它会使内存低地址部分出现许多小碎片，而每次分配查找时都要经过这些分区，因此增加了开销。
		\item 邻近适应（Next Fit）算法。也称循环首次适应算法，由首次适应算法演变而成。不同之处是，分配内存时从上次查找结束的位置开始继续查找。邻近适应算法试图解决该问题。它让内存低、高地址部分的空闲分区以同等概率被分配，划分为小分区，导致内存高地址部分没有大空闲分区可用。通常比首次适应算法更差。
		\item 最佳适应（Best Fit）算法。空闲分区按容量递增的次序排列。每次分配内存时，顺序查找到第一个能满足大小的空闲分区，即最小的空闲分区，分配给作业。最佳适应算法虽然称为最佳，能更多地留下大空闲分区，但性能通常很差，因为每次分配会留下越来越多很小的难以利用的内存块，进而产生最多的外部碎片。
	\item 最坏适应（Worst Fit）算法。空闲分区按容量递减的次序排列。每次分配内存时，顺序查找到第一个能满足要求的空闲分区，即最大的空闲分区，从中分割一部分空间给作业。与最佳适应算法相反，最坏适应算法选择最大的空闲分区，这看起来最不容易产生碎片，但是把最大的空闲分区划分开，会很快导致没有大空闲分区可用，因此性能也很差。 \\
	综合来看，首次适应算法的开销小，性能最好，回收分区也不需要对空闲分区重新排序。
	\end{enumerate}
	
	\paragraph{（3）基于索引搜索的分配算法}
	当系统很大时，空闲分区链可能很长，此时采用顺序分配算法可能很慢。因此，在大、中型系统中往往采用索引分配算法。索引分配算法的思想是，根据其大小对空闲分区分类，对于每类（大小相同）空闲分区，单独设立一个空闲分区链，并设置一张索引表来管理这些空闲分区链。当为进程分配空间时，在索引表中查找所需空间大小对应的表项，并从中得到对应的空闲分区链的头指针，从而获得一个空闲分区。索引分配算法有以下三种。
	
	\begin{enumerate}
	\item 快速适应算法。空闲分区的分类根据进程常用的空间大小进行划分。分配过程分为两步：
	\begin{enumerate}
		\item 首先根据进程的长度，在索引表中找到能容纳它的最小空闲分区链表；
		\item 然后从链表中取下第一块进行分配。 
	\end{enumerate}
	优点是查找效率高、不会产生内存碎片；缺点是回收分区时，需要有效地合并分区，算法比较复杂，系统开销较大。
	
	\textbf{命题追踪：伙伴关系的概念（2024）}
	
	\item 伙伴系统。规定所有分区的大小均为 2 的 \( k \) 次幂（\( k \) 为正整数）。当需要为进程分配大小为 \( n \) 的分区时（\( 2^{i - 1} < n \leq 2^i \)），在大小为 \( 2^i \) 的空闲分区链中查找。若找到，则将该空闲分区分配给进程。否则，表示大小为 \( 2^i \) 的空闲分区已耗尽，需要在大小为 \( 2^{i + 1} \) 的空闲分区链中继续查找。若存在大小为 \( 2^{i + 1} \) 的空闲分区，则将其等分为两个分区，这两个分区称为一对伙伴，其中一个用于分配，而将另一个加入大小为 \( 2^i \) 的空闲分区链。若不存在，则继续查找，直至找到为止。回收时，需要要将相邻的空闲伙伴分区合并成更大的分区。
	
	\item 哈希算法。根据空闲分区链表的分布规律，建立哈希函数，构建一张以空闲分区大小为关键字的哈希表，每个表项记录一个对应空闲分区链的头指针。分配时，根据所需分区大小，通过哈希函数计算得到哈希表中的位置，从中得到相应的空闲分区链表。
	\end{enumerate}
	
	在连续分配方式中，我们发现，即使内存有超过 1GB 的空闲空间，但若没有连续的 1GB 空间，则需要 1GB 空间的作业仍然是无法运行的；但若采用非连续分配方式，则作业所要求的 1GB 内存空间可以分散地分配在内存的各个区域，当然，这也需要额外的空间去存储它们（分散区域）的索引，使得非连续分配方式的存储密度低于连续分配方式。非连续分配方式根据分区的大小是否固定，分为分页存储管理和分段存储管理。在分页存储管理中，又根据运行作业时是否要将作业的所有页面都装入内存才能运行，分为基本分页存储管理和请求分页存储管理。
	
	\subsection{基本分页存储管理}\footnote{本章后面的内容与《计算机组成原理考研复习指导》一书的 3.6 节高度相关，建议结合复习。}
	固定分区会产生内部碎片，动态分区会产生外部碎片，这两种技术对内存的利用率都比较低。我们希望内存的使用能尽量避免碎片的产生，这就引入了分页的思想：将内存空间分为若干固定大小（如 4KB）的分区，称为页框、页帧或物理块。进程的逻辑地址空间也分为与块大小相等的若干区域，称为页或页面。操作系统以页框为单位为各个进程分配内存空间。
	
	从形式上看，分页的方法像是分区相等的固定分区技术，分页管理不产生外部碎片。但它又有本质的不同点：块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申从形式上看，分页的方法像是分区相等的固定分区技术，分页管理不产生外部碎片。但它又有本质的不同点：块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申请主存可用空间并执行。这样，进程只会在为最后一个不完整的块申请一个主存块空间时，才产生主存碎片，所以尽管会产生内部碎片，但这种碎片相对进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片）。
	
	\subsubsection{1. 分页存储的几个基本概念}
	\paragraph{（1）页面和页面大小}
	进程的逻辑地址空间中的每个页面有一个编号，称为页号，从 0 开始；内存空间中的每个页框也有一个编号，称为页框号（或物理块号），也从 0 开始。进程在执行时需要申请内存空间，即要为每个页面分配内存中的可用页框，这就产生了页号和页框号的一一对应。
	
	为方便地址转换，页面大小应是 2 的整数次幂。同时页面大小应该适中，页面太小会使进程的页面数过多，这样页表就会过长，占用大量内存，而且也会增加硬件地址转换的开销，降低页面换入/换出的效率；页面过大又会使页内碎片增多，降低内存的利用率。
	
	\paragraph{（2）地址结构}
	\subparagraph{命题追踪：分页系统的逻辑地址结构（2009、2010、2013、2015、2017、2024）}
	某个分页存储管理的逻辑地址结构如图 3.7 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.45\textwidth]{5254.png}
		\label{fig:ttttttttt}
	\end{figure}
	
	地址结构包含两部分：前一部分为页号 \( P \)，后一部分为页内偏移量 \( W \)。地址长度为 32 位，其中 0～11 位为页内地址，即每页大小为 \( 2^{12}\text{B} \)；12～31 位为页号，即最多允许 \( 2 ^ {20} \) 页，在实际题目中，页号、页内偏移、逻辑地址可能是用十进制数给出的，注意进制之间的转换。
	
	\paragraph{（3）页表}
	为了便于找到进程的每个页面在内存中存放的位置，系统为每个进程建立一张页面映射表，简称页表。进程的每个页面对应一个页表项，每个页表项由页号和块号组成，它记录了页面在内存中对应的物理块号，如图 3.8 所示。进程执行时，通过查找页表，即可找到每页在内存中的物理块号。可见，页表的作用是实现从页号到物理块号的地址映射。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{0411.png}
		\label{fig:tttttttttt}
	\end{figure}
	
	\subsubsection{2. 基本地址变换机构}
	地址变换机构的任务是将逻辑地址转换为内存中的物理地址。地址变换是借助于页表实现的。图 3.9给出了分页存储管理系统中的地址变换机构。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.65\textwidth]{0544.png}
		\label{fig:ttttttttttt}
	\end{figure}
	
	\begin{tcolorbox}[colback=gray!20, colframe=gray!50, title={注 意}]
		在页表中，页表项连续存放，因此页号可以是隐含的，不占用存储空间。
	\end{tcolorbox}
	
	为了提高地址变换的速度，在系统中设置一个页表寄存器（PTR），存放页表在内存的始址 \( F \) 和页表长度 \( M \)。寄存器的造价昂贵，因此在单 CPU 系统中只设置一个页表寄存器。平时，进程未执行时，页表的始址和页表长度存放在本进程的 PCB 中。当进程被调度执行时，才将页表始址和页表长度装入页表寄存器中。设页面大小为 \( L \)，逻辑地址 \( A \) 到物理地址 \( E \) 的变换过程如下（假设逻辑地址、页号、每页的长度都是十进制数）：
	
	% 命题追踪部分，用不同颜色或样式突出，这里用灰色背景模拟原文效果
	\begin{tcolorbox}[colback=gray!10, colframe=gray!30, title={命题追踪}]
		页式系统的地址变换过程（2013、2021、2024）
	\end{tcolorbox}
	
	\begin{tcolorbox}[colback=gray!10, colframe=gray!30, title={命题追踪}]
		页表项地址的计算与分析（2024）
	\end{tcolorbox}
	
	\begin{enumerate}
		\item 根据逻辑地址计算出页号 \( P = \lfloor A / L \rfloor \)、页内偏移量 \( W = A \% L \)。（注：\(\lfloor  \rfloor\) 表示向下取整，\(\%\) 表示取余，与原文描述对应 ）
		\item 判断页号是否越界，若页号 \( P \geq \) 页表长度 \( M \)，则产生越界中断，否则继续执行。
		\item 在页表中查询页号对应的页表项，确定页面存放的物理块号。页号 \( P \) 对应的页表项地址 \( = \) 页表始址 \( F + \) 页号 \( P \times \) 页表项长度，取出该页表项内容 \( b \)，即为物理块号。
		\item 计算物理地址 \( E = bL + W \)，用物理地址 \( E \) 去访存。注意，物理地址 \( = \) 页面在内存中的始址\( + \)页内偏移量，页面在内存中的始址 \( = \) 块号\( \times \)块大小（页面大小）。
	\end{enumerate}
	
	以上整个地址变换过程均是由硬件自动完成的。例如，若页面大小 \( L \) 为 \( 1\text{KB} \)，页号 \( 2 \) 对应的物理块为 \( b = 8 \)，计算逻辑地址 \( A = 2500 \) 的物理地址 \( E \) 的过程如下：\( P = \lfloor 2500 / 1024 \rfloor = 2 \)（因为 \( 1\text{KB} = 1024 \text{B} \) ），\( W = 2500 \% 1024 = 452 \)，查找得到页号 \( 2 \) 对应的物理块的块号为 \( 8 \)，\( E = 8 \times 1024 + 452 = 8644 \)。
	
	计算条件用十进制数和用二进制数给出，过程会稍有不同。页式管理只需给出一个整数就能确定对应的物理地址，因为页面大小 \( L \) 是固定的。因此，页式管理中地址空间是一维的。
	
	页表项的大小不是随意规定的，而是有所约束的。\uline{如何确定页表项的大小？}
	
	页表项的作用是找到该页在内存中的位置。以 32 位内存地址空间、按字节编址、一页 \( 4\text{KB} \) 为例，地址空间内一共有 \( 2^{32}\text{B} / 4\text{KB} = 2^{20} \) 页，因此需要 \( \log_2 2^{20} = 20 \) 位才能保证表示范围能容纳所有页面，又因为内存以字节作为编址单位，即页表项的大小 \( \geq \lceil 20 / 8 \rceil = 3\text{B} \)。所以在这个条件下，为了保证页表项能够指向所有页面，页表项的大小应大于或等于 \( 3\text{B} \)。当然，也可以选择更大的页表项，让一个页面能够正好容纳整数个页表项，或便于增加一些其他信息。
	
	下面讨论分页管理方式存在的两个主要问题：①每次访存操作都需要进行逻辑地址到物理地址的转换，地址转换过程必须足够快，否则访存速度会降低；②每个进程引入页表，用于存储映射机制，页表不能太大，否则内存利用率会降低。
	
	\subsubsection{3. 具有快表的地址变换机构}
	由上面介绍的地址变换过程可知，若页表全部放在内存，则存取一个数据或一条指令至少要访问两次内存：第一次是访问页表，确定所存取的数据或指令的物理地址；第二次是根据该地址存取数据或指令。显然，这种方法比通常执行指令的速度慢了一半。
	
	为此，在地址变换机构中增设一个具有并行查找能力的高速缓冲存储器——快表（TLB），也称相联存储器，用来存放当前访问的若干页表项，以加速地址变换的过程。与此对应，主存中的页表常称为慢表。具有快表的地址变换机构如图 3.10 所示。
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{2055.png}
		\label{fig:t2}
	\end{figure}
	
	\begin{tcolorbox}[colback=gray!10, colframe=gray!30, title={命题追踪}]
		具有快表的地址变换的性能分析（2009）
	\end{tcolorbox}
	
	在具有快表的分页机制中，地址的变换过程如下：
	\begin{enumerate}
		\item CPU 给出逻辑地址后，由硬件进行地址转换，将页号与快表中的所有页号进行比较。
		\item 若找到匹配的页号，说明要访问的页表项在快表中有副本，则直接从中取出该页对应的物理块号，与页内偏移量拼接形成物理地址。这样，存取数据仅一次访存即可实现。
		\item 若未找到匹配的页号，则需要访问内存中的页表，读出页表项后，就能得到该页的物理块号，再与页内偏移量拼接形成物理地址，最后用该物理地址去访存。若快表未命中，则存取数据需要两次访存。注意，找到页表项后，应同时将其存入快表，以便后面可能的再次访问。若快表已满，则须按照特定的算法淘汰一个旧页表项。
	\end{enumerate}
	
	一般快表的命中率可达 90\%以上，这样分页带来的速度损失就可降低至 10\%以下。快表的有效性基于著名的局部性原理，后面讲解虚拟内存时将具体讨论它。
	
	\subsubsection{4. 两级页表}
	引入分页管理后，进程在执行时不需要将所有页调入内存页框，而只需将保存有映射关系的页表调入内存。但仍需考虑页表的大小。以 32 位逻辑地址空间、页面大小 \( 4\text{KB} \)、页表项大小 \( 4\text{B} \) 为例：页内偏移为 \( \log_2 4\text{K} = 12 \) 位，页号部分为 \( 20 \) 位，则每个进程页表中的页表项数可达 \( 2^{20} \) 之多，仅页表就要占用 \( 2^{20} \times 4\text{B} / 4\text{KB} = 1\text{K} \) 个页，还要求是连续的。这显然是不切实际的。
	
	\begin{tcolorbox}[colback=gray!10, colframe=gray!30, title={命题追踪}]
		多级页表的特点和优点（2014）
	\end{tcolorbox}
	
	解决上述问题的方法有两种：①对于页表所需的内存空间，采用离散分配方式，用一张索引表来记录各个页表的存放位置，这就解决了页表占用连续内存空间的问题；②只将当前需要的部分页表项调入内存，其余的页表项仍驻留磁盘，需要时再调入（虚拟内存的思想），这就解决了页表占用内存过多的问题。读者也许发现这个方案就和当初引进页表机制的思路一模一样，实际
	
	
	
	\chapter{数据链路层}
	
	\section*{【考纲内容】}
	
	\begin{enumerate}
		\item 数据链路层的功能
		\item 组帧
		\item 差错控制
		
		
检错编码：纠错编码
		
		\item 流量控制与可靠传输机制
		
		
流量控制、可靠传输与滑动窗口机制；停止--等待协议；
		
后退 $N$ 帧协议（GBN）；选择重传协议（SR）
		
		\item 介质访问控制
		\begin{enumerate}
			\item 信道划分：频分复用、时分复用、波分复用、码分复用
			\item 随机访问：ALOHA 协议；CSMA 协议；CSMA/CD 协议；CSMA/CA 协议
			\item 轮询访问：令牌传递协议
		\end{enumerate}
		\item 局域网
		
		
局域网的基本概念与体系结构；以太网与 IEEE 802.3；
		
IEEE 802.11 无线局域网；VLAN 的基本概念与基本原理
		
		\item 广域网
		
		
广域网的基本概念；点对点协议（PPP）
		
		\item 数据链路层设备
		
		
以太网交换机及其工作原理
		
	\end{enumerate}
	
	\section*{【复习提示】}
	
	本章是历年考试中考查的重点。要求在了解数据链路层基本概念和功能的基础上，重点掌握滑动窗口机制、三种可靠传输协议、各种 MAC 协议，特别是 CSMA/CD 协议、CSMA/CA 协议和以太网帧格式，以及局域网的争用期和最小帧长的概念、二进制指数退避算法。此外，中继器、网卡、集线器、网桥和局域网交换机的原理及区别也要重点掌握。
	
		\section{介质访问控制}
	介质访问控制所要完成的主要任务是，为使用介质的每个节点隔离来自同一信道上其他节点所传送的信号，以协调活动节点的传输。图\ref{fig:broadcast_communication}是广播信道的通信方式，节点A、B、C、D、E共享广播信道，假设A要与C通信，B要与D通信，因为它们共用一条信道，若不加控制，则两对节点之间的通信可能会因互相干扰而失败。用来决定广播信道中信道分配的协议属于数据链路层的一个子层，称为介质访问控制（Medium Access Control，MAC）子层。
	
	\begin{figure}[h]
		\centering
		\caption{广播信道的通信方式}
		\label{fig:broadcast_communication}
		\includegraphics[width=0.2\textwidth]{图315.png} % 请替换为实际图片路径
	\end{figure}
	
	常见的介质访问控制方法有信道划分介质访问控制、随机访问介质访问控制和轮询访问介质访问控制。其中前者是静态划分信道的方法，而后两者是动态分配信道的方法。
	
	
	
	\subsection{信道划分介质访问控制}
	
	信道划分介质访问控制将使用同一传输介质的多个设备的通信隔离开来，把时域和频域资源合理地分配给这些设备。信道划分介质访问控制通过复用技术实现。所谓复用，是指在发送端把多个发送方的信号组合在一条物理信道上进行传输，在接收端把收到的复用信号分离出来，并发送给对应的接收方，如图\ref{fig:multiplexing}所示。当传输介质的带宽超过传输单个信号所需的带宽时，通过在一条介质上传输多个信号，还能提高传输系统的利用率。
	
	信道划分的实质是通过分时、分频、分码等方法，将原来的一个广播信道，逻辑上分为几个用于在两个节点之间进行通信的互不干扰的子信道，即将广播信道转变为若干个点对点信道。
	
	信道划分介质访问控制分为以下4种。
	
	\begin{figure}[h]
		\centering
		\caption{复用技术原理示意图}
		\label{fig:multiplexing}
		\includegraphics[width=0.3\textwidth]{图316.png} % 请替换为实际图片路径
	\end{figure}
	
	\subsubsection{时分复用（TDM）}
	时分复用（Time Division Multiplexing, TDM）是指将信道的传输时间划分为一段段等长的时间片，称为TDM帧，每个用户在每个TDM帧中占用固定序号的时隙，每个用户所占用的时隙周期性地出现（其周期就是TDM的长度），所有用户在不同的时间占用同样的信道资源，如图\ref{fig:tdm_principle}所示。TDM帧实际上是一段固定长度的时间，它与数据链路层的帧不是同一个概念。
	
	\begin{figure}[h]
		\centering
		\caption{时分复用的原理示意图}
		\label{fig:tdm_principle}
		\includegraphics[width=0.4\textwidth]{图317.png} % 请替换为实际图片路径
	\end{figure}
	
	从某个时刻来看，时分复用信道上传送的仅是某对用户之间的信号；从某段时间来看，传送的是按时间分割的复用信号。因为时分复用是按固定次序给用户分配时隙的，当用户在某段时间暂无数据传输时，其他用户也无法使用这个暂时空闲的线路资源，所以时分复用后的信道利用率不高。统计时分复用（Statistic TDM, STDM）也称异步时分复用，它是对TDM的一种改进。STDM帧与TDM帧不同，它并不固定分配时隙，而按需动态分配时隙，当用户有数据要传送时，才会分配到STDM帧中的时隙，因此可以提高线路的利用率。例如，假设线路的数据传输速率为6000b/s，3个用户的平均速率都为2000b/s，当采用TDM方式时，每个用户的最高速率为2000b/s，而在STDM方式下，每个用户的最高速率可达6000b/s。
	
	\subsubsection{频分复用（FDM）}
	频分复用（Frequency Division Multiplexing, FDM）是指将信道的总频带划分为多个子频带，每个子频带作为一个子信道，每对用户使用一个子信道进行通信，如图\ref{fig:fdm_principle}所示。所有用户在同一时间占用不同的频带资源。每个子信道分配的频带可不相同，但它们的总和不能超过信道的总频带。在实际应用中，为了防止子信道之间互相干扰，相邻信道间还要加入“隔离频带”。
	
	\begin{figure}[h]
		\centering
		\caption{频分复用的原理示意图}
		\label{fig:fdm_principle}
		\includegraphics[width=0.4\textwidth]{图318.png} % 请替换为实际图片路径
	\end{figure}
	
	频分复用的优点在于充分利用了传输介质的带宽，系统效率较高，实现也较容易。
	
	\subsubsection{波分复用（WDM）}
	波分复用（Wavelength Division Multiplexing, WDM）即光的频分复用，它在一根光纤中传输多种不同波长（频率）的光信号，因为波长不同，各路光信号互不干扰，最后用光分用器将各路波长分解出来。因为光波处于频谱的高频段，有很大的带宽，所以可以实现多路的波分复用。
	
	\subsubsection{码分复用（CDM）}
	码分复用（Code Division Multiplexing, CDM）是采用不同的编码来区分各路原始信号的一种复用方式。与FDM和TDM不同，它既共享信道的频率，又共享时间。
	
	实际上，更常用的名词是码分多址（Code Division Multiple Access, CDMA），其原理是将每个比特时间再划分成$m$个短的时间槽，称为码片（Chip），通常$m$的值是64或128，下例中为简单起见，设$m$为8。每个站点被指派一个唯一的$m$位码片序列。发送1时，站点发送它的码片序列；发送0时，站点发送该码片序列的反码。当两个或多个站点同时发送时，各路数据在信道中线性相加。为了从信道中分离出各路信号，要求各个站点的码片序列相互正交。
	
	简单理解就是，A站向C站发出的信号用一个向量来表示，B站向C站发出的信号用另一个向量来表示，两个向量要求相互正交。向量中的分量，就是所谓的码片。
	
	下面举例说明CDMA的原理。
	
	令向量$\boldsymbol{S}$表示A站的码片向量，$\boldsymbol{T}$表示B站的码片向量。假设A站的码片序列被指派为00011011，则A站发送00011011就表示发送比特1，发送11100100就表示发送比特0。为了方便计算，将码片中的0写为 - 1，将1写为 + 1，因此A站的码片序列是（-1 -1 -1 +1 +1 -1 +1 +1）。
	
	不同站的码片序列相互正交，即向量$\boldsymbol{S}$和$\boldsymbol{T}$的规格化内积为0：
	\[
	\boldsymbol{S} \cdot \boldsymbol{T} \equiv \frac{1}{m}\sum_{i = 1}^{m}S_iT_i = 0
	\]
	
	任何站的码片向量和该码片向量自身的规格化内积都是1：
	\[
	\boldsymbol{S} \cdot \boldsymbol{S} = \frac{1}{m}\sum_{i = 1}^{m}S_iS_i = \frac{1}{m}\sum_{i = 1}^{m}S_i^2 = \frac{1}{m}\sum_{i = 1}^{m}(\pm 1)^2 = 1
	\]
	
	任何站的码片向量和该码片反码的向量的规格化内积都是 - 1：
	\[
	\boldsymbol{S} \cdot \overline{\boldsymbol{S}} = \frac{1}{m}\sum_{i = 1}^{m}S_i\overline{S}_i = -\frac{1}{m}\sum_{i = 1}^{m}S_i^2 = -1
	\]
	
	令向量$\boldsymbol{T}$为（-1 -1 +1 -1 +1 +1 +1 -1）。
	
	当A站向C站发送数据1时，就发送了向量（-1 -1 -1 +1 +1 -1 +1 +1）。
	
	当B站向C站发送数据0时，就发送了向量（+1 +1 -1 +1 -1 -1 -1 +1）。
	
	两个向量在公共信道上叠加，实际上是线性相加，得到
	\[
	\boldsymbol{S} + \overline{\boldsymbol{T}} = (0 \ 0 \ -2 \ 2 \ 0 \ -2 \ 0 \ 2)
	\]
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad 码分复用中数据分离的计算（2014）
	\end{tcolorbox}
	
	到达C站后，进行数据分离，若要得到来自A站的数据，则C站就必须知道A站的码片序列，让$\boldsymbol{S}$与$\boldsymbol{S} + \overline{\boldsymbol{T}}$进行规格化内积。根据叠加原理，其他站点的信号都在内积的结果中被过滤掉，内积的相关项都是0，而只剩下A站发送的信号，得到
	\[
	\boldsymbol{S} \cdot (\boldsymbol{S} + \overline{\boldsymbol{T}}) = 1
	\]
	
	所以A站发出的数据是1。同理，若要得到来自B站的数据，则
	
	\[
	\boldsymbol{T} \cdot (\boldsymbol{S} + \overline{\boldsymbol{T}}) = -1
	\]
	
	因此从B站发送过来的信号向量是一个反码向量，代表0。
	
	规格化内积是线性代数中的内容，它在得到两个向量的内积后，再除以向量的分量的个数。
	
	下面举一个直观的例子来理解频分复用、时分复用和码分复用。
	
	假设A站要向C站运送黄豆，B站要向C站运送绿豆，A站和B站与C站之间有一条公共道路，可类比为广播信道。在频分复用方式下，公共道路被划分为两个车道，分别提供给A站到C站、B站到C站的车通行，两类车可同时通行，但都只分到了道路的一半，因此频分复用（波分复用也一样）共享时间而不共享空间。在时分复用方式下，先让A站到C站的车走一趟，再让B站到C站的车走一趟，两类车交替地使用道路，因此时分复用共享空间，但不共享时间。码分复用与另外两种信道划分方式极为不同，在这种方式下，黄豆与绿豆放在同一辆车上运送，到达C站后，由C站负责把车上的黄豆和绿豆分开，因此码分复用既共享空间，又共享时间。
	
	码分复用技术具有频谱利用率高、抗干扰能力强、保密性强、语音质量好等优点，还可以减少投资及降低运行成本，主要用于无线通信系统，特别是移动通信系统。
	
	\subsection{随机访问介质访问控制}
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad 信道划分与随机访问介质访问控制的特点（2014）
	\end{tcolorbox}
	
	在随机访问协议中，不采用集中控制方式解决发送信息的次序问题，所有用户都能根据自己的意愿随机地发送信息，占用信道的全部速率。在总线形网络中，当有两个或多个用户同时发送信息时，就会产生帧冲突（也称碰撞），导致所有冲突用户的发送均以失败告终。为了解决随机访问发生的冲突，每个用户需要按照一定的规则反复地重传它的帧，直到该帧无冲突地通过，这些规则就是随机访问介质访问控制协议，其核心思想是：胜利者通过争用获得信道，进而获得信息的发送权。因此，随机访问介质访问控制协议也称争用型协议。
	
	可见，若采用信道划分机制，则节点之间的通信要么共享空间，要么共享时间，要么共享空间和时间；而若采用随机访问控制机制，则节点之间的通信既不共享时间，又不共享空间。因此，随机介质访问控制实质上是一种将广播信道转换为点到点信道的机制。
	
	\subsubsection{ALOHA协议}
	
	ALOHA协议分为纯ALOHA协议和时隙ALOHA协议两种。
	
	\subsubsection*{纯ALOHA协议}
	纯ALOHA协议的基本思想是，当总线形网络中的任何站点需要发送数据时，可以不进行任何检测就发送数据。若在一段时间内未收到确认，则该站点就认为传输过程中发生了冲突。发送站点需要等待一段随机的时间后再发送数据，直至发送成功。
	
	图\ref{fig:pure_aloha_principle}表示一个纯ALOHA协议的工作原理。每个站均可自由地发送数据帧，假定所有帧都是定长的，帧长不用比特而用发送这个帧所需的时间来表示，图中用$T_0$表示这段时间。
	
	\begin{figure}[h]
		\centering
		\caption{纯ALOHA协议的工作原理}
		\label{fig:pure_aloha_principle}
		\includegraphics[width=0.5\textwidth]{图319.png} % 请替换为实际图片路径
	\end{figure}
	
	在图\ref{fig:pure_aloha_principle}的例子中，当站1发送帧1时，其他站都未发送数据，所以站1的发送必定是成功的。但随后站2和站$N - 1$发送的帧2和帧3在时间上重叠了一部分（发生了冲突）。发生冲突的各站都必须进行重传，但并不能马上进行重传，因为这样做必然导致继续发生冲突。因此，让各站等待一段随机的时间，然后进行重传。若再次发生冲突，则需要再等待一段随机的时间，直到重传成功为止。图中其余一些帧的发送情况是，帧4发送成功，而帧5和帧6发生冲突。
	
	纯ALOHA网络的吞吐量很低，为了克服这个缺点，便产生了时隙ALOHA协议。
	
	\subsubsection*{时隙ALOHA协议}
	时隙ALOHA协议同步各站点的时间，将时间划分为一段段等长的时隙（Slot），规定站点只能在每个时隙开始时才能发送帧，发送一帧的时间必须小于或等于时隙的长度。这样就避免了用户发送数据的随意性，降低了产生冲突的可能性，提高了信道的利用率。
	
	图\ref{fig:slotted_aloha_principle}表示两个站的时隙ALOHA协议的工作原理。每个帧到达后，一般都要在缓存中等待一段小于时隙$T_0$的时间，才能发送出去。当在一个时隙内有两个或两个以上的帧到达时，在下一个时隙将产生冲突。冲突后重传的策略与纯ALOHA协议的情况相似。
	
	\begin{figure}[h]
		\centering
		\caption{两个站的时隙ALOHA协议的工作原理}
		\label{fig:slotted_aloha_principle}
		\includegraphics[width=0.5\textwidth]{图320.png} % 请替换为实际图片路径
	\end{figure}
	
	\subsubsection{CSMA协议}
	ALOHA网络发生冲突的概率很大。若每个站点在发送前都先监听公用信道，发现信道空闲后再发送，会大大降低冲突的可能性，从而提高信道的利用率，载波监听多路访问（Carrier Sense Multiple Access, CSMA）协议依据的正是这一思想。CSMA协议是在ALOHA协议基础上提出的。
	
	一种改进协议，它与ALOHA协议的主要区别是多了一个载波监听装置。
	
	根据监听方式和监听到信道忙后的处理方式的不同，CSMA协议分为三种。
	
	\subsubsection*{1-坚持CSMA}
	1 - 坚持CSMA的基本思想是：当站点要发送数据时，首先监听信道；若信道空闲，则立即发送数据；若信道忙，则继续监听直至信道空闲。“坚持”的含义是监听到信道忙时，继续坚持监听信道；“1”的含义是监听到信道空闲时，立即发送帧的概率为1。
	
	\subsubsection*{非坚持CSMA}
	非坚持CSMA的基本思想是：当站点要发送数据时，首先监听信道；若信道空闲，则立即发送数据；若信道忙，则放弃监听，等待一个随机的时间后，再重新监听。
	
	非坚持CSMA协议在监听到信道忙时就放弃监听，因此降低了多个站点等待信道空闲后同时发送数据导致冲突的概率，但也增加了数据在网络中的平均时延。
	
	\subsubsection*{$p$ - 坚持CSMA}
	$p$ - 坚持CSMA只适用于时分信道，其基本思想是：当站点要发送数据时，首先监听信道；若信道忙，则持续监听（等到下一个时隙再监听），直至信道空闲；若信道空闲，则以概率$p$发送数据，以概率$1 - p$推迟到下一个时隙再继续监听；直到数据发送成功。
	
	$p$ - 坚持CSMA检测到信道空闲后，以概率$p$发送数据，以概率$1 - p$推迟到下一个时隙继续监听，目的是降低1 - 坚持CSMA中多个站点检测到信道空闲时同时发送帧的冲突概率；采用坚持“监听”的目的是，克服非坚持CSMA中因随机等待造成的延迟时间较长的缺点。因此，$p$ - 坚持CSMA协议是非坚持CSMA协议和1 - 坚持CSMA协议的折中。
	
	三种不同类型的CSMA协议比较如表\ref{table:csma_comparison}所示。
	
	\begin{table}[h]
		\centering
		\caption{三种不同类型的CSMA协议比较}
		\label{table:csma_comparison}
		\begin{tabular}{ccc c} % 四列，左中右对齐可自定义
			\toprule
			信道状态 & 1 - 坚持 & 非坚持 & $p$ - 坚持 \\
			\midrule
			空闲 
			& 立即发送数据 
			& 立即发送数据 
			& 以概率 $p$ 发送数据，\\
			&       &       & 以概率 $1 - p$ 推迟到下一个时隙 \\
			忙 
			& 继续坚持监听 
			& 放弃监听，等待一个随机的时间后再监听 
			& 持续监听，等到下一个时隙再发送，\\
			&       &       & 直至信道空闲 \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\subsubsection{CSMA/CD协议}
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad CSMA/CD协议的特点（2015）
	\end{tcolorbox}
	载波监听多路访问/冲突检测（CSMA/CD）协议是CSMA协议的改进方案，适用于总线形网络或半双工网络环境。对于全双工网络，因为全双工采用两条信道，分别用来发送和接收，在任何时候，发收双方都可以发送或接收数据，不可能产生冲突，所以不需要CSMA/CD协议。
	
	载波监听是指每个站点在发送前和发送过程中都必须不停地检测信道，在发送前检测信道是为了获得发送权，在发送过程中检测信道是为了及时发现发送的数据是否发生冲突。站点要在发送数据前先监听信道，只有信道空闲时才能发送。冲突检测（Collision Detection）就是边发送边检测，适配器边发送数据边检测信道上电压的变化情况，当检测到电压的变化幅度超过一定的门限值时，表明发生了冲突，适配器要立即停止发送数据，等待一段随机时间后再次发送。
	
	CSMA/CD的工作流程可简单地概括为“先听后发，边听边发，冲突停发，随机重发”。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad 信道发生冲突的最短、最长时间的分析（2010）
	\end{tcolorbox}
	电磁波在总线上的传播速率总是有限的。因此，当某时刻发送站检测到信道空闲时，信道不一定空闲。如图\ref{fig:csma_cd_collision}所示，设$\tau$为单程传播时延。当$t = 0$时，A站发送数据。当$t = \tau - \delta$时，A站发送的数据还未到达B站，因为B站检测到信道空闲而发送数据。经过时间$\delta/2$后，即当$t = \tau - \delta/2$时，A站发送的数据和B站发送的数据发生冲突，但这时A站和B站都不知道。当$t = \tau$时，B站检测到冲突，于是停止发送数据。当$t = 2\tau - \delta$时，A站检测到冲突，也停止发送数据。至此，A站和B站发送数据均失败，都要推迟一段时间后重新发送。
	
	
	
	\begin{figure}[h]
		\centering
		\caption{CSMA/CD冲突情况示意图}
		\label{fig:csma_cd_collision}
		\includegraphics[width=0.6\textwidth]{图321.png} % 请替换为实际图片路径
	\end{figure}
	
	从图\ref{fig:csma_cd_collision}不难看出，A站在开始发送数据后最多经过时间$2\tau$（端到端传播时延的2倍）就能知道有没有发生冲突（当$\delta \to 0$时）。因此，把以太网的端到端往返传播时延$2\tau$称为争用期（也称冲突窗口）。每个站在自己发送数据后的一小段时间内，存在发生冲突的可能性，只有经过争用期这段时间还未检测到冲突时，才能确定这次发送不会发生冲突。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad CSMA/CD最短帧长的理解和相关计算（2009、2016、2019、2022）
	\end{tcolorbox}
	
	现在考虑一种情况：某站发送一个很短的帧，但在发送完之前并未检测出冲突。假定这个帧在继续向前传播到达目的站之前和别的站发送的帧发生了冲突，因此目的站将收到有差错的帧（当然会把它丢弃）。然而，发送站却不知道发生了冲突，因此不会重传这个帧。为了避免发生这种情况，以太网规定了一个最短帧长（争用期内可发送的数据长度）。在争用期内若检测到冲突，就停止发送，此时已发送出去的数据一定小于最短帧长，因此凡长度小于这个最短帧长的帧，就都是因为冲突而异常中止的无效帧。最短帧长的计算公式为：
	\[ \text{最短帧长} = \text{最大单向传播时延} \times \text{数据传输速率} \times 2 \]
	
	例如，以太网规定$51.2\mu s$为争用期的长度。对于$10Mb/s$的以太网，在争用期内可发送$512bit$，即$64B$。当以太网发送数据时，若前$64B$未发生冲突，则后续数据也不会发生冲突（表示已成功抢占信道）。换句话说，若发生冲突，则一定在前$64B$。因为一旦检测到冲突就立即停止发送，所以这时发送出去的数据一定小于$64B$。于是，以太网规定最短帧长为$64B$，凡长度小于$64B$的帧，就都是因为冲突而异常中止的无效帧，收到这种无效帧时应立即丢弃。
	
	若只发送小于$64B$的帧，如$40B$的帧，则需要在MAC子层中于数据字段的后面加一个整数字节的填充字段，以保证以太网的MAC帧的长度不小于$64B$。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad 二进制指数退避算法的应用（2023）
	\end{tcolorbox}
	
	一旦发生冲突，参与冲突的两个站点紧接着再次发送就没有意义，若坚持这样做，则将导致无休止的冲突。CSMA/CD采用截断二进制指数退避算法来确定冲突后重传的时机，它让发生冲突的站点在停止发送后，推迟一个随机的时间再重新发送。算法精髓如下：
	
	1. 确定基本退避时间，一般取2倍的总线端到端的传播时延$2\tau$（争用期）。
	
	2) 从离散的整数集合$[0, 1, \cdots, (2^{k}-1)]$中随机取出一个数，记为$r$，重传所需推迟的时间就是$r$倍的争用期，即$2r\tau$。参数$k = \min[$重传次数, 10$]$，可见当重传次数不超过10时，参数$k$等于重传次数；但当重传次数超过10时，$k$就不再增大，而一直等于10。
	
	3) 当重传达16次仍不成功时，说明网络太拥挤，认为该帧永远无法正确发出，抛弃该帧并向高层报告出错（这个条件容易忽略，请读者注意）。
	
	假设适配器首次试图传送一帧，且在传送过程中检测到冲突。第1次重传时，$k = 1$，随机数$r$从整数集合$\{0, 1\}$中选择，可选的重传推迟时间是$0$或$2\tau$。若再次发生冲突，则第二次重传时，随机数$r$从整数集合$\{0, 1, 2, 3\}$中选择，因此重传推迟时间是在$0, 2\tau, 4\tau, 6\tau$这四个时间中随机选取的一个，以此类推。使用截断二进制指数退避算法可使重传需要推迟的平均时间随重传次数的增大而增大（也称动态退避），因此能降低发生冲突的概率，有利于整个系统的稳定。
	
	以太网还规定帧间最小间隔为$9.6\mu s$，相当于发送96比特的时间。这样做是为了使刚刚收到数据帧的站的接收缓存来得及清理，为接收下一帧做好准备。
	
	CSMA/CD协议的归纳如下：
	\begin{enumerate}
		\item 准备发送：适配器从网络层获得一个分组，封装成帧，放入适配器的缓存。
		\item 检测信道：若信道忙，则持续检测，直至信道转为空闲；若在$9.6\mu s$的时间内信道保持空闲（保证帧间最小间隔），则发送这个帧。
	\end{enumerate}
	
	③ 在发送过程中，适配器仍然持续检测信道。这里只有如下两种可能。
	\begin{itemize}
		\item 发送成功：在争用期内一直未检测到冲突，该帧肯定能发送成功。
		\item 发送失败：在争用期内检测到冲突，此时立即停止发送，适配器执行退避算法，等待一段随机时间后返回步骤②。若重传16次仍不能成功，则停止重传并向上报错。
	\end{itemize}
	
	\subsubsection{CSMA/CA协议}
	CSMA/CD协议已成功用于有线连接的局域网，但无线局域网不能简单地搬用CSMA/CD协议。无线局域网仍然使用CSMA，但无法使用冲突检测，主要有两个原因：
	
	1) 适配器接收到的信号强度往往远小于发送信号的强度，且在无线介质上信号强度的动态变化范围很大，因此若要实现冲突检测，则硬件上的花费会过大。
	
	2) 在无线通信中，并非所有站点都能够听见对方（但能产生冲突），即存在“隐蔽站”问题，从而使得冲突检测机制并不能检测到所有的冲突。
	
	为此，802.11标准定义了广泛用于无线局域网的CSMA/CA协议，它对CSMA/CD协议进行修改，将冲突检测改为冲突避免（Collision Avoidance, CA）。“冲突避免”并不是指协议可以完全避免冲突，而是指协议的设计要尽量降低冲突发生的概率。因为802.11无线局域网不使用冲突检测，一旦站点开始发送一个帧，就会完全发送该帧，但冲突存在时仍发送整个帧（尤其是长数据帧）会严重降低网络的效率，所以要采用冲突避免技术来降低冲突的概率。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad CSMA/CA协议的确认机制（2011）
	\end{tcolorbox}
	
	因为无线信道的通信质量远不如有线信道，所以802.11标准使用链路层确认/重传（ARQ）方案，即站点每通过无线局域网发送完一帧，就要在收到对方的确认帧后才能继续发送下一帧。可见，802.11标准无线局域网采用的停止 - 等待协议是一种可靠传输协议。
	
	为了尽量避免冲突，802.11标准规定，所有站检测到信道空闲后，还要等待一段很短的时间（继续监听）才能发送帧。这段时间称为帧间间隔（InterFrame Space, IFS）。帧间间隔的长短取决于站点要发送的帧的类型。802.11标准使用了下列三种IFS。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad CSMA/CA协议的帧间间隔（2020）
	\end{tcolorbox}
	
	1) SIFS（短IFS）：最短的IFS，用来分隔属于一次对话的各帧，使用SIFS的帧类型有ACK帧、CTS帧、分片后的的数据帧，以及所有回答AP探询的帧等。
	
	2) PIFS（点协调IFS）：中等长度的IFS，在PCF方式中使用\footnote{802.11 标准定义了两个 MAC 子层：
		
		分布式协调功能（DCF）：不采用任何中心控制，各站通过争用信道来获得发送权，是 802.11 标准的默认方式，必须实现。其工作原理基于 CSMA/CA 协议，站点通过监听信道、利用帧间间隔（如 DIFS 等）以及随机退避等机制来竞争信道发送数据 ，以避免冲突。
		
		点协调功能（PCF）：使用 AP 集中控制，用类似于探询的方法将发送权轮流交给各站，向上提供无争用服务。它是 802.11 标准的可选方式，实际中很少使用。PCF 依赖 AP 的协调，AP 会按照一定规则依次询问各个站点是否有数据要发送，从而有序地分配信道资源 ，减少冲突发生，但这种集中控制方式在一些场景下灵活性和效率欠佳。}。
	
	3) DIFS（分布式协调IFS）：最长的IFS，在DCF方式下用来发送数据帧和管理帧。
	
	802.11标准规定各站在发送数据之前，必须监听信道，只要监听到信道忙，就不能发送数据。802.11标准还规定各站采用虚拟载波监听机制，即让源站将它要占用信道的持续时间（包括目的站发回ACK帧所需的时间）及时通知给所有其他站，以便使所有其他站在这段时间内都停止发送，这样就大大减少了冲突的概率。“虚拟载波监听”表示其他站并未监听信道，而是因收到了源站的通知才不发送数据，这种效果就像是其他站都监听了信道。
	
	当信道从忙态转为空闲时，任何一个站要发送数据帧，不仅要等待一个DIFS的间隔，还要进入争用窗口，计算随机退避时间以便再次试图访问信道（避免多个站同时发送），因此降低了冲突发生的概率。当且仅当检测到信道空闲且这个数据帧是要发送的第一个数据帧时，才不使用退避算法，其他所有情况都必须使用退避算法，具体为：①在发送第一个帧之前检测到信道忙；②每次重传；③每次成功发送后要发送下一帧。CSMA/CA的退避算法与CSMA/CD稍有不同，第$i$次退避在$[0 \cdots, (2^{2 + k} - 1)]$个时隙中随机选择一个，扩大了随机选择退避时间的范围。当时隙范围最大达到255时（对应于第6次退避），就不再增加，不同于CSMA/CD的1023。
	
	CSMA/CA算法的归纳如下：
	
	1. 若站点最初有数据要发送（而非发送不成功再进行重传），且检测到信道空闲，那么在等待时间DIFS后，就发送整个数据帧。
	
	2. 否则，站点执行CSMA/CA退避算法，选取一个随机退避值。一旦检测到信道忙，退避计时器就保持不变。只要信道空闲，退避计时器就进行倒计时。
	
	3. 当退避计时器减至0时（这时信道只可能是空闲的），站点就发送整个帧并等待确认。
	
	4. 发送站若收到确认，就知道已发送的帧被目的站正确接收。这时要发送第二帧，就要从步骤2）开始，执行CSMA/CA退避算法，随机选定一段退避时间。
	
	若发送站在规定时间（由重传计时器控制）内未收到确认帧ACK，就必须重传该帧，再次使用CSMA/CA协议争用该信道，直到收到确认，或经过若干次重传失败后放弃发送。
	
	\subsubsection*{处理隐蔽站问题：RTS和CTS}
	在图\ref{fig:hidden_station_problem}中，站A和站B都在AP的覆盖范围内，但站A和站B相距较远，彼此都听不见对方。当站A和站B检测到信道空闲时，都向AP发送数据，导致冲突发生，这就是隐蔽站问题。
	
	\begin{figure}[h]
		\centering
		\caption{A站和B站同时向AP发送信号，发生冲突}
		\label{fig:hidden_station_problem}
		\includegraphics[width=0.4\textwidth]{图322.png} % 请替换为实际图片路径
	\end{figure}
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad CSMA/CA协议的信道预约方法（2018）
	\end{tcolorbox}
	
	为了避免该问题，802.11标准允许发送站对信道进行预约，如图\ref{fig:rts_cts_mechanism}所示。
	
	\begin{figure}[h]
		\centering
		\caption{使用RTS和CTS帧的冲突避免}
		\label{fig:rts_cts_mechanism}
		\includegraphics[width=0.4\textwidth]{图323.png} % 请替换为实际图片路径
	\end{figure}
	
	1. 源站发送数据之前，先监听信道，若信道空闲，则等待时间DIFS后，广播一个请求发送RTS（Request To Send）控制帧，它包括源地址、目的地址和这次通信所需的持续时间。
	
	2. 若AP正确收到RTS帧，且信道空闲，则等待时间SIFS后，广播一个允许发送CTS（Clear To Send）控制帧，它也包括这次通信所需的持续时间。

	3. 源站收到CTS帧后，再等待时间SIFS，就可发送数据帧。

	4. 若AP正确收到了源站发来的数据，则等待时间SIFS后就向源站发送确认帧ACK。
	
	AP覆盖范围内的其他站听到CTS帧后，将在CTS帧中指明的时间内抑制发送。CTS帧有两个目的：①给源站明确的发送许可；②指示其他站在预约期内不要发送。
	
	\begin{tcolorbox}[colframe=black, colback=white]
		\kaishu \textbf{命题追踪} \quad NAV值的分析（2024）
	\end{tcolorbox}
	
	需要说明的是，源站在RTS帧中填写的所需占用信道的持续时间，是从收到RTS帧后，到目的站最后发送完ACK帧为止的时间，即“SIFS + CTS + SIFS + 数据帧 + SIFS + ACK”。而AP在CTS帧中填写的所需占用信道的持续时间，是从收到CTS帧后，到目的站最后发送完ACK帧为止的时间，即“SIFS + 数据帧 + SIFS + ACK”。
	
	在图\ref{fig:hidden_station_problem}中，虽然站B检测不到站A发送给AP的RTS帧，但却能检测到AP发送给站A的CTS帧，站B根据CTS帧中的持续时间设置自己的网络分配向量（NAV），NAV指出了信道忙的持续时间，意味着站A和AP以外的站点都不能在这段时间内发送数据。
	
	使用RTS帧和CTS帧会使网络的通信效率有所下降，但这两种控制帧都很短，与数据帧相比开销不算大。相反，若不使用这种控制帧，则一旦发生冲突而导致数据帧重发，浪费的时间会更多。信道预约不是强制性规定，各站可自行决定使用或不使用。或只有当数据帧长超过某个数值时才使用RTS帧和CTS帧，这样就更为划算。
	
	此外，数据帧也可携带本次通信所需的持续时间，这些都属于虚拟载波监听机制。站点只要监听到RTS帧、CTS帧或数据帧中的任何一个，就能知道信道将被占用的持续时间，而不需要真正监听信道上的信号，因此虚拟载波监听机制能减少隐蔽站带来的碰撞问题。
	
	CSMA/CD与CSMA/CA主要有如下区别：
	\begin{enumerate}
		\item CSMA/CD可以检测冲突，但无法避免；CSMA/CA发送数据的同时不能检测信道上有无冲突，本节点处没有冲突并不意味着在接收节点处就没有冲突，只能尽量避免。
		\item 传输介质不同。CSMA/CD用于总线形以太网，CSMA/CA用于无线局域网802.11a/b/g/n等。
		\item 检测方式不同。CSMA/CD通过电缆中的电压变化来检测；而CSMA/CA采用能量检测、载波检测和能量载波混合检测三种检测信道空闲的方式。
	\end{enumerate}
	
	总结：CSMA/CA在发送数据帧之前先广播告知其他站点，让其他站点在某段时间内不要发送数据帧，以免发生冲突。CSMA/CD在发送数据帧之前监听，边发送边监听，一旦发生冲突，就立即停止发送。
	
	\subsection{轮询访问：令牌传递协议}
	在轮询访问中，用户不能随机地发送信息，而要通过一个集中控制的监控站，以循环方式轮询每个节点，再决定信道的分配。典型的轮询访问控制协议是令牌传递协议。
	
	在令牌传递协议中，一个令牌（Token）沿着环形总线在各站之间依次传递。令牌是一个特殊的控制帧，它本身并不包含信息，仅控制信道的使用，确保同一时刻只有一个站独占信道。当环上的一个站希望发送帧时，必须等待令牌。站点只有取得令牌后才能发送帧，因此令牌环网络不会发生冲突（因为令牌只有一个）。站点发送完一帧后，应释放令牌，以便让其他站使用。因为令牌在网环上是按顺序依次传递的，所以对所有联网计算机而言，访问权是公平的。
	
	令牌环网络中令牌和数据的传递过程如下：
	\begin{enumerate}
		\item 当网络空闲时，环路中只有令牌帧在循环传递。
		\item 当令牌传递到有数据要发送的站点时，该站点就修改令牌中的一个标志位，并在令牌中附加自己需要传输的数据，将令牌变成一个数据帧，然后将这个数据帧发送出去。
		\item 数据帧沿着环路传输，接收到的站点一边转发数据，一边查看帧的目的地址。若目的地址和自己的地址相同，则接收站就复制该数据帧，以便进一步处理。
		\item 数据帧沿着环路传输，直到到达该帧的源站点，源站点收到自己发出去的帧后便不再转发。同时，通过检验返回的帧来查看数据传输过程中是否出错，若出错，则重传。
		\item 源站点传送完数据后，重新产生一个令牌，并传递给下一站点，交出信道控制权。
	\end{enumerate}
	
	令牌传递协议非常适合负载很高（多个节点在同一时刻发送数据的概率很大）的广播信道，若采用随机介质访问控制，则发生冲突的概率很大。令牌传递协议既不共享时间，又不共享空间；它实际上在随机访问介质访问控制的基础上，限定了有权发送数据的节点只能有一个。
	
	即使是广播信道也可通过介质访问控制机制，使广播信道变为逻辑上的点对点信道，所以说数据链路层研究的是“点到点”之间的通信。
	
	
	
\end{document}